
=====
Kotlin 1.2: общий код для JVM и JavaScript
2017-11-29, 16:13
Вчера компания JetBrains выпустила релиз Kotlin 1.2. Новая версия — большой шаг на пути к тому, чтобы сделать Kotlin наиболее удобным инструментом для разработки всех компонентов современного приложения.
В версии Kotlin 1.1 мы официально выпустили поддержку JavaScript — возможность транслировать код на Kotlin в JS и выполнять его в браузере. В этой версии мы добавляем к этому возможность переиспользования кода между JVM и JavaScript. Теперь вы можете использовать одну и ту же реализацию бизнес-логики во всех компонентах вашего приложения — бэкэнде, фронтэнде в браузере и мобильном приложении под Android. Мы также работаем над библиотеками, которые в этом помогают — в частности, над кросс-платформенной библиотекой для сериализации.

Kotlin 1.2 доступен из коробки в IntelliJ IDEA 2017.3, которая также выходит на этой неделе. Если вы используете Android Studio или более старую версию IntelliJ IDEA, вы можете обновиться при помощи диалога Tools | Kotlin | Configure Kotlin Plugin Updates.
В этот релиз внесли большой вклад наши внешние контрибьюторы, и мы хотели бы поблагодарить всех, кто отправлял нам фидбэк и баг-репорты, а особенно — тех, кто присылал нам пулл-реквесты.
Мультиплатформенные проекты позволяют вам собрать все компоненты вашего приложения — бэкэнд, фронтэнд и Android-приложение — из общей базы кода. Такой проект состоит из общих модулей, которые содержат не зависящий от платформы код, а также платформенно-зависимых модулей, которые содержат код для конкретной платформы (JVM или JS) и могут использовать библиотеки этой платформы. Для того, чтобы вызывать платформенно-зависимый код из общего модуля, вы можете указывать ожидаемые (expect) декларации — декларации, для которых все платформенно-зависимые модули должны предоставить фактические (actual) реализации.

Более подробную информацию об использовании мультиплатформенных проектов можно найти в документации.
Как уже упоминалось, мы также работаем над библиотеками, которые помогают переносить код в общие модули:
Обратите внимание, что поддержка мультиплатформенных проектов выпущена в экспериментальном статусе. Это означает, что писать такие проекты можно уже сейчас, но в будущем у нас может возникнуть необходимость изменить её дизайн (если это случится, мы предоставим инструменты для миграции существующего кода).
В ходе работы над версией 1.2 мы вложили много усилий в то, чтобы сделать компиляцию быстрее. Мы уже достигли ускорения около 25% по сравнению с версией 1.1, и мы видим большой потенциал для дальнейших оптимизаций, которые мы планируем выпускать в обновлениях 1.2.x.
На графике показана разница во времени компиляции двух больших проектов в JetBrains, использующих Kotlin:

Новая версия также включает менее существенные улучшения в языке и стандартной библиотеке:
Более полную информацию об изменениях в языке и стандарной библиотеке можно найти в документе What's new in Kotlin 1.2.
Со времени релиза Kotlin 1.1 в марте этого года распространение его использования по всему миру сильно расширилось. Кульминацией стала KotlinConf, наша первая конференция, которую мы провели в Сан-Франциско 2-3 ноября, и на которую приехало около 1200 наших пользователей. Вы можете посмотреть записи всех докладов на сайте конференциии.
Kotlin также стал официально поддерживаемым языком для разработки под Android. Kotlin-плагин входит в комплект поставки Android Studio начиная с версии 3.0, а на сайте Android можно найти официальные примеры и стайл-гайд. Уже сейчас Kotlin используют более 17% проектов в Android Studio, в том числе многие приложения как самых ярких стартапов, так и компаний из списка Fortune 500.

На стороне сервера состоялся релиз Spring Framework 5.0 с большим количеством фич по поддержке Kotlin, а vert.x официально поддерживает Kotlin начиная с релиза 3.4.0. Кроме того, поддержка билд-скриптов на Kotlin уже входит в комплект поставки Gradle, и проект Gradle Kotlin DSL уже близок к релизу 1.0.
Количество строк кода в open-source репозиториях на GitHub превысило 25 миллионов. А на Stack Overflow Kotlin является одновременно одним из самых быстрорастущих и наименее нелюбимых языков.

Рост сообществ вокруг Kotlin также производит впечатление. По всему миру существует более 100 групп пользователей Kotlin, а докладов про Kotlin столько, что мы с трудом справляемся их все отслеживать — но для тех, про которые мы знаем, карта даёт хорошее представление о том, насколько распространено использование Kotlin.

Для тех, кто только начинает использовать Kotlin, доступно всё больше книг (в том числе наша собственная “Kotlin in Action”, которая уже переведена на русский, японский, китайский и португальский), онлайн-курсов, самоучителей и других ресурсов.
Чтобы рассказать вам больше о новом релизе, мы проведём вебинар о разработке мультиплатформенных проектов при помощи Kotlin 1.2 7 декабря, в 8 вечера по Москве. Не забудьте зарегистироваться; количество участников ограничено!
Кроме того, наша команда проведёт AMA (Ask Me Anything, открытое интервью) на Kotlin Reddit 5 декабря. Мы начнём в 2 часа дня по Москве и будем отвечать на ваши вопросы в течение суток.
Как всегда, попробовать Kotlin можно прямо в браузере, на сайте try.kotlinlang.org
Совместимость: В Kotlin 1.2 язык и стандартная библиотека обратно совместимы (по модулю багов): код, который компилировался и работал в версиях 1.0 и 1.1, будет также работать в версии 1.2. Для того, чтобы большие команды могли переходить на новую версию постепенно, мы добавили опцию компилятора, которая отключает возможность использования новых фич языка и библиотеки. Информацию о возможных трудностях можно найти в документации.
Хорошего вам Kotlin!
User
=====
Google, Softline, GDG и #tceh организуют второй «Google Cloud Developer Meetup»
2017-11-29, 17:10
Пользователь
=====
Медленнее, плавнее: разбираемся с React Fiber
2017-11-29, 16:52

16 сентября 2017 года вышла React Fiber — новая мажорная версия библиотеки. Помимо добавления новых фич, о которых вы можете почитать здесь, разработчики переписали архитектуру ядра библиотеки. Я как React-разработчик решил разобраться, что за зверь этот Fiber, какие задачи он решает, за счёт чего и как в итоге можно применить полученные знания на проектах, над которыми я тружусь в компании Live Typing. Разобрался и пришёл к неоднозначным выводам.
Чтобы понять, что поменяли в новой архитектуре, нужно разобраться в недостатках старой. Для примера рассмотрим следующее демо:
У нас имеется два компонента, исходный код которых вы можете посмотреть здесь Первый компонент работает на старой версии архитектуры, которая называлась Stack, второй — с помощью Fiber. Разница заметна невооруженным глазом: анимация второго компонента работает значительно плавнее анимации первого. 
За счёт чего возникает задержка анимации компонента, реализованного на Stack? Давайте откроем вкладку Performance в браузере и посмотрим на поле Frames, а также на время выполнения функции SierpinskiTriangle (под ней мы подразумеваем выполнение метода render компонента SierpinskiTriangle). В этой функции происходит процесс сравнения старого и нового виртуального дерева. От того, насколько быстро выполняется этот процесс, зависит частота смены кадра. В данном случае она равняется 700 ms, и это долго. 

Рисунок 1. Работа компонента на ядре Stack
Отсюда мы можем сделать вывод, что основной проблемой старой архитектуры было долгое выполнение метода render компонента SierpinskiTriangle. Ускорить его за счёт какой-то оптимизации самого алгоритма вряд ли удалось бы. 
Рисунок 2 иллюстрирует, как React на ядре Fiber отрисовывает компонент. Мы видим, что кадры меняются с частотой один раз в 17 ms. Грубо говоря, Fiber каким-то образом разбивает функцию, которая выполняется долго, на небольшие функции, которые выполняются быстро. 

Рисунок 2. Работа компонента на ядре Fiber 
Как Fiber дробит функцию на части? Для этого необходимо управлять процессом выполнения этой функции, а также предоставлять возможность:
Для реализации вышеперечисленного нам необходимо определить, как разделить работу по сравнению старого и нового DOM деревьев на части. 
Если посмотреть на React, то все компоненты в нём являются функциями. А отрисовка React-приложения — это рекурсивный вызов функций от самого младшего компонента до старшего. Мы уже видели, что, если функция изменения нашего компонента долго отрабатывает, то возникает задержка. Для решения данной проблемы мы можем воспользоваться двумя методами, которые предоставляю браузеры:
В итоге план такой: нам нужно просчитать часть изменения нашего интерфейса по событию requestIdleCallback, и, как только мы будем готовы отрисовать компонент, запросить requestAnimationFrame, в котором это произойдёт. Но всё ещё необходимо как-то прервать выполнение функции сравнения виртуальных деревьев и при этом сохранять промежуточные результаты. Для решения этой проблемы разработчики React решили разработать свою версию стека вызовов. Тогда у них будет возможность останавливать выполнение функций, самостоятельно давать приоритет выполнения функциям, которым он больше необходим, и так далее.
Реимплементации стека вызовов в рамках React-компонентов и есть новый алгоритм Fiber. Преимущество реимплементации стека вызовов в том, что вы можете хранить его в памяти, останавливать и запускать тогда, когда вам это необходимо.
Реализацию поиска числа Фибоначчи с использованием стандартного стека вызовов можно увидеть ниже.
Сначала разберём, как выполняется функция поиска числа Фибоначчи на обычном стеке вызовов. В качестве примера будем искать третье число.
Итак, в стеке вызовов создаётся кадр стека, в котором будут храниться локальные переменные и аргументы функции. В данном случае кадр стека изначально будет выглядеть таким образом:

Т.к. n > 2, то мы дойдем до следующей строки:
Здесь вновь будет вызвана функция fib. Создастся новый кадр стека, но n будет уже на единицу меньше, то есть 2. Локальные переменные всё так же будут undefined. 

И т.к. n=2, то функция возвращает единицу, а мы возвращаемся обратно на строку 5
Стек вызовов выглядит так:

Далее вызывается функция поиска числа Фибоначчи для переменной b, строка 6. Создаётся новый кадр стека:
Функция, как и в предыдущем случае, возвращает 1.
Кадр стека выглядит так:

После чего функция возвращает сумму a и b.
Дисклеймер: В данном случае у нас показано, как исполняется поиск числа Фибоначчи с реимплементацией стека вызовов. Похожим способ реализован Fiber.
Изначально у нас создается переменная fiber, который в нашем случае является кадром стека. arg — аргумент нашей функции, returnAddr — адрес возврата, a — значение функции.
Т.к. fiber.arg в нашем случае равен 3, что больше 2, то мы переходим на строку 17,
где у нас создаётся новый fiber (кадр стека). В нём мы сохраняем ссылку на предыдущий кадр стека, аргумент на единицу меньше и начальное значение нашего результата. Таким образом, мы воссоздаём стек вызовов, который у нас создавался при рекурсивном вызове обычной функции поиска числа Фибоначчи.
После чего мы в обратную сторону итерируемся по нашему стеку и считаем наше число Фибоначчи. строки 7-15.
Стал ли быстрее React после внедрения Fiber? Согласно этому тесту — нет. Он стал даже медленнее примерно в 1,5 раза. Но внедрение новой архитектуры дало возможность рациональнее пользоваться главным потоком браузера, за счёт чего работа анимаций стала плавнее.
Front End Developer
=====
MSA и не только: как мы создаем высоконагруженные сервисы для банка
2017-11-30, 11:48
Организация
=====
Да, PVS-Studio умеет выявлять утечки памяти
2017-11-29, 17:43
DevRel
=====
Олимпиада «Я — профессионал», трек «Информационная и кибербезопасность»
2017-11-29, 18:44
User
=====
Можно ли запихнуть распознавание номеров в любой тамагочи?
2017-12-01, 05:50
Computer Vision, Machine Learning
=====
Распознавание лиц. Создаем и примеряем маски
2017-12-12, 12:01
Пока сообщество iOS-разработчиков спорит, как писать проекты, пока пытается решить, использовать ли MVVM или VIPER, пока пытается подSOLIDить проект или добавить туда реактивную турбину, я попытаюсь оторваться от этого и рассмотреть, как работает под капотом еще одна технология с графика Hype-Driven-Development. 
В 2017 году на вершине графика хайпа — машинное обучение. И понятно почему:
Машинное обучение — широкая тема, остановлюсь на распознавании лиц и попытаюсь разобраться, какие технологии были до рождества христова CoreML, и что появилось после релиза фреймворка Apple.
Задача распознавания лиц — часть практического применения теории распознавания образов. Она состоит из двух подзадач: идентификации и классификации (тут подробно об отличиях). Идентификация личности активно используется в современных сервисах, таких как Facebook, iPhoto. Распознавание лица используется повсеместно, начиная от FaceID в iPhone X, заканчивая использованием при наведении целей в военной технике.
Человек распознает лица других людей благодаря зоне мозга на границе затылочной и височной долей — веретеновидной извилине. Мы распознаем разных людей с 4-х месяцев. Ключевые особенности, которые выделяет мозг для идентификации, — глаза, нос, рот и брови. Также человеческий мозг восстанавливает лицо целиком даже по половине и может определить человека лишь по части лица. Все увиденные лица мозг усредняет, а потом находит отличия от этого усредненного варианта. Поэтому людям европеоидной расы кажется, что все, кто принадлежит монголоидной расе, на одно лицо. А монголоидам трудно различать европейцев. Внутреннее распознавание настроено на спектральном диапазоне лиц в голове, поэтому, если какой-то части спектра не хватает данных, лицо считается за одно и тоже.
Задачи по распознаванию лиц решают уже более 40 лет. В них входит:
Один из оптимальных алгоритмов для нахождения лица на картинке и его выделения — гистограмма направленных градиентов.
Есть и иные алгоритмы. Здесь описывается подробно, как происходит поиск зоны с лицом по алгоритму Виолы-Джонса. Он менее точный и хуже работает с поворотами лица.
Решений, которые включают алгоритмы для распознавания образов, много. Список популярных библиотек, которые используются в iOS:
рис 1. Структура библиотеки DLIB
DLIB
рис 2. Структура библиотеки OpenCV
OpenCV (Open Source Computer Vision Library) 
рис 3. Структура CoreML
iOS Vision Framework
Существуют платные платформы, которые предоставляют решения для задачи распознавания образов. Большинство развивает собственные алгоритмы и технологии. Само собой, эти технологии активно развиваются и используются военными, поэтому некоторые решения засекречены и не имеют открытых исходников.
рис 4. Визуальное отображение структур лица
Цель определения landmarks — нахождение точек лица. Первый шаг в алгоритме — определение локации лица на картинке. После получения локации лица ищут ключевые контуры:
Каждый из этих контуров является массивом точек на плоскости.
рис 5. dlib 68 landmarks
На картинке можно четко увидеть структуры лица. При этом в зависимости от выбранной библиотеки количество landmarks отличается. Разработаны решения на 4 landmarks, 16, 64, 124 и более. 
Перейдем к практической части. Попробуем построить простейшую маску на лице по полученным landmarks. Ожидаемым результатом будет маска вида: 
рис 6. Маска, визуализирующая алгоритм триангуляции Делоне
Триангуляция Делоне — триангуляция для множества точек S на плоскости, при которой для любого треугольника все точки из S за исключением точек, являющихся его вершинами, лежат вне окружности, описанной вокруг треугольника. Впервые описана в 1934 году советским математиком Борисом Делоне.
рис 7. Пример триангуляции Делоне. Из каждой точки порождается окружность, проходящая через две ближайшие в метрике Евклида
Реализуем алгоритм триангуляции Делоне для нашего лица в камере.
Шаг 1. Внутри вы увидите обертку, которая принимает массив точек в двухмерном пространстве и возвращает массив треугольников.
А vertex это wrapper для CGPoint, дополнительно содержащий номер конкретного landmark.
Шаг 2. Перейдем к отрисовке полигонов на лице. Включаем камеру и показываем изображение с камеры на экране:
Шаг 3. Далее получаем кадры с камеры
рис 8. Пример полученного кадра с камеры
Шаг 4. Ищем лица на кадре 
Шаг 5. Ищем landmarks на лице
рис 9. Пример найденных landmarks на лице
Шаг 6. Далее поверх рисуем нашу маску. Берем полученные треугольники из алгоритма Делоне и рисуем в виде layers.
рис 10. Финальный результат — простейшая маска поверх лица
Полная реализация алгоритма триангуляции Делоне на Swift здесь.
И пара советов по оптимизации для искушенных. Рисовать новые layers каждый раз — дорогая операция. Постоянно вычислять координаты треугольников по алгоритму Делоне тоже дорого. Поэтому берем лицо в высоком разрешении и хорошем качестве, которое смотрит в камеру, и прогоняем один раз алгоритм триангуляции Делоне на этой фотографии. Полученные треугольники сохраняем в текстовый файл, а дальше используем эту треугольники и меняем у них координаты. 
MSQRD, Snapchat, VK, даже Авито — все используют маски. 
рис 11. Примеры масок в snapchat
Реализовать простейший вариант маски легко. Берем landmarks, которые получили выше. Выбираем маску, которую хотите применить, и размещаем на ней наши landmarks. При этом существуют простейшие 2D проекции, а есть более сложные 3D маски. Для них вычисляют преобразование точек, которое переведет вершины маски на кадр. Чтобы landmarks, отвечающие за уши, отвечали за уши нашей маски. Далее просто отслеживаем новые положения landmarks лица и меняем нашу маску. 


В этой области есть непростые задачи, которые решаются при создании масок. К примеру, сложности отрисовки. Еще сильнее задачу усложняют моменты скачков landmarks, так как в этом случае маски искажаются и будут вести себя непредсказуемо. А так как захват кадров с камеры мобильного телефона — это хаотичный процесс, включающий в себя быструю перемену света, тени, резкие подергивания и так далее, то задача становится весьма трудоемкой. Еще одной проблемой становится построение сложных масок.
Как развлечение или решение простой проблемы это интересно. Но как и в других областях, если вы хотите решать крутые задачи, то придется потратить время на обучение.
Решение задач распознавания образов, лиц, автомобильных номеров, пола, возраста становится все более востребованным. IT-компании на этом рынке вводят технологии для решения таких задач постепенно и незаметно для пользователя. Китай инвестирует 150 млрд в machine learning в течение ближайших лет, чтобы стать первым в этой области.
В следующей статье расскажу, как идентифицировать конкретного человека по выделенному лицу и фильтровать нечеткие фотографии перед идентификацией.
Solution Architecture
=====
Разработчики о самых грязных программных трюках в играх
2017-11-30, 10:12
Переводчик-фрилансер
=====
Как мы переписали архитектуру Яндекс.Погоды и сделали глобальный прогноз на картах
2017-11-30, 10:37
Привет, Хабр!
Как говорится, по традиции раз в год мы в Яндекс.Погоде выкатываем что-нибудь новенькое. Сначала это был Метеум – традиционный прогноз погоды с помощью машинного обучения, затем наукастинг – краткосрочный прогноз осадков на основе метеорологических радаров и нейронных сетей. В этом посте я расскажу вам о том, как мы сделали глобальный прогноз погоды и построили на его основе красивые погодные карты.

Сперва пару слов про продукт. Погодные карты — способ узнавать погоду, очень популярный на западе и пока что не очень популярный в России. Причиной тому является, собственно, сама погода. Из-за особенностей климата наиболее населенные регионы нашей страны не подвержены внезапным погодным катаклизмам (и это хорошо). Поэтому интерес к погоде у жителей этих регионов скорее бытовой. Так, людям в центральной России важно знать, например, какая погода будет в Москве в выходные или что в четверг в Питере будет дождь. Такую информацию проще всего узнать из таблицы, в которой будет дата, время и набор погодных параметров. 
С другой стороны, жителям Восточного Побережья США важнее знать траекторию очередного урагана с красивым женским именем, а фермерам из Дакоты — следить за распространением града по полям, на которых растет кукуруза. Такую информацию гораздо проще узнавать из карты, чем из множества таблиц. Так и получилось, что погодные сервисы в России — это скорее таблицы, а на Западе — скорее карты. Однако, и в России существуют паттерны потребления погоды, когда пользователю нужно знать где именно будет погода, которая ему нужна: это люди, выбирающие место для пикника в выходные, спортсмены, особенно с приставкой "винд" и "кайт" и, наконец, дачники. Именно для этих категорий пользователей мы и сделали свой продукт. А теперь я расскажу о том, что у него под капотом. 
Мы сразу решили, что для построения карт наш прогноз должен стать глобальным. Ну хотя бы потому, что карты, покрывающие не весь земной шар, отдают средневековьем. Таким образом, нам нужно было расширить Метеум до глобального покрытия. Однако, предыдущая архитектура системы плохо поддавалась горизонтальному масштабированию.
Краткое содержание предыдущих серий. Как помнит внимательный читатель, в первой реализации Метеума мы рассчитывали прогноз погоды по мере необходимости. Как только пользователь попадал к нам на сайт, мы собирали для его координат список факторов и передавали в обученную модель Матрикснета. За сбор факторов отвечал микросервис, который мы называли vector-api. Микросервис был хорош всем, кроме одного: при добавлении новых факторов и/или при расширении географии покрытия, мы приближались к лимиту памяти физических машин, на которых микросервис работал. Кроме того, само по себе формирование ответа итогового погодного API содержало дорогую по времени и по нагрузке на процессор операцию с применением модели Матрикснета. Оба фактора сильно препятствовали построению глобального прогноза. Плюс к тому, в нашем бэклоге образовалась целая очередь из факторов, которые увеличивали точность прогноза в экспериментах, но не могли поехать в продакшен в связи с ограничениями, описанными выше. 
Также мы столкнулись с недостатками выбранной архитектуры для карты осадков, горячо полюбившейся многим пользователям. Для хранения и обработки данных, необходимых для построения информации об осадках, использовалась СУБД PostgreSQL с расширением PostGIS. Во время летних гроз количество запросов в тяжелые ручки в секунду молниеносно превращалось из сотен в десятки тысяч, что влекло за собой высокое потребление процессоров и сетевых каналов серверов баз данных. Эти обстоятельства послужили дополнительным стимулом задуматься о будущем сервиса и применить иной подход в обработке и хранении погодных данных.
Альтернативой расчету погоды в рантайме был предварительный расчет прогнозов для большого набора координат по мере обновления факторов. Мы остановились на глобальной сетке, покрывающей сушу с разрешением 2x2 км, а воду – с разрешением 10x10 км. Сетка разбита на квадратики 3 на 3 градуса – эти квадратики позволяют нам параллельно готовить факторы для модели и обрабатывать результаты. Вот как это выглядит на карте.

Разбиение областей глобальной регулярной сетки
Процесс подготовки погодных факторов начинается с кластера Meteo. В соответствии с названием, на этом кластере происходит «классическая метеорология». Здесь мы скачиваем данные наблюдений и прогнозы моделей GFS (США), ECMWF (Англия), JMA (Япония), CMC (Канада), EUMETSAT (Франция), Earth Networks (США) и многих других поставщиков. Здесь же происходит расчет погодной модели WRF для наиболее интересных для нас регионов. Метеорологические данные, полученные от партнеров или в результате расчетов, обычно запакованы в форматы GRIB или NetCDF с разными уровнями сжатия. В зависимости от поставщика или способа расчета, эти данные могут покрывать Московскую область или весь мир и весить от 200Мб до 7Гб. 
Из метеокластера файлы с прогнозами погоды попадают сначала в MDS (Media Storage, хранилище для больших кусков бинарной информации), а потом в YT — нашу Яндексовую Map-Reduce систему. Работа в YT делится на два этапа, условно названные "подвоз" и "применение". Подвоз — это подготовка факторов для последующего применения обученной модели. Факторы надо корректно переинтерполировать на итоговую сетку, привести к единым единицам измерения и разрезать на квадратики 3 на 3 градуса для параллельной обработки и применения. Сложность процедуры здесь состоит в больших объемах данных и в необходимости проделывать упражнение каждый раз, когда пришли новые данные о прогнозе для какой-либо области.
После того как первая ступень отработала, начинается применение заранее обученных моделей машинного обучения. В первой реализации Метеума мы могли прогнозировать только два основных погодных параметра с помощью машинного обучения: это температура и наличие осадков. Теперь, когда мы перешли на новую схему расчетов, мы можем использовать тот же подход для вычисления остальных параметров погоды. Применение машинного обучения дает ощутимый прирост в точности для новых параметров: давления, скорости ветра и влажности. Ошибка прогнозирования этих параметров на 24 часа вперед падает на величину до 40%. Помимо того, что для многих пользователей важны максимально точные показатели ветра и давления, это улушение позволяет нам более точно рассчитывать еще один популярный параметр — температуру по ощущениям. Он складывается из обычной температуры, скорости ветра и влажности. Еще одним заметным новшеством стал новый способ расчета погодных явлений. Теперь в его основе лежит мультиклассификационная формула — она определяет не только наличие или отсутствие осадков, но также их тип (дождь, снег, град), а еще наличие и балльность облачности. 
Все эти модели нужно применить за ограниченное время для каждой точки регулярной сетки. После того, как модели применились, есть еще один слой обработки данных — бизнес-логика. В этом разделе мы приводим переменные, спрогнозированные с помощью машинного обучения, к нужным нам единицам, а также делаем прогноз консистентным. Поскольку прямо сейчас модели ML достаточно мало знают о физике процессов, в основном опираясь на факторы, полученные из метеомоделей, мы можем получить неконсистентное состояние погоды, например "дождь при температуре -10". У нас есть идеи как делать более правильно в этом месте, однако прямо сейчас это решается формальными ограничениями.
Сложность при применении обученных моделей состоит в том, что для каждого прогноза надо выполнить порядка 14 миллиардов операций. Факторов, необходимых для каждого расчёта — сотни, и список этих факторов весьма подвижен: мы постоянно с ними экспериментируем, пробуем новые, добавляем целые группы, выкидываем слабые. Далеко не все факторы берутся напрямую от поставщиков. Мы экспериментируем с факторами-функциями нескольких параметров. Правила формирования такого количества фичей очень сложно поддерживать в виде кода на Python: громоздко, трудно делать ленивые вычисления, трудно анализировать и диагностировать, какие фичи (или исходные данные для фичей) уже не нужны. Поэтому мы изобрели свой аналог LISP. Строго говоря, это не LISP, а уже готовое AST, которое очень похоже на диалект LISP, в котором учтена специфика наших данных. Процессор этого LISP-а мы сделали таким, как нам надо: ленивым и кэширующим. Поэтому мы, во-первых, не вычисляем факторы, которые стали уже не нужны, во-вторых, не вычисляем дважды, то, что нужно дважды. Эти механизмы автоматически распространяются на все расчёты. А благодаря тому, что это формальное AST мы можем: легко анализировать что нужно, а что не нужно; сериализовать и хранить отдельно части логики, писать бизнес-части в логи, формировать большие части логики автоматически (минуя представление в виде кода), версионировать их для проведения экспериментов и так далее. Оверхед же получился совершенно незначительный, так как все операции выполняются сразу над матрицами.

Общая схема поставки данных в API
После расчета прогнозов мы записываем их в специальный формат – ForecastContainer и загружаем эти контейнеры в микросервис отдачи данных из памяти. Дело в том, что на весь мир с нашей сеткой в 0.02 градуса получается около 1 166 400 000 значений с плавающей точкой, а это 34Гб данных только на один параметр. Таких параметров у нас больше 50 – поэтому держать эти данные полностью в памяти одной физической машины не представлялось возможным. Мы начали искать формат, поддерживающий быстрое чтение сжатых данных. Первым кандидатом стал HDF5 – у которого есть функциональность чанкования данных и поддержки буфера распакованных чанков. Вторым кандидатом стал наш --самописный-- проприетарный формат – матрицы float'ов сжатые LZ4 и записанные в Flatbuffer. Результаты тестов показали, что открытие файла с данными для работы занимает в два раза меньше времении у Flatbuffers, чем у HDF5, как и чтение произвольной точки из кеша. В итоге сейчас данные для 50 переменных занимают 52.1Гб.
Так как требования к потреблению памяти и времени ответа были очень высоки еще с постановки задачи, старый микросервис написанный на Python мы решили переписать на C++. И это дало свои результаты: Время ответа сервиса в 99 квантиле упало со 100мс до 10мс.
Перевод сервиса на новую архитектуру бэкендов позволил нам отдавать прогнозы погоды нашим внутренним и внешним партнерам напрямую, минуя кэши и расчеты моделей в рантайме, дал возможность при срочной необходимости с легкостью масштабировать нагрузки с использованием облачных технологий Яндекса.
Итого, новая архитектура позволила нам уменьшить тайминги ответов, держать бОльшие нагрузки и избавиться от рассинхрона данных, отдаваемых разным партнерам. Данные Яндекс.Погоды представлены в большом количестве сервисов Яндекса и не только: главная страница Яндекса, плагин погоды в Яндекс.Браузере, погода на Рамблере и так далее. В результате все они получили возможность отдавать именно те значения, которые в этот момент видят пользователи основной страницы Яндекс.Погоды.
Для того, чтобы отрисовать прогнозы на наших новых картах, необходимо достаточно много дополнительной обработки. Сначала мы запускаем MapReduce операции на тех же данных, что отдаются микросервисом и API для формирования данных на весь мир на регулярной широтно-долготной сетке с разрешением в 0.02 градуса. На выходе для каждого параметра: температуры, давления, скорости и направления ветра, а также для каждого горизонта: времени прогноза в будущее или времени факта в прошлое мы получаем матрицы размером 9001*18000.
После этого мы строим проекцию Меркатора по этим данным и нарезаем их на тайлы в соответствии с требованиями API Яндекс.Карт. Здесь мы встречаемся с одной из самых больших сложностей в цепочке подготовки погодных карт: количеством тайлов, которые нужно оперативно обновлять при генерации каждого нового прогноза. Так каждый следующий уровень приближения карты (zoom) требует в 4 раза большего количества тайлов чем предыдущий. Несложно прикинуть, что для всех уровней приближения от 0 вплоть до 8 нужно подготовить 


По мере готовности картинок мы загружаем их во внутреннее хранилище файлов Яндекса с высокой параллельностью. Как только весь слой карты для всех зумов загрузился, мы подменяем значение индекса, по которому фронт понимает, куда идти за новыми тайлами. Таким образом достигается достаточно высокая скорость появления новых прогнозов на карте и консистентность данных в пределах времени прогноза.
Даже несмотря на серьезную подготовку на бэкенде, отрисовка и анимация погодных карт тоже представляет собой сложную задачу. Рассказать все в одной статье не получится, поэтому здесь мы сделали акцент на самой заметной фиче — отрисовке анимированных частиц, показывающих направление втера. 

Так выглядят частицы анимации ветра на картах погоды
Для оптимизации потребления ресурсов клиентского устройства, анимация ветра выполнена с использованием WebGL. WebGL позволяет задействовать значительные ресурсы графического адаптера, разгружая процессорный поток выполнения кода, а также оптимизируя расход аккумулятора. Задачей процессора в этом случае становится установка аргументов выполнения шейдерных программ. Для перемещения частиц используется подход хранения положения частицы в 2х цветовых каналах текстуры для каждой оси (x/y). Таких текстур положения две: одна хранит текущее положение частиц, вторая предназначена для сохранения нового состояния.
Процесс отрисовки частиц описан несколькими WebGL программами. Для большей совместимости использована первая версия этого стандарта. В браузере использование WebGL выполняется через соответствующий контекст элемента canvas. Так как графический ускоритель способен выполнять в несколько раз больше параллельных операций через процессор, то перемещение частиц следует выполнять через программу WebGL. Выходной результат выполнения такой программы представляет набор точек в заданном пространстве. Это пространство по умолчанию является видимой областью своего canvas, то есть, экраном пользователя. Однако есть возможность указать целью отрисовки текстуру, используя фреймбуферы.
При старте слоя ветра процессор генерирует начальное положение частиц, создавая типизированный массив элементов в диапазоне 0...255, в количестве (частицы * компоненты) (RGBA). Из этого массива создаются две WebGL текстуры положения. Данные по скорости ветра также записываются в текстуру, чтобы видеокарта имела к ним доступ. Красный и зеленый канал этой текстуры содержат значение параллельной и меридиональной скоростей соответственно, причем значение 127 соответствует отсутствию ветра, значения меньше 127 задают скорость ветра в отрицательном направлении по оси, значения больше — в положительном. С помощью подготовленных текстур происходит отрисовка текущего положения частиц. После отрисовки, одна из текстур положения обновляется, используя вторую текстуру как источник данных по текущему состоянию. Следующие видимые кадры будут сформированы при помощи отрисовки предыдущего снимка положения частиц с увеличенной прозрачностью, поверх которого будет нанесено текущее положение частиц. Таким образом получаются частицы с затухающими хвостами.

Так выглядят частицы анимации ветра в процессе их создания
Как оказалось, текущий алгоритм кодирования положения частиц в пикселях текстуры для качественной визуализации требует поддержки на аппаратном уровне высокой точности вычислений и float-значений в самих текстурах, чего зачастую нет на мобильных устройствах, поэтому алгоритм будет переработан и улучшен.
Вот примерно все, что я хотел рассказать вам об архитектуре Яндекс.Погоды. За этот год мы полностью переделали сервис изнутри (об этом рассказано выше) и снаружи (практически все платформы, на которых присутствует Я.Погода были существенно перерисованы). Финалом этих изменений стали интерактивные погодные карты, которые вы можете попробовать на нашем сервисе.
Однако, мы никогда не останавливаемся на достигнутом. В следующем году вас ждет много интересных продуктовых и технологических апдейтов: от сервиса, позволяющего исследовать климат в разных уголках Земли до ответа на вопрос "чем дышит человек". И это, разумеется, далеко не все. Оставайтесь с нами.
Всегда ваша,
Команда Яндекс.Погоды
Пользователь
=====
Настройка системы WEB — тестирования на основе headless chromium-browser, chromedriver, nightwatch и node.js на Ubuntu
2017-11-29, 21:46
https://wsofter.ru
=====
Сетевой JTAG программатор для Altera Quartus Prime из Raspberry Pi3
2017-11-29, 22:33
Программист
=====
Две геометрические задачки, которые попадались на собеседовании, и где они обитают
2018-01-20, 22:59
Программист C++
=====
Разработка для Sailfish OS: использование датчиков (часть 2)
2017-12-01, 10:46
Пользователь
=====
Mikrotik: Ограничение скорости скачивания для определенных IP-адресов
2017-11-29, 23:16
Senior Backend PHP Developer
=====
Разбор квеста Digital Security ICO
2017-12-01, 10:18

Перед ежегодной конференцией ZeroNights 2017, помимо Hackquest 2017, мы решили организовать еще один конкурс, а именно — провести свое ICO (Initial coin offering). Но только не такое, как все привыкли видеть, а для хакеров. А как мы могли понять, что они хакеры? Они должны были взломать ICO! За подробностями прошу под кат.
Для начала — легенда. 
Задача участников проекта состояла в следующем:
И вот так выглядела часть, на которой отображались заявки в whitelist, сам whitelist и заветная кнопка, "которую должен был нажать владец". 

Если проанализировать предоставленные ссылки на etherscan.io, то вырисовывается следующая архитектура смарт-контраков:
После прочтения ICO становится очевидно, что просто так заявку на рассмотрение владельцу не подашь:
Где же взять столько эфиров? Первое, что может прийти в голову, — намайнить! Сеть-то тестовая, участников должно быть немного… Но нет. В сети Rinkbey используется Proof-of-Authority консенсус, поэтому майнят только избранные ноды и раздают полученый эфир всем желающим. Так что один из вариантов был — наплодить аккаунтов в Twitter, Google+, github.com и собрать необходимое количество эфира. По расчетам, имея чуть более сотни аккаунтов, можно было собрать такое количество за сутки. Если кто-то из участников, читающих разбор, прибегал к такому решению, — отпишитесь в комментах, нам интересен ваш опыт. 
Те, кому такой вариант показался скучным, могли заметить в описании (или контракте), что есть некая лотерея, которая каждые пять блоков предоставляет возможность любому подать заявку. Все, что нужно, — угадать число, которое загадал лотерейный робот.
Функция смарт-контракта, которую вызывал робот, выглядит так:
Предполагалось, что участники в течение пяти блоков отправляют свои числа, а после этого робот присылает то, которое загадал он. Смарт-контракт должен был проверить, угадал ли кто-то из игроков. В случае успеха участник отправлялся в desires. Казалось бы, с точки зрения смарт-контракта нет никаких уязвимостей: робот своей транзакцией закрывал раунд и подсмотреть, какое число было загадано, можно было только постфактум. Но на самом деле нет.
Для того, чтобы узнать, какое число загадал робот, и — самое главное — послать транзакцию раньше него, нужно было понимать, как эти транзакции обрабатываются сетью. Если коротко:
все новые транзакции сначала попадают в пул неподтвержденных (общий для всех участников сети), и майнеры, при формировании очередного блока, набирают себе транзакций именно оттуда. Но не в том порядке, как они были присланы, а в по убыванию комиссии и порядкового номера транзакции — gasPrice и nonce соответственно (на самом деле, цена "газа" — это только одна из составляющих комиссии, которую получает майнер; вторая — это сам потраченый газ).
Таким образом, все, что нужно было — это посмотреть на число, которое отправил робот, пока транзакция еще находилась в пуле неподтвержденных, и отправить следом свою с этим же числом, но большей ценой "газа". Принимая во внимание, что нахождение нового блока занимает 12-30 секунд, — у атакующего было достаточно времени, чтобы провернуть Front-running attack. Пример эксплоита можно изучить тут.
(Лирическое отступление) Если бы в конкурсе участвовал какой-нибудь Интернет-провайдер, имеющий возможность провести BGP hijacking атаки, по аналогии с описанными здесь, то он так же мог бы управлять порядком транзакций.
Итак, заявка подана. Остается только ждать, пока владелец внесет меня в белый список и я смогу купить заветные HACK-коины. Звучит скучно, не правда ли? Может, есть способ как-то еще попасть в белый список? Смотрим в контракт:
К функции addParticipant, которая вызывается при нажатии на одноименную кнопку в веб-интерфейсе, применен модификатор onlyController поэтому, чтобы ее вызвать, надо подписать транзакцию приватным ключом адреса controller. А может, за неимением такового, заменить самого владельца? Изучая исходники одного из наследуемых контрактов, — Controlled — можно заметить, что предусмотрена смена владельца через функцию changeController:
На самом деле, функция актуальна только для смарт-контакта HACK-коина (чтобы при деплое изменить controller с адреса разработчика на адрес контракта ICO), но, поскольку контракт Controlled полезен и наследуется обоими контрактами, то и changeOwner будет у обоих. Пробуем? Неудача :( Разработчик предусмотрел это и вызвал функцию сразу при деплое со своим же адресом. 
После того, как были исчерпаны все теории, могло и вправду показаться, что владелец заносит участников с whitelist вручную. Но это, конечно же, не так.
Для прохождения второго этапа необходимо было провести blockchain stored XSS-атаку против владельца. Намек на это можно было увидеть в функции изменения email:
Заметили? Никаких проверок на то, что содержится в строке _email нет. Нет их главным образом потому, что делать такие проверки дорого (потраченый gas), да и никаких встроеных функций для работы со строками нет. Выходит, единственный барьер защиты будет, скорее всего, реализован на клиентской стороне. Взглянем, как реализовано добавление email на страницу Statistic:
Конечно же, участники видели уже отрендеренный вариант. Но ничего не мешало экспериментировать:
С точки зрения соревнования, также важно приходить уже с рабочим вектором атаки, поскольку в блокчейне все могут увидеть действия соперников!
Вот первый присланный вектор:
Полезная нагрузка в большинстве случаев подтягивалась отдельно (и я даже не пытался ходить по этим ссылкам и смотреть, что там), но вот авторский пример:
Стоит также отметить, что атака удалась потому, что владелец использует клиент geth с разлоченным аккаунтом coinbase. Если бы применялся какой-то кошелек, то при инициации транзакции пользователю высветилось бы окошко, требующее подтверждение транзакции. Однако, не стоит считать, что использование кошелька спасет от всех бед. Атакующий все еще может управлять данными, из которых формируется транзакция (например, подменить адрес, выбранный владельцем на свой). 
Ну что ж, мы почти у цели. Пора покупать 31337 HACK-коинов. Смотрим функцию покупки:
С ходу видно, что смарт-контракт не позволяет приобрести больше 1000 HACK-коинов, а нужно более 31337. Однако не беда! Обратите внимание, как происходит проверка баланса покупателя. Учитывается только текущий баланс! Логичным решением будет просто перевести коины куда-то еще и купить снова.
Смотрим, как можно сделать перевод:
Функция-то есть для перевода, но к ней приведен модификатор, который должен ограничить возможность ее вызова до окончания ICO. Однако, на самом деле модификатор не выполняет своей функции вне зависимости от условия, поскольку это самое условие еще нужно обработать. Вот правильный вариант:
Таким образом, можно повторить операцию "покупка-вывод" 13 раз и накопить заветное количество токенов. Другой вариант — применить функцию transferFrom — процесс немного более сложный, но тоже рабочий.
Этап бонусный, потому что не задумывался как этап вовсе, но многих заставил серьезно погуглить, и после прохождения прислать эмоциональный отзыв вместе с флагом (в рамках приличия, конечно же). Итак, подпись к форме на сайте гласит:
Для передачи флага требовалось именно off-chain взаимодействие с участниками (то есть вне сети Ethereum). Поскольку приглашение можно было получить именно в обмен на флаг (секрет), а хранение секретов в блокчейне дело непростое — даже если зашифровать флаг, то как отдать правильному участнику ключ? Собственно, поэтому мы просили участника сгенерировать подписанную транзакцию с того адреса, на котором имеется нужное количество HACK-коинов и отправить ее на бекенд ico.dsec.ru, а не в сеть. Вот подробный пример, как можно сгенерировать подобную транзакцию.
Вот и все. Спасибо всем, кто принял участие в ICO и наши поздравления победителям! Для тех, у кого проснулось желание пройти квест — он поработает еще пару дней :)
Так же огромное спасибо тем людям, которые перед ZeroNights нашли время и помогли мне.
Pentester, Security researcher. @_p4lex
=====
Противоестественная диагностика
2017-12-12, 15:01
Developer
=====
Serverless tensorflow на AWS Lambda
2018-08-30, 13:10

Машинное обучение и нейросети становятся все более незаменимыми для многих компаний. Одна из основных проблем, с которыми они сталкиваются — деплой такого рода приложений. Я хочу показать показать практичный и удобный способ подобного деплоя, для которого не требуется быть специалистом в облачных технологиях и кластерах. Для этого мы будем использовать serverless инфраструктуру.
В последнее время множество задач в продукте решается с применением моделей, созданных машинным обучением или нейросетями. Часто это задачи, которые много лет решались обычными детерменистскими методами теперь легче и дешевле решать через ML.
Имея современные фреймворки типа Keras или Tensorflow и каталоги готовых решений становится проще создавать модели, которые дают необходимую для продукта точность.
Мои коллеги называют это “коммодитизацией машинного обучения” и в чем-то они правы. Самое главное — что сегодня легко найти/скачать/натренировать модель и хочется иметь возможность также легко ее деплоить. 
Опять же при работе в стартапе или маленькой компании часто нужно быстро проверять предположения, причем не только технические, но и рыночные. И для этого нужно быстро и несложно деплоить модель, ожидая не сильный, но все же трафик.
Для решения такой задачи деплоя мне понравилось работать с облачными микросервисами.
Amazon, Google и Microsoft недавно предоставили FaaS — function as a service. Они относительно дешевые, их легко деплоить (не требуется Docker) и можно параллельно запускать практически неограниченное количество сущностей.
Сейчас я расскажу, как можно задеплоить модели TensorFlow/Keras на AWS Lambda — FaaS от Amazon. Как итог — API для распознавания содержания на изображениях стоимостью 1$ за 20000 распознаваний. Можно дешевле? Возможно. Можно проще? Вряд ли.
Рассмотрим диаграмму различных видов деплоя приложений:

Слева мы видим on premise — когда мы владеем сервером. Далее мы видим Infrastructure-as-a-Service — здесь мы уже работаем с виртуальной машиной — сервером, расположенным в датацентре. Следующий шаг — Platform-as-a-Service, когда у нас уже нет доступа к самой машине, но мы управляем контейнером в котором будет исполняться приложение. И наконец-то Function-as-a-Service, когда мы контролируем только код, а все остальное спрятано от нас. Это хорошие новости, так как мы увидим позже, что дает нам очень крутую функциональность.
AWS Lambda — это имплементация FAAS на платформе AWS. Кратко про имплементацию. Контейнером для него является zip архив [код + библиотеки]. Код такой же как на локальной машине. AWS разворачивает этот код на контейнерах в зависимости от количества внешних запросов (триггеров). Границы сверху по сути нет — текущее ограничения — 1000 одновременно работающих контейнеров, но его легко можно поднять до 10000 и выше через саппорт.

Главные плюсы AWS Lambda:
Прежде всего хочу уточнить, что для своих примеров я использую Tensorflow — открытый фреймфорк, который позволяет разработчикам создавать, тренировать и деплоить модели машинного обучения. На данный момент это самая популярная библиотека для глубокого обучения и ее используют как эксперты, так и новички.
На данным момент основным способом деплоя моделей машинного обучения является кластер. Если мы хотим сделать REST API для глубокого обучения, он будет выглядеть следующий образом:

(Изображение из блога AWS)
Кажется громоздким? В то же время вам придется позаботиться о следующих вещах:
На AWS Lambda архитектура будет выглядеть заметно проще:

Во-первых такой подход очень масштабируемый. Он может обработать до 10 тысяч одновременных запросов без прописывания какой-либо дополнительной логики. Такая особенность делает архитектуру идеальной для обработки пиковой нагрузки, так как ей не требуется дополнительное время на обработку.
Во-вторых вам не придется платить за простой сервера. В Serverless архитектуре оплата идет за один реквест. Это означает, что если у вас будет 25 тысяч реквестов, вы заплатите только за 25 тысяч реквестов, в независимости каким потоком они пришли. Таким образом не только стоимость становится более прозрачной, но и стоимость сама по себе очень низкая. Для примера на Tensorflow, который я покажу позднее стоимость составляет 20-25 тысяч запросов за 1 доллар. Кластер с аналогичным функционалом стоит гораздо больше, а выгоднее он становиться только на очень большом количестве реквестом (>1 миллиона).
В-третьих инфраструктура становится гораздо больше. Не нужно работать с докером, прописывать логику масштабирования и распредления нагрузки. Если коротко — в компанию не придется нанимать дополнительного человека на поддержку инфраструктуры, а если вы датасаентист, то вы сможете сделать все своими руками.
Как вы увидите ниже, деплой всей инфраструктуры для вышеупомянутого приложения требуется не более 4 строк кода.
Было бы некорректно не сказать о недостатках serverless инфраструктуры и о тех случаях, когда она работать не будет. У AWS Lambda есть жесткие ограничения на время обработки и на доступную память, которые надо иметь в виду.
Во-первых, как я и упоминал ранее кластеры становятся более выгодными после определенного числа реквестов. В случаях когда у вас нет пиковой нагрузки и много реквестов, кластер будет более выгоден.
Во-вторых, у AWS Lambda есть небольшое, но определенное время старта (100-200мс). Для приложений глубокого обучения требуется еще некоторое время на скачивание модели с S3. Для примера, который я буду показывать ниже, холодный запуск будет составлять 4.5 секунды, а теплый — 3 секунды. Для некоторых приложений это может быть не критично, но если ваше приложение сфокусировано на максимально быстрой обработке одиночного реквеста, кластер будет более хорошим вариантом.
Теперь перейдем к практической части.
Для этого примера я использую достаточно популярное применение нейронных сетей — распознавание изображений. Наше приложение берет картинку на вход и возвращает описание объекта на ней. Такого рода приложения широко используются для фильтрации изображений и классификации множества изображений на группы. Наше приложение будет пытаться распознать фотографию панды. 

Памятка: Модель и оригинальный код доступны здесь
Мы будем использовать следующий стек:
Для начала вам нужно установиться и настроить Serverless фреймворк, который мы будет использовать для оркестрации и деплоя приложения. Ссылка на гайд. 
Сделайте пустую папку и запустить следующую команду:
Вы получите следующих ответ:
Как вы видите, наше приложение успешно распознало картинку с пандой (0,89).
Вуаля. Мы успешно задеплоили нейронную сеть для распознавания изображений на Tensorflow на AWS Lambda.
Начнем с конфигурационного файла. Ничего нестандартного — мы используем базовую конфигурацию AWS Lambda.
Если мы посмотрим на сам файл 'index.py', то мы увидим, что сначала мы скачиваем модель ('.pb' файл) в папку '/tmp/' на AWS Lambda, а потом импортируем ее стандартным образом через Tensorflow.
Ниже ссылки на части кода в Github, которые вы должны иметь в виду если вы хотите вставить свою собственную модель:
Скачивание модели с S3:
Импорт модели:
Скачивание изображения:
Получение предсказаний из модели:
Теперь давайте добавим API к лямбде.
Самый простой способ добавить API это модифицировать конфигурационный YAML файл.
Теперь давайте передеплоим стек:
Получаем следующее.
Чтобы протестировать API можно просто открыть качестве ссылки:
Или использовать curl:
Мы получим:
Мы создали API для модели на Tensorflow на основе AWS Lambda с помощью Serverless фреймворка. Все удалось сделать достаточно просто и такой подход сэкономил нам много времени по сравнению с традиционным подходом.
Модифицируя конфигурационный файл, можно подключить множество других AWS сервисов, например SQS для потоковой обработки задач или сделать чатбота, использую AWS Lex.
В качестве моего хобби я портирую множество библиотек, чтобы сделать serverless более дружелюбным. Вы можете найти их здесь. У проекта MIT лицензия, поэтому можете спокойно модифицировать и использовать его для своих задач.
Библиотеки включают в себя следующие примеры:
Я очень рад видеть, как другие используют serverless для своих проектов. Обязательно скажите обратную связь в комментариях и удачной вам разработки.
Пользователь
=====
В «Налог на Гугл» внесли изменения
2017-11-30, 08:10
Пользователь
=====
Дайджест событий для HR-специалистов в IT-области на декабрь 2017
2017-11-30, 08:56
Помогаем строить карьеру в IT
=====
Как я применил когортный анализ участвуя в соревновании по сбросу веса
2017-11-30, 09:42
Аналитик
=====
Заразить во благо: как мы исполняем паразитный код
2017-11-30, 10:01
Пользователь
=====
Идеальный мавен. Часть 1
2017-11-30, 10:32
Знаю, он не идеальный, но по крайней мере я попытаюсь рассказать, как его к этому приблизить. 
В одну заметку всё не войдёт, поэтому сначала план:
Итак, начнем с постановки задачи. Предположим у нас есть группа людей (компания, фирма, кружок), которые разрабатывают проекты на Java. При этом у них есть как проекты с открытым кодом (OSS), так и проекты с закрытым кодом. Проекты, назовём их внутренние, разрабатываются независимо друг от друга, но между ними есть зависимости. Что хочется:
Важно понимать, что решать эти проблемы мы будем в комплексе, поэтому без реализации всех хотелок, система работать вообще не будет или будет, но не в полную силу. 
Так же я хочу сказать, что все ниже написанное, это моё личное мнение, я осознаю, что мавен можно настроить по-разному, и я со своей стороны очень надеюсь найти нечто новое в обсуждении этой статьи, что поможет сделать существующую модель еще лучше. В свою защиту могу сказать, что модель, которую я буду описывать уже работает, и не один год. Открытая часть лежит на BitBucket, в частности, здесь.
Для управления сторонними зависимостями у мавена есть специальная секция dependencyManagement и механизм наследования. Казалась бы – вот и ответ, делаем «корпоративный» POM и наследуем от него корневые POM’ы всех наших проектов. Да, так и будет, но вот детали…. Итак, вот он наш будущий «корпоративный» POM:
Всё стандартно, но хочу обратить внимание не некоторые вещи:
С какими проблемами/непонятками обычно сталкиваются при использовании «корпоративного» POM’а? 
Все это мы разберём в следующих частях это статьи. Сейчас кратко остановлюсь на двух последних пунктах. 
«Корпоративный» POM описывает общие правила для всех ваших проектов в компании – как-то версия Java, зависимости и их версии, общие ограничения на проекты, за которыми может следить мавен, контакты на разработчиков и т.д. В «корпоративном» POM’е не должно быть информации («зависимостей») о конкретных внутренних проектах — их версий, специфичных профайлов и подобных вещей. 
Версии библиотек/плагинов можно легко отслеживать с помощью самого мавена (плагин versions-maven-plugin). Для этого добавим в наш «корпоративный» POM в секцию pluginManagement следующий фрагмент
А рядом с pom.xml создадим файл rules.xml со следующим содержимым
В принципе это делать не обязательно, и строчку <rulesUri>file://${user.dir}/rules.xml</rulesUri> можно убрать. У меня это файл служит для фильтрации различных «мусорных» версии которые я предпочитаю игнорировать.
После этого достаточно запустить команды versions:display-dependency-updates и versions:display-plugin-updates и получить результаты:
Кстати, вторая команда предупредит вас если вы используете какой-то плагин без указания его версии (правда, для этого ее надо запускать на проектных POM‘ах) 
Вкратце это всё про «корпоративный» POM. В следующей части я планирую рассказать о типовой структуре проекта и, возможно, о организации деполоя артефактов в центральный и корпоративный репозитории.
Человек
=====
Анатомия коллтрекинга: как анализировать звонки в компанию
2017-11-30, 12:32
User
=====
Пример реализации автоматизированного процесса резервного копирования и восстановления баз данных встроенными средствами
2017-12-01, 08:41
MS SQL Server and .NET Developer, DBA
=====
In-Memory Computing Summit 2017 San Francisco
2017-12-01, 11:54
Digital Consultant
=====
Scala в ЕРАМ: обучение и проекты
2017-11-30, 12:28
User
=====
Zimbra с сертификатом, подписанным УЦ Active Directory
2017-12-01, 10:12
Сетевой инженер
=====
Цена JavaScript
2017-11-30, 13:08
автор, переводчик, редактор
=====
Стоимость операций в тактах ЦП
2017-11-30, 13:30
Редактор
=====
Как работает JS: особенности и сфера применения WebAssembly
2017-11-30, 14:21
Пользователь
=====
Как программистам не дают больше ничем заниматься
2017-11-30, 20:50
автор, переводчик, редактор
=====
Docker, как показатель зрелости
2017-11-30, 15:41
Когда мы были детьми, мы многого не понимали, что на данный момент является абсолютно очевидным. "Почему я не могу есть одни конфеты? Почему бутерброды — не еда, я же это ем? Почему я не могу спать с хомяком? Зачем чистить зубы каждый день, да еще целых два раза?!" Действительно, зачем? Но будучи взрослыми, зрелыми людьми, мы не задаемся подобными вопросами.
Похожая ситуация происходит и в мире IT (возможно, это касается любой профессиональной деятельности). Многих вещей на начальных стадиях мы можем не понимать. "Зачем, да и вообще кому нужны всякие Symfony, если есть WordPress? Зачем тратить время на автоматическое тестирование, если и так все работает? Git? Linux? SOLID, DRY, DDD, TDD? Что за страшные слова?". Новичкам сложно понять, зачем нужно так много всего. Осознание приходит со временем и вскоре такие вещи становятся не только понятными, но и очевидными.
Хочу рассказать свою историю того, как я дорос до Docker и что этому поспособствовало.
Docker достаточно простая вещь. Но вы когда-нибудь пытались объяснить простому смертному, что это такое? Я пытался. Не получилось. Я и сам долгое время не понимал, зачем нужна эта штука. Я слышал про него, пытался вникнуть, но каждый раз бросал. Видимо, было еще рано — не созрел. Но в определенный момент произошла "Эврика"! Это был достаточно долгий путь, о котором я хотел бы рассказать другим. Возможно, эта статья поможет Вам сократить свой путь и начать пользоваться этим замечательным инструментом как можно раньше.
Когда я начал работать веб-разработчиком, я попал в одну очень маленькую контору. Опыта у меня было 0, мы разрабатывали простые сайты на собственной, как мы это называли, "CMS". Слово OpenSource для меня было незнакомым, все функции писались руками, архитектуры не было никакой. Дыры в безопасности были подобны черным дырам. Если сайты типа "Блог", "Самый простой интернет-магазин" или "Новостная лента" были еще как-то реализуемы, то вещи чуть сложнее вызывали дикую боль и страдания. Работали мы через FTP, из-за чего возникали серьезные проблемы, когда два человека работали над одним и тем же файлом. Если кто-то что-то сломал или случайно затер, это пропадало на веки вечные.
И тут к нам пришел заказчик, по совместительству мой хороший друг, с одним незаурядным на фоне всех других наших работ проектом. Мы приступили за работу, но вскоре контора закрылась и я стал напрямую работать с этим проектом один. Тогда у меня и появилась возможность сделать все по-человечески...
На втором этапе было принято несколько решений, порядок которых я уже и не помню. Скорее всего они происходили одновременно.
Во-первых, весь код я стал хранить в приватном Git репозитории на Bitbucket. Это, пожалуй, первое, чему стоит научиться любому разработчику. Это повысило защищенность кода от стороннего вмешательства. VPS мог сломаться, его могли взломать, просто не успели заплатить. Думаю, мысль ясна. Помимо этого, я закрыл FTP и деплой происходил по Git Push Webhook из ветки release или test. 
Во-вторых, вместо одного сайта, было запущено два почти одинаковых. Первый — релиз, который обслуживал реальных клиентов. Второй — тестовый, который был запаролен, имел все самые последние нестабильные фичи. Последний нужен для того, чтобы не выкатывать в продакшн то, что не проверено и может работать не правильно. Хоть и работали эти два сайта на одном хосте, но были независимы.
В-третьих, что самое главное, разработка стала вестись локально. На ноутбук устанавливался стек LAMP, все это дело настраивалось, проект запускался локально на поддомене .dev (UPD Этот домен как оказалось занят и использовать его не стоит. Спасибо alexkbs, за комментарий). Это значительно ускорило разработку, позволило работать offline, пропала боязнь что-то сломать и жить стало чуточку проще.
Но вдруг я накосячил и пришлось переустановить Ubuntu. Заново приходится устанавливать и настраивать весь этот стек… Это дело можно было автоматизировать bash скриптом, но будет жесткая привязка к системе. Пусть я работаю один, но у начальства есть планы нанять второго, поэтому хотелось бы упростить жизнь и ему, и себе. После нескольких таких косяков я все таки нашел решение.
Vagrant — проще говоря, это инструмент, который дает Вам возможность настраивать VirtualBox скриптом. То есть, если написать правильный конфиг, то одной командой vagrant up можно загрузить готовый образ Debian, установить туда необходимый софт, настроить и получить готовое тестовое окружение.
Ну не чудо ли? На самом деле чудо! Это заметно упростило процесс разворачивания среды разработки, очистило основную систему от ненужных пакетов, и, благодаря виртуализации, я смог настроить машину как можно ближе к настройкам хостинга.
Виртуализация здорово подходит для разработки. Но в продакшн оно не годится. Это лютый овехед для системы. К тому же этот метод не решает несколько проблем:
Да и лишними несколько гигабайт оперативки никогда не бывает.
А вот теперь возьмите все проблемы из всех этапов выше и представьте, что есть инструмент, который просто и элегантно решит их за Вас.
Что такое Docker? В интернете очень много сравнений с контейнерами, которые используются для транспортировки товара. Это верно. 
Но представьте себе большой зал и много людей, которые над чем-то работают. Не обязательно, что они делают одно и то же. Бухгалтера, юристы, плотники, маляры, аналитики, программисты. Да, они иногда взаимодействуют. Но из-за того, что у них нет личного рабочего пространства, такая работа больше похожа на хаос. Беда с документами, кто-то у кого-то ручку стащил, кто-то сел на чужой стул, притронулся к не своему компу и п.р. Но если поставить перегородки, каждому человеку или группе людей выделить личное рабочее место, то и жить становится проще. Каждый занимается своим делом, пользуется своими ресурсами, никому не мешает. Взаимодействие происходит через выделенные каналы — почту, архивы или двери.
Также и Docker. Большой зал — это Ваша система (в действительности, всё немного сложнее, но для общего понимания концепций можно и упростить). Docker может создать изолированное пространство для процесса или группу процессов, делающих свою работу. Взаимодействие происходит путем открытых портов, общих файлов, внутренних и внешних сетей. Никто никому не мешает. Таким образом, Вы можете на одном хосте иметь хоть 20 версий PHP или еще чего-нибудь. А благодаря образам (images) Вы можете, образно говоря, заказать готовый офис со всеми плюшками.
Подключение к проекту новых компонентов стало вопросом нескольких строк благодаря docker-compose. Вот несколько примеров:
Нужно обновить версию MySQL? Сколько времени и усилий пришлось бы приложить, чтобы сделать это на обычном VPS хостинге? Тут это вопрос 3-4 символов.
Благодаря тому, что Docker работает внутри рабочей системы, а не создает новую, он не сильно нагружает машину для разработки и вполне пригоден для прода. И это большой плюс, потому что код, запущенный на машине разработчика с Ubuntu, будет работать ровно также, как и на сервере с Debian
Еще одна польза от такого решения — автоматическое тестирование стало значительно проще. Проверка происходит внутри изолированных контейнеров, которые создаются перед каждым запуском. Поэтому тесты стабильны и выполняются или не выполняются всегда.
Docker понять не просто, несмотря на его простоту. Для этого нужно подрасти. У меня не было надежного руководства, поэтому все приходилось постигать самому. Возможно кто-то как я благодаря этой статье сможет вырасти немного быстрее.
Хочу отметить одну мысль. Внедрение Docker может быть порой избыточным. Если у Вас личный блог, одностраничник или какой-либо другой простой проект, то настройка сервера съест у Вас время и ресурсы, а на выхлопе профита либо не будет, либо он будет отрицательным. Но если это крупный и долгосрочный проект, до Docker скорее всего будет правильным решением. Это как автоматические тесты. Для маленьких проектов они не нужны, тогда как для крупных обязательны.
И еще, я никого не хочу учить плохому. Если я предоставил какую-то ложную информацию, пожалуйста, исправьте меня. Если допустил ошибку, помогите исправить.
DevOps Engineer
=====
SOC for beginners. Глава 3. Использование внешних источников данных об угрозах для Security Operation Center
2017-11-30, 16:03
Пользователь
=====
Встречайте GoLand 2017.3 — новая Go IDE от JetBrains
2017-11-30, 16:21
Пользователь
=====
Запускаем сервис зарплат на «Моём круге»
2017-11-30, 16:31
Помогаем строить карьеру в IT
=====
РусГИС — платформа для сложных задачек с аналитикой и геоданными
2017-12-01, 16:44
User
=====
iOS-meetup SuperJob (видеоотчет)
2017-11-30, 18:28
Пользователь
=====
Как становятся профессионалами в Университете ИТМО: Олимпиада «Я — профессионал», трек «Фотоника»
2017-11-30, 17:26
User
=====
Кто в медицине ИСПДн в соответствие с законом приводил, тот в цирке не смеётся
2017-11-30, 19:37
Корпоративный облачный провайдер
=====
AlphaGo Zero совсем на пальцах
2017-12-04, 11:27
Бортинженер летающих тарелок
=====
Как мы внедряли DevOps: тестирование production-окружения с Azure Web App
2017-12-07, 13:51
Автор
=====
Тестирование LLVM
2017-12-04, 05:01
Программист
=====
Многоканальная атрибуция глазами Calltouch
2017-12-01, 17:45


























































































User
=====
Зачем я купил Mac Mini (Late 2012) накануне 2018 года?
2017-12-05, 21:41

~ Продать MacBook Pro, которому уже перевалило за 3 года
~ Сменить 13'' экран на парочку 22'' и повысить комфорт разработки
~ Получить мощную машину для быстрой компиляции проектов на Swift
Это основные причины, повлиявшие на покупку Mac Mini. Первые два пункта достаточно очевидны, но вот к последнему пункту явно нужны комментарии...
У меня был MacBook Pro 13'' (Mid 2014) с самым мощным процессором в своей линейке (i7-4578U) и 8 GB оперативной памяти. Износ аккумулятора был примерно 1400 циклов и зарядное устройство доживало свои последние дни. Менять батарею и покупать зарядное обошлось бы в 22000₽ у "офицалов" или от 6000₽ в других местах. Но стоило ли продлять жизнь аппарату, который уже не устраивает по производительности?!
За последний год особенно остро встал вопрос скорости компиляции проектов на Swift. Как и многие, я задавался вопросом ускорения компиляции и читал различные материалы на эту тему. Вот, например, полезный скрипт, которым время от времени пользуюсь для поиска "медленных" функций:
Но, несмотря на удачные решения по оптимизации, производительности MacBook не хватало для шустрой сборки проектов.
Первым делом следовало разобраться — а что конкретно требуется от компьютера, чтобы он быстро собирал проекты. Интуитивно я понимал, что основная нагрузка ложится на процессор, но полноценные исследования провести все не хватало времени. Пару раз сравнил время компиляции одного и того же проекта на MacBook Pro 15'' (Mid 2015) с процессором (i7-4770HQ) и на моём ноутбуке. Разница была очевидной — практически в 2 раза быстрее справлялся с задачей MacBook Pro 15'' (Mid 2015). Тогда и заметил, что есть зависимость между временем компиляции и оценкой производительности процессора по версии cpubenchmark.net.
Еще больше подтверждений данной зависимости нашел, когда наткнулся на репозиторий, где люди на одном и том же проекте тестировали время сборки и публиковали результаты.
Все в той же таблице я и обнаружил Mac Mini (Late 2012), который находился среди лидеров. Мне показалось это явление странным, ведь я неоднократно проверял характеристики самого свежего Mac Mini (Late 2014) и показатели его производительности не были впечатляющими.
Как оказалось, у модели 2012 года было все в порядке с производительностью. 4-х ядерный процессор не оставлял шансов 2-х ядерному процессору модели 2014 года. Помимо достойного процессора, этот аппарат еще допускает замену / расширение памяти. Поэтому, must have набор 16 GB RAM + SSD воткнуть получится.

Все происходило через аукцион. От момента зарождения идеи до полной ее реализации прошло всего несколько дней. Удалось разменять старый ноутбук на интересующий комплект без каких-то особых доплат.
Продажа MacBook Pro 13'' (Mid 2014) ≈ 50000₽
Покупка Mac Mini MD388 + Клавиатура + Трэкпад ≈ 25000₽
Покупка 2 x 8 GB RAM Kingston KVR16S11/8 ≈ 10000₽ (Новое)
Покупка 2 x Монитор HP 22w + кронштейн ≈ 15000₽ (Новое)
За сумму ≈ 50000₽ что-то подобное по производительности найти сложно (если вообще возможно). Альтернативой может оказаться Hackintosh, подход к сборке которого достаточно доступно описан в публикации. Однако, в публикации сборка сравнивается с крайне слабым Mac Mini (Mid 2011) c процессором i5-2415M, который покупать для разработки я бы не рекомендовал. Вот если бы сравнили сборку с Mac Mini (Late 2012) с процессором i7-3720QM, то результаты не были бы столь убедительными.
Во время проверки Mac Mini использовал команду для терминала, которая выводит модель процессора, например: Intel® Core(TM) i7-3615QM CPU @ 2.30GHz. Ведь целью было приобретение именно 4-х ядерного аппарата и данная проверка давала хоть какую-то дополнительную уверенность при покупке с рук.
Очень долго не мог решиться какую оперативную память покупать. Существуют модели, которые разные фирмы делают специально для техники Apple, но они дороже и ждать долго пока их доставят. Выбор модулей памяти Kingston KVR16S11/8 был основан исключительно на отзывах людей с маркета. Им огромное спасибо!
Я так и не разобрался, сможет ли этот Mac Mini адекватно выводить картинку на два монитора с разрешением превышающим Full HD. Поэтому не стал рисковать и взял мониторы именно с Full HD.
После смены старого MacBook Pro на еще более древний Mac Mini, объем оперативной памяти увеличился с 8 GB до 16 GB и маленький 13'' экран сменился на два 22''. Осталось разобраться с производительностью.
Предварительно, перед продажей MacBook, было измерено время сборки на одном из проектов: 115 сек, 110 сек, 112 сек. Средний результат получился 112 сек. Условия сборки проекта были всегда точно соблюдены: открыт только Xcode, очистка (clean), ожидание индексации, сборка (build). Результаты Mac Mini подтвердили предположения о зависимости оценки производительности процессора и времени компиляции: 75 сек, 70 сек, 70 сек. Средний результат 72 сек.
Приобретением доволен, все ожидания оправдались. Процессор действительно играет очень важную роль и старые 4-х ядерные оказываются производительнее, чем свежие 2-х ядерные. Не удивлюсь, если MacBook Pro 13'' (Mid 2017) в максимальной комплектации будет собирать проекты медленнее, чем приобретенный мной Mac Mini.
… осталось привыкнуть к отсутствию мобильности и свободы, которую предоставлял ноутбук.
iOS разработчик
=====
Кому и насколько нужно поспевать за прогрессом?
2017-11-30, 22:02
Химик и программист.
=====
Учим машину разбираться в генах человека
2017-12-08, 11:52
Автор
=====
Вышел PHP 7.2.0
2017-12-01, 00:27
Пользователь
=====
О создании пэйлоадов для разных платформ с помощью msfvenom
2017-12-01, 12:41
Пользователь
=====
Учим CSS Grid за 5 минут
2017-12-01, 02:46
Пользователь
=====
Ещё один Telegram-бот для видеонаблюдения
2017-12-01, 02:56
В данной статье мы рассмотрим основные принципы работы telegram-бота для видеонаблюдения.

Прочитав статью о созданном телеграм-боте из подручных материалов, захотелось поделиться с общественностью своим решением для видеонаблюдения.
В отличие от подхода автора указанной статьи, я целенаправленно закупал необходимое оборудование для организации видеонаблюдения в квартире с целью дополнительной безопасности. Я купил IP-камеру которая умела следить за движением в кадре, складывать в папку на ftp и даже отправлять на email. Но всё это было, во-первых, неудобно, во-вторых, в прошивке была ошибка, что отправка на e-mail не работала должным образом, ну и, наконец, было много ложных срабатываний. Поэтому, решено было написать telegram-бота, как самый удобный способ для оповещения и управления наблюдением.
Итак, из железа у меня — камера фирмы IPEYE, которая умеет брать питания через сеть (POE), POЕ-инжектор, небольшой сервачок на Ubuntu Server, интернет с резервированием через USB-модем, ну и UPS для поддержки питания всей системы.
У меня всего одна камера, направленная на входную дверь. В идеале, мне хотелось, чтобы я получал изображение входящего в эту дверь человека. Сначала я пробовал настроить детекцию движения силами самой камеры:

Для этого я написал простенький демон на php, который следит за папкой на предмет появления новых изображений, открывает их и анализирует разницу в освещённости заранее выбранных областей, примерно так (области специально выбраны выше, чтобы человек случайно не закрывал области):

Итак, наш демон определил, что движение в кадре есть и дверь открыта. Дальше надо отправить это изображение в чат telegram. Для этого нам надо зарегистрировать нового бота через BotFather и получить API-ключ. Для взаимодействия с API Telegram я использовал библиотеку longman/telegram-bot. Телеграм предоставляет два способа получения новых сообщений long-polling и веб-хук. Второй способ мне показался предпочтительнее, но для его работы потребуется статичный URL с SSL-сертификатом. Для этого используем letsencrypt или самоподписанный сертификат который надо отправить BotFather. Подробнее о регистрации ботов можно почитать в документации телеграма.
Когда получаешь много сообщений, начинаешь их просто игнорировать, поэтому необходима была некая автоматизация, чтобы оповещения отправлялись только тогда, когда никого нет дома. Сделать это можно очень просто, достаточно просканировать сеть на наличие в ней определённых wi-fi клиентов. Для этого надо установить arp-scan (sudo apt-get install arp-scan), тогда определить что клиент с необходимым мак-адресом подключён можно так:
Здесь я специально указываю конкретный ip-адрес, чтобы не сканировать всю сеть. Но для этого надо зафиксировать в DHCP ip-шник для этого MAC-адреса. Но, в принципе, указывать ip необязательно.

Фотографии это, конечно, хорошо. Но лучше всего покажет входящего человека — видео. Т. к. видео у нас никуда не пишется, "раздобыть" его можно только записав поток. К счаcтью, моя камера предоставляет несколько rtsp потоков закодированных в h264. Чтобы отправить видео, надо записать поток, и отправить его как файл. Для этого воспользуемся avconv (форк ffmpeg в Ubuntu):
Здесь я пишу видео в фоне, после чего перемещаю его в папку для слежения. Демон подхватит новый файл и отправит его как видео. Т. к. звука в видео у меня нет, Telegram автоматически конвертирует его в gif на desktop-клиенте, что очень удобно.
Демон написан на php, и хотя он может работать месяцами, от ошибок никто не застрахован. Поэтому, хорошо бы следить, что демон не завершил процесс. Можно, было бы, конечно, настроить supervisor, но я решил сделать просто автоподнятие по cron-у. Если процесс жив, демон повторно не запускается:
Благодаря всем проделанным действиям, бот оповещает меня о входящем человеке двумя фотографиями и пятисекундным видео. Как бонус, бот также может оповещать о времени ухода и прихода определённых wi-fi клиентов, и находятся ли они сейчас дома.
С результатом вы можете ознакомиться на github.
User
=====
Восемь возможностей C++17, которые должен применять каждый разработчик
2017-12-01, 07:58
Мы поговорим о восьми удобных изменениях, которые влияют на ваш повседневный код. Четыре изменения касаются самого языка, а ещё четыре — его стандартной библиотеки.
Некоторые примеры я брал из докладов на конференциях Russian C++ User Group — за это огромное спасибо её организаторам и докладчикам! Я брал примеры из:
Удобно декомпозировать std::pair, std::tuple и структуры с помощью нового синтаксиса:
В C++17 есть ограничения декомпозиции при объявлении:
Декомпозиция при объявлении в принципе может раскладывать любой класс — достаточно один раз написать подсказку путём специализации tuple_element, tuple_size и get. Подробнее читайте в статье Adding C++17 structured bindings support to your classes (blog.tartanllama.xyz)
Декомпозиция при объявлении хорошо работает в контейнерах std::map<> и std::unordered_map<> со старым методом .insert() и двумя новыми методами :
Пример декомпозиции с try_emplace и декомпозиции key-value при обходе map:
Ключевые правила:
Вы можете создавать свои подсказки для автоматического вывода параметров шаблона: см. Automatic_deduction_guides
Интересная особенность: конструктор из initializer_list<> пропускается для списка из одного элемента. Для некоторых JSON библиотек (таких как json_spirit) это может оказаться фатальным. Не играйтесь с рекурсивными типами и контейнерами STL!
Избегайте вложенности пространств имён, а если не избежать, то объявляйте их так:
Ключевые правила:
В C++ приходится добавлять break после каждого case в конструкции switch, и об этом легко забыть даже опытному разработчику. На помощь приходит атрибут fallthrough, который можно приклеить к пустой инструкции. Фактически атрибут приклеивается к case, следующему за пустой инструкцией.
Чтобы воспользоваться преимуществами атрибута, в GCC и Clang следует включит предупреждение -Wimplicit-fallthrough. После включения этой опции каждый case, не имеющий атрибута fallthrough, будет порождать предупреждение.
В проектах с высокими требованиями к производительности могут практиковать отказ от выброса исключений (по крайней мере в некоторых компонентах). В таких случаях об ошибке выполнения операции сообщает код возврата, возвращённый из функции. Однако, очень легко забыть проверить этот код.
Если вы используете, например, свой класс ошибок, то вы можете указать атрибут единожды в его объявлении.
Иногда программисты создают переменную, используемую только в отладочной версии для хранения кода ошибки вызванной функции. Возможно, это просто ошибка дизайна кода, и возвращаемое значение следовало обрабатывать всегда. Тем не менее:
Правила:
Класс string_view хорош тем, что он легко конструируется и из std::string и из const char* без дополнительного выделения памяти. А ещё он имеет поддержку constexpr и повторяет интерфейс std::string. Но есть минус: для string_view не гарантируется наличие нулевого символа на конце.
Применение optional<> и variant<> настолько широко, что я даже не буду пытаться полностью описать их в этой статье. Ключевые правила:
Пример кода с optional:
Пример кода с variant: здесь мы используем variant для хранения одного из нескольких состояний в случае, когда разные состояния могут иметь разные данные
Преимущество variant в его подходе к управлению памяти: данные хранятся в полях значения типа variant без дополнительных выделений памяти. Это делает размер типа variant зависимым от типов, входящих в его состав. Так может выглядеть таблица размеров на 32-битных процессорах (но это неточно):

Может быть, для манипуляций с байтами лучше опираться на библиотеку GSL (C++ Core Guidelines Support Library).
Ключевые правила:
Чем плох boost::filesystem? Оказывается, у него есть несколько проблем дизайна:
Любой опытный программист знает о разнице в обработке путей между Windows и UNIX-системами:
Конечно же filesystem абстрагируется от подобных различий и позволяет легко работать как с платформо-зависимыми строками, так и с универсальным UTF-8:
Функция std::clamp дополняет функции min и max. Она обрезает значение и сверху, и снизу. Аналогичная функция boost::clamp доступна в более ранних версиях C++.
Правило "не переизобретайте clamp" можно обобщить: в любом крупном проекте избегайте дублирования маленьких функции и выражений для округлений, обрезаний значений и т.п. — просто один раз добавьте это в свою библиотеку.
Аналогичное правило работает для задач обработки строк. У вас есть своя маленькая библиотека для строк и парсинга? В ней есть парсинг или форматирование чисел? Если есть, замените свою реализацию на вызовы to_chars и from_chars
Функции to_chars и from_chars поддерживают обработку ошибок. Они возвращают по два значения:
Поскольку в прикладном коде способ реакции на ошибку может различаться, следует помещать вызовы to_chars и from_chars внутрь своих библиотек и утилитных классов.
Пользователь
=====
Лямбда-потрошитель
2017-12-01, 10:52
Хотя недавно была выпущена Java 9 с новой модульной системой, многие еще продолжают пользоваться привычной восьмой версией, с лямбдами. В течение полугода я плотно работал с ней и всеми ее нововведениями. Если с новыми методами коллекций и Optional все понятно, то с лямбдами не все так очевидно. В частности, как они реализованы и как влияют на производительность. И главное — чем они отличаются от старых добрых анонимных классов.
Для начала было бы неплохо разобраться с типами лямбд. Тут все достаточно просто, есть две разновидности:
Буду действовать по порядку. Посмотрим, как код компилируется с лямбдами. Возьмем самый простой пример, который создает и сразу же вызывает лямбду:
Пока мне хватит стандартной функциональности, встроенной в JDK. Чтобы посмотреть содержимое класс-файла, можно использовать:
javap -p -c -v -constants TestRun.class
Эта команда выведет содержимое методов и constant pool для класса:
В методе main есть всего лишь две инструкции: invokedynamic создает экземпляр некоторого класса, а invokeinterface вызывает метод call() у объекта, который лежит на стеке. Еще в классе есть constant pool, в нем находится описание #2 — метода, для которого будет создана лямбда, и #3 — описание метода интерфейса. Также появился странный метод lambda$main$0(), который мы не заказывали. Но если приглядеться, то он как раз и содержит код лямбды: создает переменную типа Integer и возвращает ее. На него и ссылается структура #2 из constant pool.
Сразу пара ссылок:
 - Спецификация инструкции invokedynamic
 - Описание структуры из constant pool
Этот пример дает больше вопросов, чем ответов. Совершенно непонятно, каким образом вызов интерфейсного метода приводит нас к сгенерированному lambda$main$0(). Для выяснения этого придется залезть в содержимое лямбды.
Чтобы двигаться дальше мне понадобятся спецсредства. Хотелось бы узнать, что находится внутри объектов, у которых мы вызываем метод. Для этих целей можно воспользоваться дополнительным параметром:
-Djdk.internal.lambda.dumpProxyClasses=[dir]
Если его добавить, то во время выполнения получим в папке [dir] прокси классы, которые генерирует фабрика.
Дальше тоже буду двигаться от простого. Сначала разберу пример с лямбдами, которые не содержат ссылок на окружающий контекст:
Код сгенерирует TestNonCapturing$$Lambda$1.class, который очень прост:
Это финальный класс, который работает со статически сгенерированным методом TestNonCapturing.lambda$main$0(). Вызывающий код из main обращается к собственному методу через обертку, которую сгенерирует инструкция invokedynamic во время выполнения.
Теперь посмотрим внутрь лямбд, которые ссылаются на переменные окружения. Для этого достаточно, например, обратиться к переменной метода, в котором создается лямбда:
TestCapturingVariable$$Lambda$1.class будет немного сложнее:
Тут уже появился контекст, у конструктора есть аргумент int var1. Вызывая TestCapturingVariable.lambda$main$0, мы передаем локальную переменную arg$1. Экземпляр лямбды получается через геттер. Почему появился геттер над конструктором — я, честно говоря, не знаю. Полагаю, это детали имплементации в JVM. Если у вас есть ответ на этот вопрос, буду рад его узнать в комментариях.
Попробую немного усложнить пример и добавлю вызов метода экземпляра класса:
Внезапно: Exception in thread "main" java.lang.VerifyError
В данном случае JVM смутило то, что instanceMethod приватный и он вызывается из другого класса. Можно его сделать публичным или добавить –noverify к командной строке. Содержимое TestCapturingMethod$$Lambda$1.class будет следующим:
Как видно из декомпилированного кода, разница небольшая, arg$1 из параметра превратился в экземпляр класса, у которого вызывается метод. В методе call() еще появился автобоксинг.
Теперь более-менее понятно, что находится внутри самих объектов. Попробую разобраться, как это работает и есть ли различия между замыканиями и простыми лямбдами на этом примере:
Тут в цикле генерируются захватывающие и незахватывающие лямбды, затем печатается их хеш. Вывод будет примерно следующий:
Очевидно, что в одном случае каждый раз создается новый объект, а в другом — нет. Похоже, JVM кое-что оптимизировала, и лямбда-фабрика генерирует новые объекты только тогда, когда это действительно необходимо. Вполне логично снова использовать объект, если его содержимое не зависит от окружения. При вызове лямбды, захватывающей контекст, каждый раз будет создан новый объект — и этот случай представляет больший интерес для исследования, т.к. может быть неявно добавлена нагрузка на GC. Но тут оказалось тоже не все так просто.
Просветленный читатель заметит, что, если объект не выходит за рамки метода, то он, скорее всего, попадет под escape analysis, будет создан на стеке и никакой нагрузки на GC не будет. Но кто вызывает лямбды в том же методе, где и создает их? Основная идея здесь: лямбда – это функция высшего порядка, функция, которая принимает на вход или возвращает другую функцию. Таким образом, почти всегда лямбда выходит за границы метода, где она была создана. Любая книга или статья по Java 8 наполнена подобными примерами.
Еще более просветленный читатель заметит, что иногда методы могут быть включены друг в друга JIT компилятором во время выполнения — и тогда escape analysis сработает.
Сразу пара ссылок по теме:
 - Escape analysis
 - Method inlining
Возьмем следующий пример. Если до этого примеры были искусственные, то этот приближен к реальности. В цикле вызывается замыкание, создаваемое в отдельном методе, который можно считать функцией высшего порядка:
Запущу этот код под VisualVM на одну минуту:
Как ни странно, ничего криминального тут нет, хотя на каждый вызов getLambda должен создаваться новый объект. Теперь попробую отключить инлайнинг, добавив параметр -XX:MaxInlineLevel=0. И тут картина сильно меняется:
Почему вначале все было ровно и гладко, а потом поменялось? Когда JIT работал на полную и я не вставлял ему палки в колеса, метод getLambda включался в main, и новый Runnable аллоцировался на стеке метода. Поэтому проблем не возникало. При отключении инлайнинга все стало работать ровно так, как оно выглядит в Java коде, обе оптимизации отключились (inlining, а следом за ним и escape analysis), и появилась нагрузка на GC, т.к. создание объектов перешло со стека на кучу.
В этом примере я искусственно отключил оптимизацию, но, думаю, несложно представить себе следующие ситуации:
Пора подвести итог моему небольшому исследованию. Что удалось выяснить:
 - Есть разные типы лямбда-выражений: хотя синтаксис у них одинаковый, внутри они устроены по-разному и работают тоже по-разному.
 - Достаточно незаметно можно перейти от одного типа лямбд к другому, таким образом изменив нагрузку на GC.
 - Сам по себе вызов метода у лямбды ничем не отличается от вызова любого другого метода, никакой рефлексии тут нет.
И еще пара слов о старых добрых анонимных классах, попробую их сравнить с лямбда выражениями:
 - Анонимный класс генерируется во время компиляции, код лямбды создает фабрика во время выполнения.
 - Генерация кода на лету может быть быстрее, чем загрузка из classpath. Т.к. обращение к classpath может вызвать чтение с диска, некоторые тесты подтверждают, что холодный старт у лямбд быстрее, чем у анонимных классов.
 - Код лямбды помещается в сгенерированный метод того же класса, где она создается. Весь код анонимного класса в нем же и содержится.
 - Анонимные классы обладают явным синтаксисом. Мы точно знаем, что на каждый вызов будет создан один объект. Незахватывающие лямбда тут делают оптимизацию и неявно переиспользуют один объект.
Надеюсь, эта статья помогла приподнять завесу тайны. Понять, как работают лямбда-выражения и что у них внутри. Теперь я вполне осознанно добавляю зависимость замыканий на окружающий контекст, понимая, к чему это может привести. Что дальше делать со всей этой информацией:
 - Если вы разрабатываете приложение, у которого нет жестких требований по производительности, то можно положиться на JIT компилятор. В большинстве случаев он спасает. Но и тут не стоит забывать про такие простые правила, как например, не делать большие методы. Это влияет не только на читабельность.
 - В критическом по нагрузке коде, нужно быть внимательным с лямбдами. Если они вдруг превратятся в замыкания, у этого могут быть последствия. Поэтому:
 - Стоит избегать ссылок на переменные метода или экземпляра класса
 - Ссылаться лучше всего на статические методы
Инженер
=====
DevDay про микросервисы. Запись лучших докладов
2017-12-01, 09:29
Developer Relations
=====
Инди-разработчик начал техподдержку по телефону, и вот что получилось
2017-12-01, 10:31
автор, переводчик, редактор
=====
Сколько стоит CRM-система?
2017-12-01, 11:41
Разработчик
=====
Анонс встречи WordPress Meetup #4 в Харькове
2017-12-01, 11:22
Пользователь
=====
Конструктор XML-строки из PHP-массива
2017-12-01, 11:22
Как часто приходится работать с XML PHP-разработчикам? Не так часто, на самом деле. Обычно потребность возникает при интеграции со сторонним сервисом, такие как BetaPRO, OnTime или CDEK. И вот тут обычно возникает такая ситуация, когда ваш код становится похожим на
и это еще не все! Нужно позаботиться о том, чтобы значения атрибутов и содержимое, заключенное в теги, не содержало спецсимволов, присущие XML. Если для конкретно этого запроса можно быть уверенным, что ничего из спецсимволов сюда не попадет, то контролировать каждый запрос вовсе бы не хотелось. Поэтому через "фильтр" пропускается все. Отсюда следует, что нужно еще "загнаться" с htmlspecialchars или с CDATA, или с XMLWriter, и знать, как это применить и не раз еще "свернуть себе кровь". Как вы видите, времени стоит "убить" достаточно, а результат-то хочется уже сейчас. Эх… А как хотелось бы, чтобы XML можно было бы создавать так же быстро, как JSON: отдал массив, а тебе XML-строку, и никаких заморочек. Опечалившись сложившейся ситуацией я в далеком 2015ом году я решил сделать такой конструктор.
Вашему вниманию представляю xml-constructor для PHP начиная с версии 5.4 и до 7.2 на момент публикации данной статьи.
Для начала использования установим данный пакет через Composer:
Его так же можно просто скопировать вручную куда вы хотите, т.к. пакет не имеет никаких доп. зависимостей, кроме как наличия libxml в самом PHP.
Теперь создадим XML-строку используя PHP-массив:
Результат:
Вот и вся работа! Об остальном позаботится xml-constructor.
И давайте попробуем передать что-то "запрещенное" в значения и посмотрим, как будет вести себя xml-constructor:
Результат:
Создание XML-строки сводится к тому, что нужно передать PHP-массив с нужными ключами и в правильной структуре. Ключей всего четыре:
Каждый элемент массива должен содержать массив с одним ключом tag, как минимум. Ключи attributes, content и elements необязательные.
Первый уровень вложенности есть ничто иное, как корни XML-документа, т.е.:
Результат:
Из конфигурации все только самое необходимое.
Для применения конфигурации нужно передать массив ключ-значение в конструктор первым аргументом:
Результат:
Расширение очень простое и привносит массу удобств во время интеграции с сервисами использующими XML для своего API. Стоит ли использовать xml-constructor — решать только вам.
Спасибо за потраченное на прочтение время!
Продвинутый пользователь ПК
=====
Как при помощи токена сделать удаленный доступ более безопасным?
2017-12-05, 16:06
В 2016 году компания Avast решила провести  эксперимент над участниками выставки Mobile World Congress. Сотрудники компании создали три открытые Wi-Fi точки в аэропорту возле стенда для регистрации посетителей выставки и назвали их стандартными именами «Starbucks», «MWC Free WiFi» и «Airport_Free_Wifi_AENA». За 4 часа к этим точкам подключились 2000 человек.
В результате эксперимента был составлен доклад, в котором сотрудники компании Avast проанализировали трафик всех подключившихся к открытым Wi-Fi точкам людей. Также была раскрыта личная информация 63% подключившихся: логины, пароли, адреса электронной почты и т.п. Если бы не представленный на выставке доклад, то участники эксперимента никогда бы и не поняли того, что кто-то получил доступ к их данным.
Мы подключаемся к сети своей компании из дома, гостиницы или кафе и даже не понимаем, какой ущерб можем ей нанести.
Согласно статистическим исследованиям, более 40 процентов сотрудников компаний работают удаленно хотя бы один день в неделю.
А ведь получается, что сотрудник, работающий удаленно через Интернет, намного уязвимее локального пользователя и представляет потенциальную угрозу для компании. Поэтому безопасности удаленных пользователей необходимо уделять особое внимание.
Удаленное рабочее место пользователя порождает, в сравнении с локальным офисным рабочим местом, три дополнительных фактора угроз:
Поэтому при организации удаленного доступа должны соблюдаться три основных принципа информационной безопасности:
Для организации работы удаленных сотрудников можно использовать следующие механизмы защиты:
Мы расскажем об одном из механизмов защиты — это VPN.
VPN-подключение обеспечивает более безопасное соединение с корпоративной сетью и Интернетом.
Сферы применения VPN:
Сетевая инфраструктура вашей компании может быть подготовлена к использованию VPN с помощью программного или аппаратного обеспечения.
Существует большое количество платных и бесплатных сервисов VPN.
Такие сервисы в основном работают на 4 протоколах:
Также существуют сервисы VPN для корпоративного использования. Один из самых известных — это OpenVPN. Он является безопасным и недорогим сервисом.
Его плюсами являются:
Иногда нет смысла использовать сторонние сервисы, если аналогичные возможности встроены в операционную систему.
Мы хотим продемонстрировать, как настроить безопасное VPN-подключение по протоколу SSTP, используя стандартные возможности Windows.
Защита VPN-соединения в таком случае выполняется посредством механизмов шифрования трафика с помощью цифрового сертификата (SSL), предоставляемого со стороны VPN-сервера. Программное обеспечение клиентской операционной системы в процессе установки VPN-соединения выполняет проверку сертификата VPN-сервера, в частности, проверяет не отозван ли сертификат сервера, а также проверяется стоит ли доверять корневому сертификату Центра сертификации, выдавшего сертификат для VPN-сервера. Именно поэтому одним из требований для успешной работы VPN-подключения по протоколу SSTP, является возможность автоматического обновления списка корневых сертификатов через Интернет.
Протокол SSTP является современным и безопасным протоколом. Дополнительным преимуществом является его возможность работы через повсеместно доступный порт протокола HTTPS (TCP 443), используемый для обычного веб-браузинга, то есть VPN-соединение по протоколу SSTP будет работать практически через любое Интернет-соединение.
Само по себе VPN-соединение является зашифрованным. А вот использование логина и пароля для аутентификации в VPN совсем небезопасно. Но есть выход — это двухфакторная аутентификация. Она позволяет пользователю подтвердить свою личность двумя способами. Желательно использовать для ее настройки аппаратное средство (токен или смарт-карту). Тогда при установке VPN-соединения пользователю потребуется не пароль, а само устройство и его PIN-код.
Основное преимущество аппаратного устройства при использовании VPN — это уникальность закрытого ключа. Она обусловлена тем, что закрытый ключ с устройства нельзя скопировать и воспроизвести. Ведь если средство аутентификации не обладает уникальностью, то нельзя быть уверенным в том, что пользователь, получивший доступ, является тем самым пользователем, которому этот доступ был назначен.
В случае с использованием пароля ситуация совсем другая. Любой человек, специально или случайно узнавший ваш пароль, может им воспользоваться без вашего ведома. А это значит, что он может делать от имени владельца пароля все, что захочет. Отследить такую ситуацию довольно сложно, особенно, если злоумышленник технически подкован.
Настройку VPN-подключения мы начнем с развертывания простого VPN-сервера на базе Windows Server 2012 R2.
Такой сервер, установленный на стандартном оборудовании, вполне можно использовать для небольшой офисной сети с потребностью организации удаленного подключения для нескольких десятков сотрудников (30-50 человек).
Откроем Диспетчер серверов и щелкнем по ссылке Добавить роли и компоненты.
Выберем роль Удаленный доступ.
Выберем службу ролей DirectAccess и VPN (RAS).
Нажмем на кнопку [Установить]. В результате запустится процесс установки роли удаленного доступа.
В окне мастера начальной настройки удаленного доступа выберем Развернуть только VPN.
После этого добавим сервер. В окне Маршрутизация и удаленный доступ выберем пункт меню Действие и подпункт Добавить сервер. Далее подтвердим добавление.
Щелкнем по названию добавленного сервера правой кнопкой мыши и выберем Настроить и включить маршрутизацию и удаленный доступ.
Выберем пункт Особая конфигурация.
В качестве настраиваемой конфигурации укажем Доступ к виртуальной частной сети (VPN).
Запустим службу, для этого нажмем на кнопку [Запустить службу].
Сервер почти готов.
Для примера мы используем самый простой и очевидный способ — зададим статистический пул адресов для 5 пользователей.
Откроем свойства добавленного сервера.
Выберем пункт Статистический пул адресов и нажмем на кнопку [Добавить].
В окне Новый диапазон IPv4-адресов укажем начальный и конечный IP-адрес.
Нажмем на кнопку [Применить]
Роль удаленного доступа сконфигурирована, теперь откроем порты в брандмауэре.
Для протокола TCP откроем порты 1723 и 443.
Для протокола UDP откроем порты 1701, 500 и 50.
На следующем этапе настроим локальную политику безопасности.
Откроем список локальных политик безопасности и выберем пункт Назначение прав пользователя.
Выберите политику Разрешить вход в систему через службу удаленных рабочих столов.
Нажмите на кнопку [Добавить пользователя или группу].
Найдите имя подразделения Пользователи домена и добавьте его.
Ну и предпоследним шагом будет настройка доступа для конкретных пользователей.
Откройте Диспетчер серверов, выберите пункт Средства и подпункт Пользователи и компьютеры Active Directory.
Найдите имя необходимого пользователя, зайдите в его Свойства, на вкладке Входящие звонки выберите настройку Разрешить доступ. Нажмите на кнопку [Применить].
И напоследок проверим разрешен ли удаленный доступ в свойствах системы.
Для этого откроем свойства системы, выберем пункт Настройка удаленного доступа и установим переключатель Разрешить удаленные подключения к этому компьютеру.
Вот и все, настройка сервера на этом завершена. Теперь настроим VPN-соединение на компьютере, который будет использоваться для удаленного доступа.
Настройка VPN на компьютере с Windows 10 предельно проста. Для ее реализации понадобятся данные учетной записи (логин, пароль), IP-адрес сервера и соединение с Интернетом. Для организации аппаратной двухфакторной аутентификации понадобится токен.
Никаких дополнительных программ устанавливать не нужно, в самой Windows уже все есть.
Давай приступим к настройке. В качестве примера аппаратного средства я буду использовать устройство для безопасного хранения ключей и сертификатов Рутокен ЭЦП PKI.
Для настройки подключения нам понадобится сертификат, который содержит политики Smart Card Logon и Client Authentication.
Процесс создания такого сертификата мы ранее уже описывали. Ссылка на описание здесь.
Откроем окно Центр управления сетями и общим доступом. Щелкнем по ссылке Создание и настройка нового подключения или сети.
Откроется окно Настройка подключения или сети. Выберем пункт Подключение к рабочему месту и нажмем на кнопку [Далее].
Щелкнем по ссылке Использовать мое подключение к Интернету (VPN).
В поле Адрес в Интернете укажем данные VPN-сервера.
В поле Имя объекта назначения укажем название VPN-подключение.
Установим флажок Использовать смарт-карту и нажмем на кнопку Создать.
VPN-подключение создано. Но нам необходимо изменить его параметры.
Снова откроем окно Центр управления сетями и общим доступом и щелкнем по ссылке Изменение параметров адаптера.
В окне Сетевые подключения щелкнем правой кнопкой мыши по названию созданного VPN-подключения и выберем пункт Свойства.
Перейдем на вкладку Безопасность и выберем следующие параметры.
Таких настроек VPN-подключения достаточно для того, чтобы успешно подключиться по безопасному VPN-протоколу к указанной сети. Однако после того, как VPN-подключение выполнено, весь сетевой трафик с компьютера будет по умолчанию направляться на шлюз указанной сети. Это может привести к тому, что во время подключения к VPN работа с ресурсами Интернета будет не возможна. Для того, чтобы исключить данную проблему перейдем на вкладку Сеть, щелкнем по строке IP версии 4 (TCP/IPv4) и нажмем на кнопку Свойства.
На странице со свойствами IP версии 4 нажмем на кнопку [Дополнительно].
Снимем флажок Использовать основной шлюз в удаленной сети.
Подтвердим все внесенные изменения. Процесс настройки завершен.
Теперь давайте проверим подключение.
В панели задач на рабочем столе щелкнем по значку Доступ к Интернету и выберем созданное VPN-подключение. Откроется окно Параметры.
Щелкнем по названию VPN-подключения и нажмем на кнопку Подключиться.
Введите PIN-код токена и нажмите на кнопку [ОК].
В результате созданное VPN-соединение будет установлено.
Для проверки статуса VPN-соединения откроем окно Сетевые подключения, найдем название созданного подключения. Его статус должен быть «Подключено».
Чтобы разорвать VPN-соединение в том же окне найдем созданное подключение, щелкнем по его названию правой кнопкой мыши и выберем пункт Подключить/Отключить.
Когда VPN-соединение установлено, весь трафик начинает проходить через VPN-сервер.
Надежность защиты VPN-трафика заключается в том, что даже если злоумышленники каким-либо образом перехватят передаваемые данные, то им все равно не удастся ими воспользоваться, так как данные зашифрованы.
А если установить ещё и специальные приложения для контроля за трафиком и настроить их, то можно будет успешно фильтровать трафик. Например, автоматически проверять его на наличие вирусов.
Надеюсь нам удалось убедить вас, что VPN это просто, доступно, а главное безопасно!
Технический писатель
=====
Greenplum 5: первые шаги в Open Source
2017-12-12, 10:25
User
=====
Мамин архитектор: как мы собирали мобильные архитектуры и радовали родителей
2017-12-01, 17:17
Всем привет! На московской конференции Mobius 2017 мы провели конкурс Мамин Архитектор, в котором просили участников придумать свои оригинальные архитектуры мобильных приложений. В результате мы получили около ста пятидесяти заявок на победу! Под катом мы собрали лучшие работы участников и проанализировали основные тренды. 
А ещё мы адаптировали наш конкурс для проведения онлайн. Вдохновляйтесь и присоединяйтесь!
Осторожно, очень много картинок, 18+!

Задачи, с которыми сталкиваются наши разработчики, в том числе и мобильные, часто очень нетривиальные. У нас достаточно уникальная ситуация для России — одно большое приложение, в которое контрибьютят десятки мобильных разработчиков. Они должны эффективно работать независимо друг от друга, максимально быстро доводить фичи до продакшна, проводить быстрые эксперименты. Это порождает целый класс интересных задач, помимо новых продуктовых фич: оптимизация перфоманса, ускорение релизного цикла, автоматическое компонентное и функциональное тестирование, сбор и анализ технических метрик, проектирование модульной архитектуры и много чего еще. 
Со всеми этими задачами у нашей команды, честно признаться, совсем не осталось времени на изобретение новой собственной архитектуры, поэтому мы вместе с YourDestiny и командой придумали и организовали наш конкурс. На самом деле, мы, конечно, изначально не предполагали делать серьёзный конкурс и рады, что участники поняли нашу иронию и творчески подошли к нему. 
Для участия в конкурсе участникам нужно было визуализировать свою оригинальную архитектуру с помощью наклеек со стандартными блоками, маркеров и других подсобных материалов, и дать ей звучное название. 

Далее работы участников вывешивались на нашем стенде, где слушатели конференции могли оценить их и проголосовать за наиболее понравившуюся. В конце дня мы подвели итоги и наградили лучших призами. Победитель получил электросамокат.

Большинству участников конференции конкурс понравился — как задумка и реализация, так и наши весёлые и общительные стендисты и призы. Но были и те, кому наша механика не зашла: например, гарантированный сувенир получили только первые 50 участников, и это некоторых расстроило. А ещё работ было действительно много, и не все получилось повесить на самые удобные для просмотра места. Мы учтём все пожелания в следующий раз. Ну и теперь — к работам участников! 
Некоторые примечательные архитектуры пришлось снять с публикации, так как они обладали признаками нарушения статей КоАП РФ. И в целом, все опубликованные ниже работы — это творчество их авторов и мы не несем за него ответственности:). 
Какие же основные тренды мы увидели? Немного диванной аналитики.
Более чем в 20 работах участников мы обнаружили признаки ненависти к такой социальной группе, как менеджеры. 
Забегая вперёд, первое место в конкурсе заняла архитектура «Вася работает», автор которой визуализировал то давление и фрустрацию, которые испытывает условный разработчик Василий. 
002. Вася Работает

Разработчики явно противопоставляют себя менеджерам и массово выплеснули эту боль на бумагу. 
126. Еб:)ь код

Целый ряд работ с говорящими названиями вроде «Так просил сделать менеджер», «Manager-oriented architecture», «Manager-driven app», «Я сам», «Crazy manager» намекают на то, что менеджеров лучше не привлекать к вопросам разработки архитектуры мобильных приложений.
041. Так просил сделать менеджер

023. Ожидание / Реальность 

025. Crazy Manager 

100. Manager oriented architecture

Двое участников конкурса откровенно призывали к насилию и сулили менеджерам плохой конец (надеемся, что до этого не дойдёт).
032. Manager Fate

109. Менеджеров на вилы

Мы были крайне удивлены тому, что нелюбовь к менеджерам стала лейтмотивом, видимо, сотрудникам продуктовых компаний в этом отношении сильно проще живётся, чем работникам аутсорса.
В нашем непредсказуемом и быстро меняющемся мире вообще нет места четким упорядоченным структурам, считают 19 участников конкурса, идеал недостижим.
003. Chaos Achitecture

016. 

124. AIW

030. omgmpvv

103. PD

Да и вообще, в реальном мире дела делают как-то так.
027. Реальный мир

028. Manager — Programmer — Result

696. Хренак хренак и в продакшон

108. Singleton

И если всё и так функционирует, то зачем что-то менять? Работает — не трогай!
102. Работает не трогай 

Нейминг — ключ к успеху, так думают около 15 участников нашего конкурса, которые занялись комбинированием элементов с целью получить привлекательные названия. Так мы получили PIVAS, EDA, COVER, RUSLAN, AVITO, VEDRO, PARADICE и другие архитектуры.
056. DRAWER

011. VEDRO

071. PIVAS

017. MAMCA
Когда уже придумал название, а элементы не совсем подходят.

043. RUSLAN

Не обошлось, конечно же, без модификаций уже известных архитектур. Чаще всего тюнили Viper.
006. VIPER-S

121. Clean HH Viper

050. VIDOR-S

35. Fully Reactive MVVM

Ну и какая бы ни была у вас архитектура, не оставляйте джунов без присмотра!
019. Junior пишет крутую архитектуру

033. Все в отпуске, остался только джун

038. JUNIOR STYLE

Авторы некоторых работ крайне художественно подошли к участию в конкурсе, и хотя их нельзя отнести ни к одному из популярных трендов, мы считаем нужным опубликовать эти работы, тем более, что 123 занял второе место в общем зачете. 
010. Код — в радость, билд — в сладость! КРБС

061. ЖигУли

99. Podlodka as architecture 

123. CLYM (carefull like your mama) 

Не будем даже пытаться резюмировать наш конкурс, лишь скажем спасибо всем, кто принял в нём участие: надеемся вы и сами получили удовольствие от процесса, а некоторые — ещё и от призов. 
Тем же, кто пропустил Mobius 2017 в Москве, мы предлагаем почувствовать себя в шкуре маминого архитектора, не выходя из дома. Для этого мы подготовили шаблон в Google Презентациях. Ключевые блоки для создания архитектуры вы найдёте внутри. Делайте копию файла (Файл → Создать копию) и творите! 
Топ-3 участников мы наградим призами. 
Топ-10 участникам отправим наборы наклеек «Мамин архитектор». 

Неважно, где вы живёте — доставим почтой или курьером. 
Крепите свои архитектуры картинками в комментариях, зовите участвовать знакомых и голосуйте. Определяющих критериев два: — количество «плюсов» за ваш комментарий с картинкой и время публикации. Если несколько участников будут соревноваться за призовое место с одинаковым количеством голосов, то приз получит тот, кто опубликовал архитектуру раньше. 
Итоги подведём в следующую пятницу, 8 декабря в 13:00 по Москве. 
Придумай свою мобильную архитектуру, порадуй маму!
Tech
=====
Принудительное введение в системы управления конфигурациями
2018-03-22, 15:32
Abstract: как заставить себя изучить любую из существующих систем конфигураций и перестать редактировать файлы на сервере руками.
Пост посвящён душевной боли, которую нужно преодолеть. Ещё в посте будет чуть-чуть технического, но большей частью пост посвящён борьбе с самим собой, отложенному вознаграждению и тому, насколько моторная память котролирует вас.
Уже многие годы (по айтишным меркам — три поколения как) существуют программы, которые позволяют автоматизировать процесс конфигурации серверов. Все эти программы сложные, они вторгаются в святую святых администраторов и заставляют их делать "всё не так, как раньше". Их изучение и интернализация (признание, что "так надо и так правильно") — абсолютный must have в карьере любого системного администратора.
Главная боль состоит в том, что система управления конфигурациями ломает привычную автоматику пальцев. Раньше вы могли поднять веб-сервер за 2 минуты почти не глядя на экран. Теперь вам предлагают потратить на абсолютно те же самые действия минут 15-20 (если вы хорошо знаете систему управления конфигурациями) или даже несколько дней (!!!!!), если вы её изучаете.
Это преступление против личной эффективности. Уменьшить её в десять (0xA) раз — и это они называют прогрессом?
Вне зависимости от всех остальных аргументов, эта мысль будет преследовать вас всё время. Даже спустя годы. Вы не просто нажимаете больше кнопок для того, чтобы сделать то же самое, но и вынуждены делать это медленнее, вы вынуждены ждать компьютер (никогда раньше вам не надо было ждать десятки секунд пока редактор "отредактирует" файл и перезапустит веб-сервер). Хуже, в момент, когда вы пишите примитивную конструкцию в конфиге, вы будете вынуждены бороться с лишними пробелами в DSL'е (специальном птичьем языке программирования, который надо выучить); думать о всякой невероятно сложной посторонней фигне; делать специальные услуги роботам. Отладка будет хуже и отвратительнее — вместо нормального сообщения об ошибке из-за опечатки в конфиге вы будете получать невнятную oversharing простыню вывода на два экрана, чтение которой занимает больше времени, чем "пойти и сделать вручную".
Ещё хуже: часто эти простыни будут касаться не ваших действий, а совершенно несвязных изменений в других местах проекта. И вы будете вынуждены с ними разбираться. Иногда это будут даже баги системы управления конфигурациями и вы будете чинить/обходить баги вместо того, чтобы заниматься своей прямой работой.
Ну как, я "продал" вам эту технологию? Готовы отбиваться всеми силами от попыток внедрить её на рабочем месте?
Перед тем, как мы продолжим говорить про системы управления конфигурациями, я хочу показать на очень иллюстративный пример из психологии зефирный эксперимент. Для тех, кому лениво читать Википедию, пересказ: детям дают зефирку, и говорят, что если они её не съедят за 15 минут, то им дадут вторую (и обе можно будет съесть). Чем старше ребёнок, тем больше вероятность, что он продержится 15 минут. Малыши не могут себя сдержать и съедают зефирку сразу, хотя потом становится обидно. Этот эксперимент проверяет механизм "отложенного удовольствия".
Именно это и предлагают системы управления конфигурациями. Вы тратите пол-часа на операцию, которую вы можете руками сделать за 3 минуты, а потом эту операцию можно выполнять снова и снова (когда нужно) без затраты трёх минут. И главное, без вовлечения головы. Чаще всего эту процедуру делегируют CI-серверу (jenkins, buildbot, etc), и тогда это открывает дверь для первого шажка к волшебной двери по имени CI/CD.
А этот шажок называется 'staging'. Копия вашего продакшена, которая ничем не занимается. На которой вы можете посмотреть ответ на вопрос "что будет, если я поменяю версию сервера?" и другие смешные эксперименты, не сломав ваш продакшен.
Безусловно, staging можно сделать и без систем управления конфигурациями. Но кто будет следить за тем, что staging похож на продакшен? Более того, кто будет следить, что после вашего смешного эксперимента стейджинг всё ещё похож на продакшен? Если вы делаете смешной эксперимент на результате предыдущего смешного эксперимента, то возможно, результат будет отличаться от того, что получится потом на продакшене.
Этот вопрос "кто будет следить?" на самом деле с подковыркой. Если у вас не гигантский раздутый штат в котором у вас может быть пара человек, которые следят за staging'ом, то ответ на него "никто". Никто не следит. Что же делать?
Ответ: "до основанья мы разрушим," и пересоздадим с нуля. Если в этом процессе от человека требуется больше минуты времени, то, разумеется, никто этого делать не будет. Слишком уныло снова и снова делать то же самое ради исправления "смешного эксперимента".
Обычная мысль "Да чего его переподнимать, я сейчас всё назад верну руками, так быстрее". На выходе — смешной эксперимент и результат его исправления, вместо "копии продакшена".
А вот если есть робот, который "пойдёт и всё сделает сам", вот тогда да, никакой проблемы. Пошёл и сделал.
Если переподнять стейджинг так просто, то почему бы его не поднять в ещё одном экземпляре? Он может быть попроще, в нём не будет какой-то из важных сложных тяжёлых компонент, но нужный кусок, над которым вы работаете прямо сейчас — почему бы и нет?
А ещё он может быть на localhost'е, в виртуалке или контейнере. Что даёт вам практически нулевую латенси при работе (приятно), поддержку оффлайнового режима (полезно). Это потребует чуть-чуть работы с системой управления конфигурациями, но куда меньше, чем может показаться. А дальше — тык-тык и у вас свежий кусок копии продакшена (или даже чего-то специфичного для вашего фичебранча из гита).
После того, как у вас есть написанный процесс превращения нескольких серверов в продакшен и вы можете повторять его снова и снова (малыми усилиями), вы вольны начинать менять кусочки этого процесса (на более удобный/правильный/модный процесс) и смотреть что получилось.
Это требует второй части системы управления конфигурациями — валидации серверов. Об этом мы поговорим чуть позже, но пока сфокусируйтесь на простой идее: вы можете попробовать поменять что-то в процессе конфигурации сервера и посмотреть что получится. За бесплатно. (От себя: когда я не уверен, я иногда запускаю 2-3 версии в параллель на разных стейджингах чтобы выбрать лучший).
Рефакторинг и хранение инструкций для системы управления конфигурациями в гите делает возможным проводить code review (если у вас больше одного человека в команде). code review это круто! Во-первых он дисциплинирует не оставлять кривизны. Во-вторых вы учитесь у друг друга как делать лучше. В третьих это увеличивает взаимное знание о проекте и происходящих в нём изменениях. Развивая линию ci/cd, с некоторыми усилиями можно даже видеть результаты прогона предлагаемых изменений на временной инсталляции — и робот может завернуть pull request просто потому, что он "всё ломает" — no human involved.
Если у нас есть набор инструкций для системы управления конфигурацией, то мы можем проверить результат. Для этого есть масса интересных решений: testinfra, goss, inspec, serverspec, etc. Фактически, эти тесты позволяют проверить результат применения вашей конфигурации. Например, "на 80ом порту слушают", "в мониторинге появилось 180 чеков", "пользователь Х может залогиниться на сервер" и т.д. Когда у вас появляются такие тесты, то вы можете уже сколько угодно играться с процессом — если тесты проходят, то вы всё сделали правильно. Благодаря этому, вы можете пробовать новое (дерзкое) и не бояться неожиданного (например, "ой, я же не подумал, что включение SSL сломает весь мониторинг").
Системы управления конфигурациями напрямую угрожают рабочим местам низкоквалифицированных системных администраторов. Если раньше нам нужно было 20 администраторов, каждый из которых мог управлять 20 серверами, то теперь команда из трёх (чуть-чуть более квалифицированных администраторов) прекрасно может справляться с 400 серверами. На самом деле один тоже может справляться, но 2-3 дают большую компетенцию команде, меньший бас-фактор (концентрацию знаний у единственного человека) и улучшают атмосферу в коллективе благодаря взаимной ответственности за качество работы. И с job security всё просто. Либо вы в списке этих трёх администраторов, либо нет.
На самом деле я чуть-чуть лукавлю и реальность обычно выглядит так: вместо 60 серверов у трёх администраторов у них на руках оказывается 400 (1000? 2000?) серверов, и варианта "нанять ещё 17 администраторов" просто не стоит по бюджетным причинам. Но это особенности растущего рынка и дефицита квалифицированных кадров, а общий аргумент всё равно остаётся: системы управления конфигурациями повышают эффективность труда, и на рынке оказываются более востребованы люди с более высокой эффективностью труда.
При всей позитивности необходимости вышесказанного, любая система управления конфигурациями — всего лишь программа. Это означает, что будут баги. В том числе обидные требующие делать неудобно и некрасиво лишь бы обойти баг. Это означает, что очевидные архитектурные фичи не будут реализованы просто потому, что у программистов иное видение направления движения проекта. Это означает, что вместо части документации будет "Документация" (которая находится в каталоге src). Как любая другая программа, она будет требовать внимания к своему внутреннему миру, уделения себе времени, компетенции (и ещё времени на обучение).
Всё это (включая баги и глубокий внутренний мир) — потребует адаптации мышления под модель, которая диктуется системой управления конфигурации. Управление конфигурациями — крайне инвазивная сущность, которая хочет быть главной во всём, и многие рабочие процессы придётся подстраивать под требования этой программы. Будет множество граничных моментов, когда станет хуже (раньше можно было сделать лучше и проще), будет множество раз, когда будет ощущение выполнения нелепого ритуала вместо нормальной работы.
Но вместе с этим будет и ещё одно ощущение — что стало меньше кнопкодавства и стало больше думания. Вместо "пойти и поменять" будет момент размышления "а правильно ли менять так?", "А почему его вообще надо менять?", будет больше рассуждений об архитектуре (почему на сервере А мы меняем одно значение а на сервере Б другое? Можем ли мы найти что-то общее между этими изменениями и описать это общее как новую сущность?)
Онтологические проблемы будут в полный рост. Размышления над "общим для разных изменений" будут постоянно приводить к появлению новых вещей, а новые вещи требуют имён. Придумывать же имена, как известно, это вторая сложная проблема в IT (первая — инвалидация кеша), и она сложна потому, что придуманное название определяет свойства и ожидания от объекта. И вам придётся каждый раз мучительно придумывать имена. Каждая ошибка — кусок кривизны в архитектуре. Если раньше изменения были "потому что надо поменять", то теперь изменения будут, потому что "так нужно для реализации свойств сепульки". Я старался избегать анекдотических примеров (из жизни), но всё-таки приведу. У меня в одном проекте ушло три недели на то, чтобы придумать имя "системная конфигурация" (для описания той части изменений, которые затрагивают настройки серверов и требуют использования ansible, как противоположность "программной конфигурации", для описания той части, которая не требует вмешательства ансибла). Идея оказалась разумной и помогла разделить невозможно переплетённый комок зависимостей "надо поменять на сервере А но только после того, как пользователь поменяет в интерфейсе Б, но если пользователь поменяет В, то нам А трогать не надо, а если пользователь поменяет Е, то поменяется В и Б, так что нам надо как-то одновременно поменять и конфигурацию сервера, и ту часть, которая ансиблом не конфигурируется". Уф… Давно забытый ужас. Который надо было прочувствовать, продумать и найти имя для отдельной сущности внутри него.
Поскольку кнопки для смены конфига на сервере жмутся всё меньше, а раздумий об абстрактном всё больше, то жизнь постепенно переползает с ssh user@server на git commit.
Это означает, что вслед за системой управления конфигурациями приходит многое из жизни программистов. Структура кода (кода!!), тесты, выразительность и понятность, code review, готовность и устойчивость кода (кода!!) к неожиданным изменениям ТЗ, само ТЗ начинает так или иначе появляться. Появляются issue, которые не "чинить сейчас", а "придумать как устранить", появляется техдолг (сейчас нафигачили, надо переписать), появляется бэклог, появляются well-known bugs. Пример: .., да, да, мы знаем, что сейчас наша конфигурация неправильно настраивает опции кеширования для дисков, но для внесения изменений нам надо сначала переписать кусок, отвечающий за классификацию блочных устройств. Это при том, что нормальный админ уже давно бы сделал hdparm -W 0 /dev/sda на 400 серверах в цикле. Возможно, к 300 серверу он бы понял, что sda иногда — это виртуальный cd-rom и надо на каждом сервере проверять кому атрибут выставляется..., но это уже другая история.
Жизнь админа становится всё меньше похожа на жизнь админа. Профессия разделяется на тех, кто пишет конфигурацию и тех, кто её сопровождает (т.е. остаётся на фронтире, наедине с сервером). Таких иногда называют SRE, и в зависимости от критичности проекта в каждый момент времени, это может быть как очень крутая и важная профессия, так и "апгрейженный саппорт".
Если я вас чуть-чуть мотивировал, но вы не знаете, с чего начать, то начните с поиска системы управления конфигурациями, которая вам по душе. Это Ansible, cfengine, Chef, Juju, Puppet, SaltStack, Quattor, etc. Вероятнее всего, список не полон. Ни одно из этих решений не является хорошим (на мой взгляд), но это лучшее, из того, что у нас есть. Выбор такой системы частично нужно основывать на известных языках программирования (это моё IMHO), ощущении от синтаксиса и жизнеспособности проекта.
После выбора ПО, не стоит бросаться с головой. Читать умные книги по такой системе надо, играться тоже можно до любого уровня сложности, но внедрять стоит очень ограниченно, не теряя контроля над ситуацией. Это означает, что начинать надо с простой автоматизации длительных действий (перепишите свой любимый скрипт деплоя на языке системы управления конфигурацией), попробуйте в него записать хотя бы часть того, что делаете руками.
Одним из интереснейших лайфхаков с системами управления конфигурациями, я считаю лабораторное применение. Когда исследуется новое ПО, то конфигурация этого ПО силами системы управления конфигурациями позволяет "попробовать снова" (если не ясно, делает ли ПО сайд-эффекты по серверу).
Заодно, описание для робота "что делать" отлично подходит на роль эскизного проекта внедрения. Одно время я отлаживал corosync, который не хотел работать в сети с лимитом на unknown unicast/multicast. В ходе отладки мне пришлось несколько десятков раз переподнимать кластер. Когда это делалось ansible'ом, "несколько десятков раз" не стали подвигом, а пришли к чему-то уровня "перезапустить лабораторию".
Ежедневное применение систем управления конфигурациями требует перестройки самых базовых инстинктов работы с серверами, но взамен очень неприятной learning curve даёт на руки не только инструмент для повышения эффективности труда, но и смену модели поведения и мышления на более эффективную. Это очень тяжёлый шаг, но шаг, который нужно обязательно сделать.
оператор ЭВМ
=====
Обзор важных юридических требований платформы Facebook
2017-12-01, 13:12
IT юрист, YouTube блогер, канал Нерезиновая Долина
=====
Поле боя — дополненная реальность. Часть II: как распознать объект и показать 3D модель
2017-12-01, 14:32
Software developer
=====
Почему умирают российские выставки по кибербезопасности
2017-12-03, 13:49
Product Manager
=====
Отчет со встречи Android Devs Meetup 22 сентября
2017-12-01, 15:16
Пользователь
=====
Dagger 2 для начинающих Android разработчиков. Внедрение зависимостей. Часть 2
2017-12-01, 15:37
Android devepoler
=====
Must-have документация для мобильного разработчика. Часть 1
2017-12-04, 12:25
 В первой части мы расскажем о том, почему важно уделять внимание техническому проектированию и опишем необходимый минимум документации, которая позволит создать «скелет» проекта на основе пользовательского интерфейса.
Данная статья является сокращенной версией руководства, доступного по ссылке в конце статьи.
Ключевую идею, с которой мы начнем, можно коротко сформулировать следующим образом: 
При разработке технической документации на проект это стоит обязательно учитывать, так как интерфейс нагляден и на его основе проще проводить разделение проекта на разделы. Да и сама модель предметной области очень хорошо описывается интерфейсом — в ней необходимо учитывать в основном те данные (и их производные), которые вводятся пользователем, отображаются на экране и управляют его поведением. Бизнес-сценарии также напрямую завязаны на поведение пользовательского интерфейса. 
В то же самое время большинство ТЗ готовится для бизнес-заказчиков и описывают не конкретные экраны или сервисы, а целые бизнес-сценарии и блоки функциональности. В дальнейшем эта документация и спецификации дизайна используются командой разработки. Для кодирования и последующей реализации используются многократные перечитки и пересказы ТЗ. 
В следующих главах мы опишем минимально необходимый набор документов, которые позволят команде использовать простые чек-листы для контроля реализации.
Прежде чем мы перейдем к разбору артефактов и извлечению из них полезных данных, давайте рассмотрим весь процесс разработки целиком. Для простоты мы выберем линейный процесс разработки, так как при использовании циклических или спиральных методологий возникают те же самые классы задач, только последовательность их выполнения может отличаться.

Итак, у проекта обычно выделяют следующие производственные классы задач:
Можно выделить и больше, но они по факту будут являться производными от обозначенных.
На этапе аналитики производится поиск решения, описание общих требований к приложению. На выходе с этапа аналитики появляются спецификации, которые являются вводными для этапа проектирования. 
Так как наше руководство предназначено в первую очередь для разработчиков, то считаем, что бриф или базовое ТЗ у вас есть.

Дальше начинается самое интересное – проектирование пользовательского интерфейса. Этот этап является ключевым и при правильном подходе очень сильно облегчает и упрощает процесс разработки. Если же данный этап пропущен, то дальше успех проекта будет зависеть только от опыта команды. 
На этапе проектирования самым важным является продумывание пользовательского интерфейса и создание схем экранов. 
Если начинать сразу с дизайна (вместо схем экранов), то придется постоянно переделывать его (дизайн) для согласования с заказчиком. На этапе проектирования это сильно замедлит процесс. Дизайн фактически является производным от UX и наполняет изначальные схемы эмоциями, выправляет композицию, добавляет анимации и другие аспекты внешнего вида и визуального поведения. Схемы экранов, в свою очередь, создают структуру приложения и моделей данных — какие данные отображать, как они будут сгруппированы, как они будут влиять на поведение интерфейса.

На выходе с этапа проектирования будет получен комплект необходимых спецификаций и ресурсов, которые вместе с ТЗ уйдут разработчику. Этап кодирования разумно начинать с построения фундамента – базовой структуры проекта, в чем нам очень поможет понимание ключевых пользовательских сценариев.
Напомним еще раз, что мобильные приложения это в первую очередь пользовательский интерфейс, поэтому и проектирование лучше начать со схем экранов и последовательности переходов между ними. Это необходимо для того, чтобы определить набор шагов, которые предстоит пройти пользователю для получения желаемого результата. Ведь бизнес-приложение создается для определенного набора ключевых сценариев (последовательности действий пользователя), с помощью которых человек может решить свои задачи.

Так как среднее время контакта человека со смартфоном составляет всего несколько минут, то количество шагов в бизнес-приложениях не должно быть большим — для пользователя в первую очередь важно получить результат (выполнить задачу или удовлетворить потребность) за время контакта с устройством. Для сложных приложений с большим количеством функциональных возможностей следует учитывать этот фактор. Хорошим выбором станет разделение приложения на относительно короткие сценарии не более 10 шагов каждый. 

Для того, чтобы определить глубину ключевых сценариев, можно использовать карту переходов и состояний, подробнее о которой будет рассказано в следующих разделах. Но для начала требуется привести в порядок структуру интерфейса.
Итак, у нас на руках есть схемы экранов от проектировщиков/дизайнеров и последовательность переходов между ними. 

Для того, чтобы разбить приложение на части (разделы предметной области), мы пойдем от экранов. Еще раз напомним, что мобильное приложение это в первую очередь интерфейс взаимодействия с пользователем, поэтому наши экраны и являются прямым отражением доступной ему модели предметной области.

Первым делом необходимо выделить экраны, которые связаны между собой, обычно они должны идти друг за другом в пользовательских сценариях. Например, часто в приложениях можно выделить раздел Account с просмотром и редактированием всей информации, связанной с профилем пользователя. 

Если вы опытный программист, то легко справитесь с разделением списка экранов на связанные разделы. В любом случае, потребуется немного практики.
Итак, у нас могут получиться следующие разделы:
Каждый раздел должен иметь название и номер. Названия разделов следует использовать для горизонтального разделения слоя работы с данными, бизнес-логики и пользовательского интерфейса. Это позволит в дальнейшем проще развивать проект.
Например, слой работы с данными (группы методов для различных серверных API) в этом случае разделится на разделы (репозитории, если вам так привычнее), каждый из которых будет обслуживать свой набор экранов:
DAL\DataServices (Repositories)
AccountDataService.cs (или AccountRepository.cs)
HelpDataService.cs
CheckoutDataService.cs
CatalogDataService.cs
В дальнейшем каждый из репозиториев может полностью скрывать всю работу с сервером, дисковым кешем и локальной СУБД. Это позволит на уровне бизнес-логики работать с репозиториями как с черными ящиками.
Дальше предстоит пронумеровать и назвать экраны (страницы, окна). На выходе у нас получится древовидная (хотя и плоская) структура интерфейса без учета последовательности переходов между экранами и их вложенности.

Имена экранов будут использоваться у нас в названиях классов для соответствующих страниц (Page) и ViewModel (или Controller для MVС):
1.1 Profile
ProfilePage
ProfileViewModel
1.2 EmailLogin
EmailLoginPage
EmailLoginViewModel
В первую очередь это важно для разработчиков, которые фактически получают готовую структуру проекта:
Как видим, уже вырисовывается скелет проект. Слой DAL можно легко выделить в отдельную библиотеку. Если же у вас используется типовая архитектура или шаблон проекта (Base-классы, NavigationService, и т.д.), то считайте, что костяк приложения у вас уже имеется.
Пример структуры проекта представлен ниже.
UI (User Interface, пользовательский интерфейс)
    \Pages
        \Account
            ProfilePage.xaml
            ...
BL (Business Logic, бизнес-логика)
    \ViewModels
        \Account
            ProfileViewModel.cs
            ...
DAL (Data Access Layer, доступ к данным)
    \DataObjects (Models)
        ProfileObject.cs (ProfileModel.cs)
        ProductObject.cs
        ...
    \DataServices (Repositories)
        AccountDataService.cs
        ...
Для того, чтобы дальше реализовывать поведение экранов, нам потребуется дополнительная информация, поэтому продолжим знакомство с необходимыми артефактами.
После того, как у нас есть схемы экранов, мы также можем провести их анализ до фактического старта разработки. Следующим полезным артефактом для нас станет таблица экранов, оперировать которой будут не только разработчики, но и тестировщики. В сводной таблице легко собрать воедино всю текстовую информацию. Ключевыми столбцами таблицы для нас станут:
1. Номер экрана
2. Краткое название (Name)
3. Список возможных состояний (States)
4. Поля ввода для валидации (Validation)
5. Описание экрана и его поведения (Behavior)
Как видим, в представленном наборе полей собрана та информация, которая позволит корректно проверить работу каждого экрана в отдельности. Можно также каждому разделу присвоить свой цвет — это упростит работу с картой переходов и состояний.

Дополнительно, в эту таблицу могут быть добавлены следующие столбцы:
 6. Список всплывающих уведомлений (alerts, sheets, dialogs)
 7. Идентификаторы UI-контролов (например, LoginButton) для написания автоматизированных UI-тестов
 8. Используемые модели (Models/Data Objects) данных
 9. Используемые на каждом экране методы DAL
 10. Используемые стили (Styles)
О каждом экране по столбцам достаточно просто вписывать короткие обозначения, которые в дальнейшем будут использоваться в программном коде и понятны в первую очередь разработчикам. Кроме столбца Behaviour (Описание экрана и его поведение), здесь ограничений лучше не делать.
Отдельно остановимся на состояниях экранов. Большинство современных приложений работает через Интернет, поэтому стоит корректно отображать пользователю информацию о состоянии загрузки:

Хорошей практикой считается, если каждый экран, загружающий данные из сети (или из локальной СУБД) будет корректно отображать пользователю каждое из описанных состояний. Фактически, отдельное состояние описывается своим набором визуальных элементов (тексты, картинки, кнопки, другие элементы), и на уровне программного кода легко управлять переключением из одного состояния в другое. Также можно фиксировать негативные сценарии (ошибка загрузки, пустые данные) для дальнейшего анализа и устранения на стороне сервера или приложения.
Можно взять за правило и на всех экранах, загружающих данные, добавлять переключение состояний. Это упростит взаимодействие пользователя с приложением. Можно также использовать различные анимации или графику в негативных состояниях (ошибки, пустые данные), чтобы сгладить эффект. 
Итак, у нас уже есть схемы экранов, список Page и ViewModel, а также детальная информация по каждому экрану. Каркас приложения можно построить, однако сейчас у нас экраны описаны независимо друг от друга и нет четкой и понятной последовательности переходов. Поэтому следующим полезным артефактом для нас станет карта переходов и состояний. 
Для того, чтобы лучше понять основные пользовательские сценарии, а также обозначить связи между экранами, можно использовать карту переходов и состояний. Плюсами карты являются ее компактность и наглядность. Даже для больших проектов карту переходов можно распечатать на принтере А4 и повесить над рабочим столом.
Итак, карта переходов начинается с точки старта – момента запуска приложения пользователем. Точек старта может быть несколько, например, один вариант запуска для авторизованного пользователя, второй – для неавторизованного, а третий – из Push-уведомления. 
Дальше добавляются прямоугольники для каждого экрана и обозначаются стрелками последовательности переходов. Можно добавить идентификаторы (AutomationId) кнопок или событий, из-за которых произошел переход, и для наглядности еще указать данные, которые будут передаваться на новый экран. 
Также у нас уже есть таблица экранов (предыдущая глава), где обозначены возможные состояния каждого из них и которые необходимо отобразить на карте переходов. Это позволит лучше понять возможные прерывания пользовательских сценариев, например, в случае ошибок или пустых данных. Если состояние прерывает (человек не может идти дальше) пользовательский сценарий, то обозначаем его минусом «-», если не прерывает, то плюсом «+». Стрелочки «назад» можно не добавлять.

Как видим, теперь у нас имеется практически вся необходимая для разработки информация в компактном виде. Эти три онлайн-документа (список экранов, таблица экранов, карта переходов) могут обновляться по мере развития проекта. 
Для создания описанных выше артефактов нам будет достаточно 3 онлайн-инструментов:
На подготовку каждого из артефактов уходит не больше одного дня, зато в дальнейшем это очень сильно упрощает процесс разработки, тестирования и развития продукта. За время медитативной подготовки документов и схем команда глубже понимает проект целиком и может уже финально оценить сложность и длительность его разработки (цифры для внутреннего использования).
Продолжение следует обязательно прочитать...
User
=====
Must-have документация для мобильного разработчика. Часть 2
2017-12-05, 10:07
Данная статья является сокращенной версией руководства, доступного по ссылке в конце статьи.
В современных приложениях с пользовательским интерфейсом широко применяются таблицы стилей. В инструментах разработки (и в нативных и в кроссплатформенных) этот механизм также реализован, поэтому до старта кодирования лучше сделать единую таблицу стилей, так как в противном случае есть высокий риск взрывного увеличения их количества. Когда каждый разработчик придумывает свои имена для внешне одинаковых элементов пользовательского интерфейса. Чтобы этого избежать, необходимо заранее подготовить если и не описания, то хотя бы названия стилей с привязкой к дизайну.

Сама по себе информация по стилю должна приходить от дизайнера (например через сервис Zeplin.io или Sketch). Для нас же важен на текущем этапе не просто набор свойств для каждого типа объекта (цвет шрифта у текста, задний фон у страницы и так далее), а их связь со страницами. Необходимо для всех объектов на экране указать названия связанных стилей. 

На основе этой таблицы можно сразу описывать необходимые стили еще на ранних этапах проекта, что позволит на этапе разработки сфокусироваться на компоновке, а не стилизации, элементов интерфейса.
Следующим шагом мы рассмотрим ту функциональность, которая может быть скрыта от пользователя.
Отдельно на этапе технического проектирования необходимо выделить фоновую (скрытую) функциональность вроде механизмов работы с СУБД, кешера, обработчика Push-уведомлений, фоновой синхронизации или сервиса управления корзиной. Данная функциональность просто не должна ускользнуть на этапе старта работ, поэтому ее лучше описать отдельно.
В каком виде все это описывать — решать вам, но это не мешает нам указывать названия фоновых сервисов (реализуют нужную функциональность), которые мы в дальнейшем будем использовать в коде. Например:
BL\Services\CartService
Фоновый Singleton-сервис синхронизации корзины. Запускается при старте приложения. Должен поддерживать работу в многопоточном окружении с очередью сообщений об изменении корзины.
BL\Services\CatalogCacheService
Реализовать табличное хранилище в виде Singleton-сервиса для временного кеширования списка товаров и информации о продавце между экранами.
DAL\DataServices\SyncDataService
Реализует механизмы поэтапной синхронизации данных. Должен поддерживать работу в ограниченном окружении нативного фонового запуска.
Как мы уже отмечали в 1 статье, все приложения создаются для решения определенных пользовательских задач. То есть фактически это набор переходов между экранами и действий пользователя (нажатие на кнопку, выбор элемента и т.д.) на них. Поэтому и наши сценарии мы также привяжем к экранам и элементам пользовательского интерфейса.

Как видим, в сценарии указаны элементы пользовательского интерфейса, что позволит в будущем проще переложить их на автоматизированное UI-тестирование.
Теперь у нас есть и вся необходимая информация как для разработки, так и тестирования (ручного и автоматизированного).
Итак, помимо первоначального ТЗ и дизайна мы также получили набор дополнительных артефактов.

Каждый из артефактов требует относительно немного времени на создание и не требует сложных знаний и опыта. Вместе они полностью описывают интерфейс приложения и весь пользовательский опыт. Данные документы могут быть созданы даже специалистами средней квалификации, а их использование очень сильно упрощает и структурирует дальнейшую разработку. 
В независимости от того, будете ли вы использовать простые онлайн-документы или специальную систему управления требованиями, следует обновлять документацию при каждом крупном релизе. Стоит контролировать, чтобы указанные в документе названия использовались всей командой при написании кода, скриптов и планировании работ (например, дробить задачи по разделам).
Небольшой объем (мало букв) документации позволяет использовать ее в качестве чек-листа во время разработки. Очень полезным для понимания проекта оказывается карта переходов и состояний, поэтому ее можно распечатать в бумажном виде и делать пометки прямо на листе.


При создании нового приложения предполагается, что у вас уже есть базовые классы и инфраструктура (сервис для навигации, механизмы работы с СУБД и REST API), которые вы будете использовать при создании скелета проекта:
Шаг 1. Создаем пустое приложение, создаем в нем инфраструктурные папки и классы.
Шаг 2. Добавляем подпапки для каждого раздела для ViewModels, Pages. Добавляем пустые классы ViewModels и Pages.
Шаг 3. Добавляем переходы между экранами и их состояния на базе карты переходов.
Шаг 4. Добавляем пустые DataServices и DataObjects на основе таблицы экранов. 
Шаг 5. Добавляем и реализуем все стили.
Шаг 6. Реализуем DataServices, возможно для начала и с Mock-данными (тестовыми из локальных JSON/CSV-файлов).
Шаг 7. Реализуем заглушки для фоновой функциональности.
Все, скелет готов! Данные подготовительные работы займут у команды 1-2 дня, но позволят получить полностью рабочий скелет проекта, уже соответствующий документации, вашей архитектуре и навигационной моделе. Дальше останется поделить работы по модулям, разделам и экранам, а в каждом конкретном случае удерживать в голове только ту информацию (несколько абзацев текста, скриншот, обозначения из кода), которая необходима в настоящий момент. Напомним, что данный сценарий эффективен только в том случае, если у вас уже имеется базовая инфраструктура.
При рефакторинге проекта можно ограничится подготовкой документации и легким косметическим переименованием. Это позволит команде получить единые артефакты и обеспечит преемственность специалистов.
При старте длительных проектов необходимо уделять внимание подготовке технической документации для всей команды, а не только заказчика. В нашем руководстве мы описали простой набор артефактов, который, с одной стороны позволяет взглянуть на проект целиком (карта переходов), а с другой — получить только самую необходимую информацию по каждому экрану или модулю (таблица).
Итак, краткое описание каждого артефакта:
Надеемся, что описанный подход позволит упростить работу вашей команды с документацией и позволит быстрее создавать качественные мобильные приложения. 
User
=====
Python’ом по машинлернингу
2018-03-19, 09:51
Сегодня только ленивый не говорит (пишет, думает) про машинное обучение, нейросети и искусственный интеллект в целом. Всего лишь в прошлом году ML сравнили с подростковым сексом — все хотят, но никто не занимается. Сегодня все озабочены тем, что ИИ нас оставит без работы. Хотя, судя по последним исследованиям Gartner, можно успокоиться, так как к 2020 году благодаря ИИ появится больше рабочих мест, чем ликвидируется. Так что, дорогой друг, учи ML, и будет тебе счастье.

Примечание: мы продолжаем серию публикаций полных версий статей из журнала Хакер. Орфография и пунктуация автора сохранены.
В этой статье мы хотим показать ML на практическом кейсе — на примере проекта, который мы делали для Актион-пресс (сервис онлайн-подписки). Уверен, описанное в этом примере может пригодиться многим. Почему многим? Да потому, что проблема, которую мы решали, называлась «сортировка и пересылка по адресу огромного количества электронных писем». Проблема гигантской переписки, которую менеджерам приходится сортировать и пересылать в соответствующие отделы, практически универсальная, и проблему эту надо решать современными способами. 
Итак, после консультаций с заказчиком мы решили разработать модель машинного обучения для максимально возможной автоматизации сортировки писем.
Думаю, тебя не удивит, что в качестве языка для этого решения мы выбрали Python. Так сложилось исторически, он высокоуровневый, а самое важное — в нем есть множество полезных библиотек для машинного обучения. Про них я расскажу ниже.
Честно говоря, про ML в данном случае рассказывать особо нечего. Набор простых бинарных классификаторов на основе логистической регрессии показал многообещающие результаты и позволил несколько абстрагироваться от самой модели, сосредоточившись на подготовке данных и построении вложенного текста. Но сам репозиторий уже использовался как основа для трех других независимых проектов, он хорошо показал себя в нескольких классификационных экспериментах и зарекомендовал себя как надежный фундамент для быстрого перехода к разработке. Поэтому задача данного раздела заключается не в демонстрации «ноу-хау», он нужен как основа для следующего за ним раздела, посвященного операционализации. 
Здесь я поделюсь своим опытом и дам некоторые рекомендации тебе, чтобы ты мог сам поэкспериментировать с этим кодом или повторно использовать его.
Чтобы сохранить конфиденциальность, исходный набор данных был заменен аналогичным общедоступным набором для классификации отзывов о McDonalds. См. файл data/data.csv.
Сами данные были представлены в файлах CSV с тремя столбцами: Id, Text и Class. И поскольку в NLTK не предусмотрена встроенная поддержка чтения данных из файлов в формате CSV, мы написали собственный модуль, позволяющий читать файлы из папки в виде одного dataframe pandas или извлекать текст в виде списков абзацев, предложений, слов и так далее в формате NLTK. 
А вот код для инициализации этого самописного модуля чтения CsvCorpusReader данными клиента. Реализацию класса можно увидеть в файле lib\corpus.py. Настоятельно рекомендую тебе ознакомиться с содержимым файла Experiments\TrainingExperiment.py.
По окончании инициализации нужно извлечь слова из документов и нормализовать их. В нашем случае после ряда экспериментов мы решили использовать набор вспомогательных функций в качестве обертки для всего процесса, чтобы скрыть обращения к библиотекам NLTK и Gensim внутри простого в использовании уровня конфигурации. 
Ниже мы даем экстрактору команду вернуть документы в виде списка слов, отбрасывая структуру абзацев или предложений (см. keep_levels=Levels.Nothing). Затем переводим каждое слово в нижний регистр, отбрасываем любые стоп-слова и выделяем основы слов. На заключительном этапе удаляем низкочастотные слова, предполагая, что это просто опечатки или что они не оказывают существенного влияния на классификацию. 
Обрати внимание, что приведенный ниже код ориентирован исключительно на выборки данных на английском языке, в то время как в оригинальной версии была реализована русская лемматизация с использованием PyMorphy2, что позволило добиться более точной классификации для русского языка.
Как только мы токенизируем наш корпус, следующим шагом будет построение вложения. Код ниже нужен, чтобы преобразовать каждый документ в ряд бессмысленных цифр для использования в классификаторе.
Мы протестировали несколько разных подходов (включая BoW, TF-IDF, LSI, RP и w2v), но классическая модель LSI с 500 извлеченными топиками дала наилучшие результаты (AUC = 0,98) в нашем случае. Для начала код проверяет наличие существующей сериализованной модели в общей папке. Если модели нет, код обучает новую модель с использованием предварительно подготовленных данных и сохраняет результат на диск. Если модель обнаружена, она просто загружается в память. Затем код преобразовывает набор данных и повторяет поток со следующим вложением.
По эффективности модель LSI превзошла гораздо более мощный алгоритм векторизации на базе word2vec и другие более сложные подходы, и это может быть обусловлено несколькими возможными причинами.
Самая очевидная из них связана с тем, что письма тех типов, которые мы искали, имели предсказуемые и повторяющиеся шаблоны слов, как в случае автоответов (например, «Спасибо за ваше письмо… Меня не будет в офисе до… Если вопрос срочный...»). Поэтому для их обработки вполне достаточно чего-то простого, например TF-IDF. LSI поддерживает общую идеологию, и эту модель можно рассматривать как способ добавления синонимов, подходящих для обработки. В то же время алгоритм word2vec, прошедший обучение на Википедии, вероятно, генерирует ненужный шум из-за сложных синонимичных структур, тем самым «размывая» шаблоны в сообщениях и, следовательно, снижая точность классификации. 
Этот подход показал, что старые и довольно простые методы по-прежнему стоит пробовать, даже в эпоху word2vec и рекуррентных нейронных сетей.
Как всегда, от обязательного рутинного кода избавиться невозможно. Дальше он нам пригодится при подготовке данных для машинного обучения с применением skit-learn. 
Как я уже говорил выше, мы используем несколько бинарных вместо одного многоклассового классификатора. Именно поэтому создаем бинарную цель для одного из классов (в этом образце это SlowService). Ты можешь изменять значение переменной class_to_find и выполнять приведенный ниже код повторно, чтобы обучить каждый одноклассовый классификатор в отдельности. Скрипт оценки сделан так, чтобы работать с несколькими моделями, и автоматически загружает их из выделенной папки. Наконец, формируется обучающий и тестовый набор данных, строки с пропусками при этом полностью исключаются.
Теперь приступаем к обучению классификатора (в нашем случае это логистическая регрессия), потом сохраним модель в том же общем каталоге, который использовали ранее для встраивания преобразований. 
Как ты мог заметить, в приведенном ниже коде мы придерживаемся специального формата имени модели: class_{0}_thresh_{1}.bin. Это необходимо для определения имени класса и соответствующего порогового значения в ходе дальнейшей оценки.
И последнее замечание, прежде чем мы продолжим. В качестве инструмента разработки я выбрал Visual Studio Code. Это простой в использовании легковесный редактор, который даже предоставляет базовые возможности IntelliSense (автозавершение кода и подсказки) для такого динамичного языка, как Python. В то же время расширения Jupyter и Python в сочетании с ядром IPython позволяют выполнять код поячеечно и визуализировать результат без повторного запуска скрипта, что всегда удобно для задач ML. Да, это похоже на стандартный Jupyter, но с IntelliSense и ориентацией на код/git. Я рекомендую тебе попробовать, хотя бы пока ты работаешь с образцом, поскольку для продуктивной разработки тут применяется множество других возможностей, связанных с VS Code.
Что касается кода ниже, строка с plot ROC threshold values — это примеры использования расширения Jupyter. Ты можешь нажать специальную кнопку Run cell (Выполнить ячейку) над ячейкой, чтобы увидеть значения TP и FP и сравнить их с пороговым значением Threshold на панели результатов справа. Мы активно использовали эту диаграмму во время работы, поскольку из-за выраженного дисбаланса в наборе данных оптимальный уровень отсечки всегда был около 0,04 вместо привычных 0,5. Если ты не можешь использовать VS Code для тестирования, можно просто запустить скрипт с помощью стандартных инструментов Python и после просмотра результатов в отдельном окне внести изменения непосредственно в имя файла.
Теперь настало время скрипта оценки: Score\run.py. Нового в нем очень мало, большую часть кода взяли из первоначального обучающего эксперимента, рассмотренного ранее. Ознакомься с содержимым этого файла в репозитории GitHub.
На вход подается файл CSV для оценки, на выходе получаем два разных файла, один содержит оцененные классы, другой — идентификаторы строк, оценить которые не представляется возможным. Я объясню причину использования файла позже, когда будем говорить об операционализации.
В конце этого раздела хочу пояснить, почему мы используем несколько бинарных вместо одного многоклассового классификатора. Во-первых, так было гораздо проще начать, чтобы работать и оптимизировать производительность на классах по отдельности. Такой подход также позволяет использовать различные математические модели для разных классов, как в случае с автоответами, которые часто имеют довольно жесткую структуру, и их можно обрабатывать с помощью простого bag of words. В то же время, с точки зрения ИТ-специалиста, что-то наподобие кода ниже может упростить развертывание, позволив подключать новые или менять существующие модели, не затрагивая другие.
Ты даже можешь опробовать код прямо сейчас, используя собственные данные со своего локального ПК, и совсем без операционализации: 
В VS Code ты даже можешь открыть раздел отладки Debug (Ctrl + Alt + D), выбрать Score (Python) в качестве конфигурации и нажать Start Debugging (Выполнить отладку), чтобы провести построчный анализ кода в редакторе. Когда алгоритмы завершат свою работу, результаты можно будет найти в файлах input.scores.csv и input.unscorable.csv в папке Score\debug.
Поддержка Python в Azure Functions до сих пор находится в раннем preview, поэтому использование его для mission critical задач нежелательно. Но часто ML к таким не относится, а потому удобство реализации может перевесить сложности с адаптацией предварительной версии.
Итак, на этом этапе у нас было два скрипта. Скрипт Experiments\TraintExperiment.py обучает модель, затем преобразованную и обученную модель он сохраняет в общий каталог, и, как предполагается, этот обучающий скрипт перезапускается на локальной машине по мере необходимости. Скрипт Score\run.py выполняется ежедневно, он сортирует новые электронные письма по мере поступления. 
В этом разделе мы поговорим об операционализации процесса с помощью Azure Functions. Функции просты в использовании, они позволяют привязать скрипт к множеству различных триггеров (HTTP, очереди, BLOB-объекты хранилища, WebHooks и так далее), предоставляют несколько автоматических привязок вывода и при этом стоят недорого: выбрав план Consumption, ты платишь всего 0,000016 доллара за каждый используемый гигабайт ОЗУ в секунду. Но здесь есть ограничения: твоя функция не может выполняться дольше десяти минут и использовать более 1,5 Гбайт ОЗУ. Если тебя это не устраивает, ты всегда можешь перейти на специальный тарифный план на базе App Service, сохранив при этом доступ к другим преимуществам serverless-подхода. Однако для нашей простой логистической регрессии и пакетов из нескольких сотен писем выбранный план оказался оптимальным.
С точки зрения программиста, функция — это папка, которая носит имя самой функции (в нашем случае это просто Score) и содержит два разных файла:
Function.json можно создать вручную или сконфигурировать средствами портала Azure. Код, который мы получили в данном случае, представлен ниже. Первая привязка — inputcsv — запускает скрипт каждый раз, когда файл с именем, соответствующим шаблону mail-classify/input/{input_file_name}.csv, появляется в выбранном по умолчанию BLOB-хранилище Azure. Оставшиеся две привязки сохраняют выходные файлы после успешного выполнения функции. В данном случае мы сохраняем их в отдельную папку output, их имена соответствуют имени входного файла с суффиксами scored или unscorable. Таким образом, ты можешь поместить файл с любым именем-идентификатором, например GUID, в папку input, и два новых файла с именем, производным от GUID, через какое-то время появятся в папке output.
Скрипт run.py для функций Azure практически аналогичен нашей первоначальной «неоперационализованной» версии. Единственное изменение касается того, как функции пропускают через себя входящие и исходящие потоки данных. Независимо от выбранного типа входных и выходных данных (HTTP-запрос, сообщение в очереди, BLOB-файл...) содержимое будет храниться во временном файле, и путь к нему будет записан в переменную среды с именем соответствующей привязки. Например, в нашем случае при каждом выполнении функции создастся файл с именем "...\Binding[GUID]\inputcsv" и этот путь будет храниться в переменной среды inputcsv. Аналогичная операция выполнится для каждого исходящего файла. Учитывая эту логику, мы внесли несколько небольших изменений в скрипт.
Это все изменения, необходимые для запуска службы при появлении файла CSV в BLOB-хранилище и получения в результате файлов, содержащих прогноз. 
Если честно, мы тестировали и другие триггеры, но обнаружили, что самая мощная функция Python — модули — становится ее проклятием в бессерверной системе. Модуль в Python — это не статическая библиотека, которую нужно подключить, как во многих других языках, а код, выполняемый при каждом запуске. Для таких долгосрочных решений, как службы, это почти незаметно, но с точки зрения функций Azure полное выполнение скрипта каждый раз влечет за собой довольно большие расходы. Это осложняет использование триггеров HTTP в Python, но batch-обработка на базе CSV-файлов, популярная во многих ML-сценариях, позволяет снизить эти расходы в расчете на строку данных до разумного минимума. 
Если ты не можешь обойтись без триггеров реального времени с Python, ты можешь попытаться перейти на выделенный тарифный план Azure App Service, поскольку это позволяет значительно увеличить вычислительные ресурсы хоста и ускорить импорт. В нашем случае простота реализации и низкая стоимость плана потребления перевесили преимущества быстрого выполнения.
Прежде чем продолжить, давай посмотрим, как можно упростить разработку с помощью Visual Studio Code. На момент написания этой статьи интерфейс Functions CLI обеспечивал начальное формирование шаблонов Python, но функций отладки не было. Тем не менее среду выполнения не так сложно имитировать, используя встроенные функции VS Code. Нам поможет файл .vscode\launch.json, позволяющий настраивать параметры отладки. Как видно из JSON ниже, при запуске debug в конфигурации Score (Python) мы просим VS Code выполнить отладку скрипта ${workspaceRoot}/Score/run.py с рабочим каталогом ${workspaceRoot}/Score, кроме того, мы задаем переменные среды для трех файлов-макетов с привязками. Это полностью имитирует то, как эта функция будет выполняться с помощью Azure Functions (не забывай проверить текущий рабочий каталог при разработке скрипта). При наличии этих настроек ты можешь просто открыть раздел отладки Debug (Ctrl + Alt + D) в VS Code, выбрать Score (Python) в качестве конфигурации и нажать Start Debugging, чтобы построчно прогнать код в редакторе.
Если ты хочешь использовать расширение Jupyter для интерактивной поячеечной разработки и выполнения, тебе нужна аналогичная конфигурация в коде функции. Поэкспериментировав, мы остановились на варианте кода ниже. Он выполняется только в среде IPython, а при нормальном выполнении или отладке через Debug игнорируется.
Теперь, когда у нас есть готовая модель и код функции, пришло время настроить нужную инфраструктуру Azure. На момент написания этой статьи поддержка Python в функциях Azure все еще находилась на этапе предварительной версии, поэтому требовались дополнительные шаги по настройке. По умолчанию в среде выполнения установлен Python версии 2.7. Чтобы перейти на более популярную версию 3.6, в соответствии с официальной статьей в wiki тебе нужно получить любой доступный пакет Python (можно использовать подготовленную среду) и поместить его в папку D:\home\site\tools. Все работает достаточно просто. Эта папка предшествует папке с установленным по умолчанию Python 2.7 в переменной PATH в момент поиска python.exe для исполнения.
Ты можешь сделать это вручную с помощью встроенного пользовательского интерфейса Kudu, как показано в статье, однако я выяснил, что предусмотренная для него специальная функция более удобна. Функция setup показывает, как мы это делали во время работы над проектом. Сначала функция проверяет, установлена ли версия 3.6, если нет, то она загружает предварительно сконфигурированный архив (.zip) с Python и извлекает его в папку D:\home\site\tools.
Далее нужно установить требуемые пакеты pip. Pip имеет встроенный API для Python, поэтому работать с ним из Python так же просто, как в обычной командной строке. Из кода ниже видно, что используемые только в Python пакеты (langid, pymorphy) установлены, поэтому от нас не требуется никаких дополнительных действий. Проблема возникнет лишь с пакетами, созданными с помощью C++. В платформе App Service отсутствует компилятор Visual C++, поэтому остается только использовать предварительно скомпилированные пакеты (wheels). Некоторые из них уже присутствуют в репозитории pip (проверить можно здесь), для других специфичных ML-пакетов можно найти необходимый wheel здесь. В данном случае я использовал Azure Blob Storage, чтобы сделать эти пакеты доступными для функции Azure. Ты можешь повторно использовать эти ссылки или перевыложить их в любое публично доступное хранилище.
Этот же подход оказался эффективен для выполнения дополнительных шагов после настройки. Чтобы наше решение работало, нужно установить два корпуса, специфичных для NLTK. Приведенный ниже код выполняется сразу после набора команд install_packages.
Поскольку весь код в функции Setup идемпотентный, ты можешь без проблем добавить другие этапы настройки или установить дополнительные пакеты. Обрати внимание, что в первый раз эта функция должна запуститься дважды: сначала, чтобы перейти на Python 3.6, а затем, чтобы установить нужные пакеты.
Несмотря на сложные настройки и ограничения, связанные с предварительным запуском, Azure Functions показали себя как довольно удобный и эффективный инструмент операционализации ML-моделей Python. Наш первоначальный проект был развернут в производственной среде, и модель ML помогла значительно улучшить результаты по сравнению с уже существующими подходами. Код для изучения и повторного использования доступен в репозитории на GitHub.
Напоминаем, что это полная версия статьи из журнала Хакер.
Tech Evangelist
=====
Видеонаблюдение в подъезде своими силами
2017-12-01, 16:53
Архитектор
=====
Колл-центр для исходящего обзвона: создаем предиктивный диалер в 3CX Call Flow Designer
2017-12-01, 17:08
User
=====
Правда ли, что будущее CPaaS за «Serverless» технологиями?
2017-12-04, 12:34
Программист, Технический писатель, DevRel
=====
Секреты React и Redux при разработке веб-приложений
2017-12-01, 17:36
User
=====
Администрирование коммутаторов Juniper с помощью Ansible
2017-12-01, 17:42
Пользователь
=====
Регрессионные тесты на утечки памяти, или как написать memory profiler для .NET приложений
2017-12-01, 17:52
Как правило, профилировщики памяти начинают использовать тогда, когда приложение уже гарантированно «течёт», пользователи активно шлют письма, пестрящие скриншотами диспетчера задач и нужно потратить уйму времени на профилирование и поиск причины. Наконец, когда разработчики обнаруживают и устраняют утечку, выпускают новую прекрасную версию приложения, лишенную прежних недостатков, есть риск, что через некоторое время утечка вернется, ведь приложение растет, а разработчики все также могут допускать ошибки.
Автоматизированное регрессионное тестирование ошибок уже давно стало мейнстримом индустрии разработки качественного ПО. Такие тесты помогают не допустить попадание ошибки к пользователю, а также по горячим следам разобраться, какое изменение в коде привело к ошибке, тем самым минимизировав время ее исправления. 
Почему бы нам не применить такой же подход к утечкам памяти? 

Этим вопросом мы задались, в очередной раз получив OutOfMemoryException во время прохождения регрессионных автотестов на x86 агентах.
Пара слов про наш продукт: мы разрабатываем Pilot-ICE — систему управления инженерными данными. Приложение написано на .NET/WPF, а для регрессионного тестирования мы используем фреймворк Winium.Cruciatus, основанный на UIAutomation. Тесты «прокликивают» через UI весь доступный функционал приложения, проверяя логику работы.
Идея внедрения тестов на утечки памяти следующая: в определенные моменты прохождения тестов подключаться к приложению и проверять количество экземпляров объектов определенных типов в памяти.
Мы рассмотрели большинство популярных .NET профилировщиков памяти, и все они сохраняют снапшоты памяти в проприетарном формате, который может быть открыт для анализа исключительно в соответствующем просмотрщике. Никакой возможности для автоматизированного анализа снапшотов нами найдено не было ни в одном из них. 
Особняком стоит dotMemory Unit – бесплатный фреймворк для юнит-тестирования, позволяющий анализировать утечки памяти в тестах. К сожалению, в нем анализ памяти ограничен процессом, выполняющим запуск тестов. Подключиться к внешнему процессу с помощью dotMemory Unit на данный момент возможности нет.
Итак, не найдя подходящего готового решения, было решено написать свой профилировщик памяти. Что он должен уметь делать:
При этом хотелось сделать так, чтобы не пришлось модифицировать само тестируемое приложение.
Как вы знаете, для вызова сборки мусора в .NET приложении может быть использован метод GC.Collect(), запускающий сборку мусора сразу во всех поколениях. Данный метод не рекомендован к использованию в продакшн-коде, и профилирование памяти — чуть ли не единственный адекватный сценарий его использования. Сборка мусора перед профилированием нужна для устранения ложных срабатываний профилировщика на недостижимых объектах, до которых просто не успел дойти GC.
Сложность состоит в том, что сборка мусора должна быть запущена во внешнем процессе, и для этого есть несколько возможных решений:
Для анализа памяти приложения мы использовали библиотеку CLR MD, предоставляющее API, сходное с расширением отладки SOS в WinDbg. С помощью него можно подключиться к процессу, обойти все объекты в кучах, получить список корневых ссылок (GC root) и зависимые от них объекты. По большому счету все, что нам необходимо — уже реализовано, нужно только всем этим правильно воспользоваться.
Вот так можно получить количество объектов определенного типа в памяти с помощью CLR MD:
Самый сложный, но вполне разрешимый момент — получение информации о том, что удерживает объект от того, чтобы быть собранным сборщиком мусора. Для этого необходимо обойти все деревья зависимостей корневых ссылок, запоминая по ходу обхода пути удержания.
Далее мы встроили все наработки в код регрессионных тестов. В тесты была добавлена информация об именах периодически утекающих типов и максимальном количестве экземпляров этого типа, которые могут находиться в памяти. Алгоритм проверки такой: после окончания теста сначала запускается сборка мусора, потом запускается анализ количества объектов интересующих нас типов, если их количество больше эталонного — рапортуется проблема и билд помечается как «упавший». Кроме того, собирается диагностическая информация о том, что держит эти объекты от сборки мусора и добавляется в артефакты билда. Вот как это выглядит для TeamCity:

Получившееся решение вышло довольно общим, и мы решили поделиться им с сообществом. С кодом проекта можно ознакомиться в репозитории на github, кроме того решение в готовом для использования виде доступно в виде nuget пакета под названием Ascon.NetMemoryProfiler. Распространяется под лицензией Apache 2.0.
Ниже пример использования API. Минималистичный, но описывающий практически весь предоставляемый функционал:
Рассмотрим на примере простого приложения, как можно написать тест на утечки памяти. Сделаем тестовый проект, добавим в него пакет Ascon.NetMemoryProfiler.
Install-Package Ascon.NetMemoryProfiler
Напишем основу для теста:
Создадим новое WPF приложение, и добавим в него несколько окон и view-model, в которые намеренно внедрим разные варианты утечек памяти:
Пожалуй, самый распространенный вид утечки памяти. Объект-владелец события после подписки начинает хранить строгую ссылку на подписчика, тем самым не давая сборщику мусора убрать подписчика на все время жизни объекта-владельца события. Пример:
В данном случае время жизни Dispatcher.CurrentDispatcher совпадаем с временем жизни приложения, и EventHandlerLeakViewModel не будет освобождена даже после закрытия ассоциированного с ней окна.
Проверим. Запускаем приложение, открываем окно, закрываем его, запускаем тест, предварительно указав в нем имя процесса и имя типа для поиска. Получаем результат:
Исправить утечку можно, вовремя отписавшись от события (например, при закрытии окна), или воспользовавшись слабыми событиями (weak events).
Довольно неочевидный способ получить утечку памяти в WPF приложении. Если целевой объект связывания не DependencyObject и не поддерживает интерфейс INotifyPropertyChanged, то данный объект будет жить в памяти вечно. Пример:
Запустим тест. Получим такой результат:
Чтобы устранить такую утечку, необходимо поддержать интерфейс INotifyPropertyChanged у класса BindingLeakViewModel, либо определить связывание как одноразовое (OneTime).
При связывании с коллекцией, не поддерживающей интерфейс INotifyCollectionChanged, коллекция никогда не будет собрана GC. Пример:
Поправим тест, чтобы он искал экземпляры типа MyCollectionItem, и запустим его.
Устранить утечку можно, использовав ObservableCollection вместо List.

Регрессионные тесты на утечки в .NET приложении писать можно, и даже совсем не сложно, особенно если у вас уже есть автоматизированные тесты, работающие с реальным приложением.
Ссылка на репозиторий и nuget пакет. 
Скачивайте, используйте в ваших .NET проектах для контроля утечек памяти. Мы будем рады пожеланиям и предложениям.
Пользователь
=====
Реализация простейшей стратегии инвестирования на базе API MOEX (Московской биржи)
2017-12-04, 13:36


QA
=====
Как выйти на путь разработки ОС
2017-12-01, 23:12
Пользователь
=====
Открываем зимний конкурс CrackMe: ломай-ревёрсь
2017-12-01, 23:38
Пользователь
=====
Arduino и сегментный ЖК индикатор
2017-12-02, 00:45
User
=====
Латентные паразиты
2017-12-02, 01:21
Биоробот
=====
tldr — альтернатива man с названием, говорящим за себя
2017-12-05, 10:21
Open Source geek
=====
Учим компьютер писать как Толстой, том I
2017-12-02, 14:12
Недавно на хабре наткнулся на эту статью https://habrahabr.ru/post/342738/. И захотелось написать про word embeddings, python, gensim и word2vec. В этой части я постараюсь рассказать о обучении базовой модели w2v. 
Итак, приступаем. 
Первым делом надо скачать данные для nltk.
В открывшемся окошке выбираем все, и идем пить кофе. Это займет около получаса.
По умолчанию в библиотеке русского языка нет. Но умельцы все сделали за нас. Качаем https://github.com/mhq/train_punkt и извлекаем все в папку
C:\Users\<username>\AppData\Roaming\nltk_data\tokenizers\punkt и
C:\Users\<username>\AppData\Roaming\nltk_data\tokenizers\punkt\PY3.
Nltk мы будем использовать для разбивки текста на предложения, а предложений на слова. К моему удивлению, все это работает довольно быстро. Ну хватит настроек, пару уже написать хоть строчку нормального кода.
Создаем папку где будут скрипты и данные. Создаем enviroment.
Активируем.
Туда же кидаем текст. Назовем файл anna.txt
Для обладателей PyCharm можно просто создать проект и в качестве интерпретатора выбрать анаконду, не создавая окружения.
Создаем скрипт train-I.py.
Подключаем зависимости.
Считываем текст.
Теперь очередь токенизатора русских предложений.
На этом остановимся по подробнее. В первой строчке мы разбиваем предложение (строку) на слова (массив строк). Затем удаляем пунктуацию, которую nltk, почему-то выносит как отдельное слово. Теперь стоп-слова. Это такие слова, от которых нашей модели пользе не будет, они лишь будут сбивать ее с основного текста. К ним относят междометия, союзы и некоторые местоимения, а также любимые некоторыми слова-паразиты. Затем убираем кавычки которых в этом романе через край.
Теперь разбиваем текст на предложения, а предложения на массив слов.
Для интереса выведем количество предложений и парочку из них.
Теперь начинаем обучать модель. Не бойтесь это не займет и получасу — 20024 предложения для gensim просто расплюнуть.
Теперь сохраняем модель в файл.
Сохраняем файл. Чтобы запустить, тем кто работает в PyCharm или Spyder достаточно нажать run. Кто пишет вручную с блокнота или другого редактора придется запустить Anaconda Promt (для этого достаточно вбить это в поиск в меню), перейти в директорию со скриптом и запустить командой 
Готово. Теперь вы можете с гордостью сказать, что обучали word2vec.
Как бы мы не старались, но Анны Каренины для обучении модели мало. Поэтому воспользуемся вторым произведением автора — Война и Мир.
Скачать можно отсюда, также в формате TXT. Перед использованием придется соединить два файла в один. Кидаем в директорию из первой главы, называем war.txt. Одной из прелестью использования gensim является то, что любую загруженную модель можно доучить с новыми данными. Этим мы и займемся.
Создаем скрипт train-II.py
Думаю, что эта часть не нуждается в объяснениях, так как в ней нет ничего нового.
Затем загружаем нашу модель, и скармливаем ей новые данные.
Здесь я немного остановлюсь. total_examples устанавливает количество слов, в нашем случае это весь словарь модели (model.corpus_count), включая новые. А `epochs количество итераций. Честное слово, сам не знаю, что значит model.iter взял из документации. Кто знает, напишите, пожалуйста, в комментариях — исправлю.
И снова сохраняем.
Не забудьте запустить.
Их нет. И пока не будет. Модель еще не совсем совершенна, откровенно говоря, она ужасна. В следующей статье я обязательно расскажу как это исправить. Но вот вам на последок:
P.S
Ссылки и используемая литература.
Надеюсь моя статья вам хоть немного понравилась.
User
=====
Я создал приложение, которое делает изучение алгоритмов и структур данных гораздо интереснее
2017-12-02, 14:28
Пользователь
=====
Текстуры кода
2017-12-02, 15:17
Системная Архитектура, Программирование
=====
Локальная автоматизация билдов(Crashlytics + Slack + FastLane)
2017-12-02, 15:46
User
=====
Эксперимент по продвижению игры в Google Play. Часть 1
2017-12-02, 15:51
User
=====
Настройка звука в Ubuntu
2017-12-02, 16:07
У меня есть хобби — написание музыки. Поэтому после установки свежей версии Ubuntu на свой ноутбук мне понадобилось настроить звук чуть более тонко, чем обычным пользователям. К моему сожалению, сделать это у меня не вышло. Тем не менее, я хочу рассказать о шагах, которые немного приблизили меня к результату. Я надеюсь, что кому-то это сэкономит время. А может, с помощью читателей я смогу пройти дальше. Если вы пользуетесь Linux и можете произнести слова sidechain и компрессия в одном осмысленном предложении — Добро пожаловать!
Для начала более подробно о сути задачи: есть ноутбук Asus N55 с Kubuntu 17.10 на борту. Нужно добиться той же конфигурации, которую можно получить на Windows:
Сразу после установки получаем нерабочий сабвуфер и нерабочие наушники.
В результате моих опытов мне удалось добиться следующего состояния системы:
Для знакомства с LMMS(с которой как оказалось нормально работать нельзя, но это отдельная тема) этого в общем-то хватает.
Если вы тоже добрались до этого состояния, то ничего нового этой статьей я вам не расскажу.
Вы когда-нибудь задумывались о том, как устроена звуковая система Linux? Вот и я нет — обычно звук либо был не нужен, либо как-то работал, колонки играли — ни о каких фокусах подключения сабвуфера или наушников речи не было.
Похоже, теперь самое время узнать об этом немного больше. Гугление показало следующую картину:
Мало кто говорит, что PulseAudio де-факто является стандартом для интеграции звука, по крайней мере в Ubuntu. Практически любой софт, который издает звуки, будет с большой долей вероятности пользоваться PulseAudio API. FF, например, для поддержки JACK надо отдельно собирать.
Поэтому удалять PulseAudio не стоит, если вы не уверены, что все чем вы будете пользоваться, умеет работать с JACK. Видимо, поэтому существует еще вариант интеграции Jack-PulseAudio. Но от этого варианта я отказался, т.к. для работы в том софте, где требуются низкие задержки, меня устраивает монопольный доступ через ALSA(под Windows это работает так же — там, где нужна низкая задержка, запрашивается монопольный доступ). Возможно, когда дело дойдет до установки какого-нибудь Ardour, мне придется вернуться к этому варианту.
Так, ну хорошо, JACK не нужен. Выходит, чтобы все заработало, мне надо настроить ALSA-компонент для своей карточки и интеграцию его с PulseAudio
Тут нам из коробки предлагают вот такую UI консоль
Мне кажется, к такому инструменту пояснения излишни. После применения настроек вы сразу сможете слышать изменения, так что пробуйте.
Еще amarao советует обратить внимание на pactl /pacmd. Если я доберусь — напишу, что там к чему с этими утилитами
Первым делом понадобятся хоть какие-то инструменты для диагностики — это пакет alsa-utils
sudo apt install alsa-utils
Теперь посмотрим, как видит наше оборудование система
aplay -l
Кроме того, настройки надо проверять. Для этих целей обнаружилась утилита speaker-test
вызов speaker-test --help покажет, что умеет утилита, но мне особенно интересна конфигурация
speaker-test -Dplughw:1,0 -c4
Тут и тут советуют поизучать dmix — это модуль микшера каналов в ALSA — буду смотреть
alsamixer — псевдографический интерфейс микшера. Половина советов по настройке ограничивается этой утилитой. И действительно, именно эта утилита часто покажет, когда канал неожиданно замьютился или у него сбросился уровень громкости.
Но настройка в микшере результата не дала.
Следующая полезная утилита — hdajacksensetest. Показывает какие разъемы детектируют подключение внешних устройств.
Ок, я вижу, что мой внешний сабвуфер подключен на порт 0x1a.
Давайте попробуем найти как можно объяснить карте, куда выводить LFO.
Большинство советов по настройке маппинга указывают на утилиту hdajackretask:
Тут вроде бы все понятно — есть порты, есть маппинг. К сожалению, никакие манипуляции с портом 0x1a к успеху не привели.
Может, после настройки надо явно перезагружать сервисы?
sudo alsa force-reload
Нет, смотрим дальше.
Для настройки маппинга есть еще одна утилита:
скачать ее можно в виде python-скрипта(!!!) вот тут.
Выглядит эта штука как прокачанный вариант hdajackretask — она даже умеет строить схему маршрутизации портов графически. Но эксперименты с этой штукой тоже не привели ни к чему, кроме полной потери звука.
Никакого гайда по настройке я не нашел, поэтому пробовал методом проб и ошибок.
Других утилит, способных помочь в настройке звука я не обнаружил. Остается попробовать только одно — искать и править конфиги. С помощью гугла и find, мне удалось обнаружить несколько локаций.
/usr/share/pulseaudio
в папке alsa-mixer тут лежат конфиги. Т.к. Pulseaudio более-менее работает тут я ничего трогать не стал.
/etc/pulse/daemon.conf
Как я понял, это настройки службы pulseaudio — один из немногих конфигов, изменения в котором на что-то влияют. именно тут настройками:
enable-lfe-remixing = yes
lfe-crossover-freq = 200
я заставил звучать свой сабвуфер из Pulseaudio так, как он должен. Для lfe-crossover-freq 200 — это очевидно частота среза в герцах.
Но тут же становится очевидно, что настроить нечто более сложное(при подключении наушников прекращать отправлять бас на сабвуфер) в этом конфиге возможности нет.
/usr/share/alsa/*
Тут лежит несколько shell-скриптов, в т.ч. alsa-info.sh, который может быть полезен при подготовке дефекта в трекере и т.д.
а в папке pcm какие-то конфиги, разобраться в которых мне не удалось:
Трогать их я пока не решился.
/etc/modprobe.d/alsa-base.conf — второй доказанно полезный конфиг. Вот эта строчка, дописанная в низ конфига, реально заставила работать наушники под ALSA:
options snd-hda-intel model=auto,auto probe_mask=1
Про настройку этого конфига написано довольно много. Одна из его целей, как я понял — сопоставить устройствам кодеки(model). Кодеки это… какой-то пресет маппингов… Есть табличка (копия есть тут). Думаю, в большинстве случаев для стандартных конфигураций этого действительно достаточно.
Там описаны модели для разных контроллеров. Но если для вас они не срабатывают, то вам рекомендуют добавить свою конфигурацию.
Звучит классно, но тут я должен сделать 2 ремарки:
Кстати, в /sys/class/sound/hwC1D0/ — лежат файлы уже непосредственно устройства, т.е. как я понял hdajackretask работает именно с этими файлами. Тут тоже, наверное, можно сделать что-то полезное, если знать куда что писать.
На этом у меня все. После правок в daemon.conf и alsa-base.conf, я получил результат, описанный в начале. Это все, чем я мог помочь вам в деле настройки звука под Ubuntu.
Большинство приведенной тут информации я получил здесь и документации ALSA на Arch Linux.
Как я и говорил, после проделанной работы появляется больше вопросов, чем ответов:
Общий вывод, который я могу сделать по результатам проделанной работы: в целом видно, что если GUI, например, сообщество более-менее занимается, то звуковая подсистема явно остается за бортом.
На сегодняшний день возможности использования этой подсистемы не выдерживают никакой конкуренции с аналогами в других популярных ОС.
Такие досадные проблемы, как неработающий сабвуфер или наушники, или отсутствие НЧ-фильтра для сабвуфера, заставляют еще раз подумать, прежде чем отказываться от предустановленного софта. Ведь никому не хочется получать от своей железки меньше отдачи из-за кривостей ОС.
з.ы. Если есть еще какие-то инструменты, мануалы, про которые полезно знать при настройке звука — пишите, я добавлю их в статью
з.з.ы. По поводу своей конкретной проблемы я завел тикет, владельцы Asus N55 — велкам:
https://bugs.launchpad.net/ubuntu/+source/alsa-driver/+bug/1733029
Разработчик
=====
Строгая типизация в нестрогих тестах
2017-12-04, 09:05
javascript, webgl, maps, react, орфография(нет)
=====
DLP-система DeviceLock 8.2 — дырявый штакетник на страже вашей безопасности
2017-12-02, 17:01
Пользователь
=====
PLATO: история первой в мире системы электронного обучения
2017-12-04, 15:50
Пользователь
=====
Капсульные сети от Хинтона
2017-12-04, 02:15
Пользователь
=====
Игра на Unity, с открытым кодом
2017-12-02, 18:36
Unity developer
=====
Пишем DSL в Koltin
2017-12-02, 19:45
Android разработчик
=====
Корпоративный «фарш» для небольших сетей
2017-12-11, 07:59
Пользователь
=====
Автоматный практикум — 2. Пример «Переправа», математические преобразования ТЗ при ОА 
2017-12-02, 22:31
Оглавление

Предыдущая статья


 Зачастую удачное предметное изображение процесса, который требуется осуществить, равносильно получению решения задачи в общем виде, это чуть ли не половина успеха при выборе ОА. Это настолько важно, что я хочу проиллюстрировать что подразумевается под термином предметный взгляд и то, как из предметного взгляда естественным образом вытекает выбор удачного ОА, а уже выбор удачного ОА сам собой подталкивает к нахождению оптимального решения. Таким образом, можно ставить знак приближённого равенства между выбором оптимального ОА и нахождением оптимального решения. В прошлой статье я продемонстрировал выбор ОА на примере модуля «Дисплей». Дальнейшая работа над «Дисплеем» это уже другая тема: модификация автоматно-реализованных программ, я вернусь к ней чуть позднее. Сегодняшняя задача ещё рельефней показывает процедуру разработки оптимального ОА.


Постановка задачи в виде «волк, коза и капуста» только лишь повод, и речь пойдёт о решении задачи в общем виде, для любого количества участников и их пищевых пристрастий, и любого количества мест в лодках. Задача — получить такой алгоритм, который позволит найти кратчайший путь осуществления переправы, или показать, что задача в такой формулировке не имеет решения.


Начну с цитаты из предыдущей статьи «бывают случаи, когда предметное изображение условий задачи даже и не намекает, каким операционным автоматом их обрабатывать. В этом случае условия задачи можно подвергнуть математической обработке, и решать не исходную задачу, а математически тождественную, но более удобную».


Если изобразить буквально, процесс перевозки будет состоять из следующих шагов


Если смотреть на это как математик, и совсем абстрагироваться от логистики речных перевозок, можно увидеть, что шаги 1, 3, 5, 7 излишни. В те моменты, когда крестьянин на берегу, никаких проблем возникнуть не может, рассматривать нужно только случаи когда крестьянин в пути. Случай крестьянина причалившего к берегу означает просто обмен содержимого лодки и берега. Крестьянин в этом случае прочно ассоциируется с лодкой и его можно больше вообще не рассматривать. Поскольку в лодке никаких эксцессов по условию задачи тоже не может произойти, её можно не рассматривать вообще, вместе с содержимым. Если что-то в данный момент находится в лодке, то на берегах будет одним объектом меньше. Всё описанное выше создавало информационный шум, не давая увидеть суть происходящего. В этом случае процесс перевозки изображается так, как показано на рис. 2.


То что я сделал это по сути тождественное преобразование. Увидеть тождественность задач показанных на рисунках 1 и 2 помогает математический опыт и математическая интуиция. Не так давно был опрос в котором автор интересовался, нужна ли математика программистам. По моему опыту, для развития математического опыта и интуиции я бы категорически посоветовал программистам изучать теорию вероятностей. Именно теория вероятностей учит и комбинаторике, и тому, что обязательно учитывать не только интересующие варианты (называемые исходы испытаний), но и все возможные варианты. Это фактически подкрепление совета из предыдущей статьи о том, что «важно изображать не только идеальный случай, но и «неудобные» варианты, которые связаны с граничными условиями. … Такой подход смещает ресурсозатраты разработки программы с этапа отладки (когда имеешь дело с кучей реализованных модулей) на этап проектирования (когда имеешь дело с чистым холстом).» В данном случае «удобные» и «неудобные» варианты не являются прямой отсылкой к теории вероятностей, речь идёт о соответствующем образе мышления, который замечательно тренируется упомянутым разделом математики. Это всего лишь совет, можно к нему не прислушиваться, как и к большинству советов в этом мире.


Поскольку задача в таком виде, как она изложена выше, довольно избитая, рассмотрим её более сложную модификацию.

Требуется перевезти в лодке с одного берега на другой козу, капусту, собаку и волка. В лодке только два места (одно для крестьянина). Известно, что если оставить волка наедине с собакой, у них будет «вооружённый нейтралитет», однако если с ними останется коза, плотоядная волчья натура взыграет, собака кинется на защиту козы, в общем, втроём их оставлять нельзя. Как осуществить перевозку?


Скажу, что изображение рис. 2 даёт последовательную картину и не даёт наглядности и в этом его главный недостаток. Изобразить задачу в виде таблицы – решение №1, лучшее решение из всех возможных, потому что теперь все исходы событий как на ладони. Строка и столбец, помеченные символом 0, означают пустой берег.


Это позволяет охватить всю картину целиком, и подключить правое полушарие, которое специализируется на обработке информации, выраженной не в словах, а в символах и образах. Основной сферой специализации правого полушария является интуиция. Конечно, одной интуиции недостаточно, но у «технарей» интуиция не доминирует над логикой, а дополняет её. При правильном подходе правое полушарие (творческое, интуитивное) будет давать пищу для размышлений левому, которое отвечает за логику и анализирует полученные факты. Информация обрабатывается левым полушарием последовательно по этапам. Поскольку весь цикл посвящён программным автоматам, то следующая аналогия укладывается в это русло: правое полушарие можно сравнить с операционным автоматом, который выдаёт решения, а левое полушарие можно сравнить с управляющим автоматом, который испытывает выбранные решения, отсеивает негодные и даёт следующее задание правому, которое «озарением» выдаёт следующую мысль. То есть изображать задачу предметно важно для того, чтобы правое полушарие, охватывающее картину целиком, имело ту пищу для размышлений, которая ему лучше подходит, и это в свою очередь позволит генерировать более релевантные пробные решения, которые станут пищей для размышлений левому полушарию.


Изображение рис. 3 математически тождественно изображению задачи, которое показано на рис. 2, а оно в свою очередь тожественно полному пошаговому изображению процесса с погрузкой и выгрузкой подобного тому, что приведено на рис 1.


При таком изображении задача сводится к нахождению пути из нижнего левого в верхний правый квадрат, которые отмечены синим цветом. Однако не все комбинации (пересечения столбцов и строк) допустимы. Например, комбинация отмеченная крестом, не может существовать, потому что при таком раскладе коза оказывается сразу на двух берегах.


Если исключить все ситуации связанные с раздваиванием, количество оставшихся вариантов оказывается существенно меньше.


Следующий шаг, диктуемой логикой, убрать состояния подобные показанному на рис. 6. Смысл этого в том, что одна коза на левом берегу и никого на правом означает (если исключить то, что волк убежал в лес), что в лодке помимо крестьянина находятся: капуста и волк. Но по условиям задачи в лодке одно место.


Существуют разновидности этой задачи, когда лодка имеет два пассажирских места. Такие разновидности тождественны решаемому варианту, просто в этом случае будет меньшее количество исключённых по перегрузу ячеек.


Далее требуется исключить все запрещённые ситуации, связанные с поеданием одних персонажей другими.


Дальше сокращать количество ячеек невозможно, все оставшиеся состояния — разрешённые. Но как пользоваться этой таблицей?


Можно заметить, что движение вдоль вертикали связано с манипуляциями предметами на левом берегу, а движение по горизонтали, связано с манипуляциями предметами на правом берегу. Например, движение показанное стрелкой означает выгрузку из лодки (на левом берегу) капусты и погрузку волка. Коза на правом берегу.


При этом возможно перемещение на любое количество ячеек по горизонтали или вертикали, но не допускается перемещение по диагонали.


Это и есть операционный автомат. Если не воспринимать слово «автомат» буквально, то ОА это способ обработки, схема манипуляции данными, предметами и т.п. В данном случае ОА это таблица, из которой удалены все «невозможные» и недопустимые состояния, по которой можно перемещаться по вертикали или по горизонтали. Теперь осталось только найти кратчайший путь в этом лабиринте, и этим будет заниматься как раз управляющий автомат. 


Выражу мысль, что часто программисты рассматривают языки программирования как промежуточный язык, на который требуется перевести исходное ТЗ, написанное на человеческом языке, прежде чем компилятор переведёт его на машинный. Для многих задач такой подход действительно обоснован и естественен, но только тогда, когда манипуляции с ОА описываются очевидной последовательностью действий, например:
1. Считать данные из окна Edit.
2. Преобразовать текст в коэффициенты функции.
3. Вывести график функции на Chart.


Однако в рассматриваемом случае попытка найти решение исходного ТЗ, буквально воплощая условия задачи, привела бы примерно к следующей последовательности действий:

1. Взять очередного персонажа,
2. Перевезти, на другой берег,
3. Если там его оставить нельзя, забрать другого персонажа и вернуть его обратно. перейти к п. 1.


Несмотря на то, что задачу нахождения пути в лабиринте" показанном на рис 8 может выполнить за считанные секунды даже ребёнок вооружённый карандашом, при решении по указанной методике, на каждом этапе могут возникать распараллеливания задачи, поскольку можно взять того или иного персонажа, и для каждого случая будет свой сценарий последующих событий. Такой подход действительно чреват рекурсией с ростом вычислительной сложности. 


Пару слов о вычислительной сложности такого решения. Для этой задачи полный перебор это не обход всей таблицы, потому что полный перебор для этой задачи подразумевает не просто перебор всех возможных комбинаций, но всех возможных последовательностей для всех возможных комбинаций. 
 


Полученная схема предполагает простой алгоритм нахождения пути.


Однако, предположительно возможен такой вариант, что потребуется движение вниз или вправо (например при решении этой задачи). Я воспользуюсь своим же советом «рассматривать неудобные случаи».


Чтобы отмести возражения по поводу того, что реальном случае правая нижняя поддиагональная половина должна быть полностью исключена по правилу дублирования персонажей, предположим, что это всего лишь фрагмент из верхней полудиагонали и больше свободных ячеек во всей таблице нет.


Такой лабиринт не обойти, если двигаться всё время вверх или вправо. Очевидно, что здесь нужен серьёзный алгоритм поиска пути. Поиск пути «в лоб» для варианта рис. 11 приведёт к перебору всех возможных путей, и хотя количество комбинаций сравнительно невелико, это чревато опять же рекурсией и имеет те же недостатки, что и вариант полного перебора. Лучше воспользоваться т.н. волновым алгоритмом или алгоритмом Ли. Не буду его здесь описывать, можно бегло поглядеть по ссылке.


Вернёмся, однако, к переправе, рис. 8. Распространение волны начинается с верхнего правого угла, которому присвоено число 0. Обратите внимание, поиск по вертикали вниз для этого столбца невозможен, поскольку все клетки в этом столбце исключены по правилу дублирования. Единственное возможное движение – влево. В строке только одна «белая» клетка. Помечаем её цифрой 1. После этого таким же способом помечается клетка 2.

Далее важный момент. Из клетки, помеченной 2, можно переместиться в любую из клеток по горизонтали за один ход, все они должны быть помечены цифрой 3.


Последовательное применение указанного алгоритма даёт в результате следующую картину. 
 

Критерий успешного окончания поиска – в вертикали с индексом 0 должна быть хоть одна «незабракованная» клетка, и она должна быть помечена цифрой. Поиск пути ведётся классическим способом: находясь в каждой клетке ищется соседняя клетка (в нашем случае любая в той же строке или в том же столбце), у которой число меньше. Зелёным цветом выделены клетки, составляющие путь.


Воспользовавшись своим же советом рассматривать неудобные варианты, предлагаю рассмотреть иллюстрацию


В этом случае кажется, что при обходе лабиринта разными путями, возникает противоречие. Однако, на самом деле противоречия нет, достаточно ставить цифры только в ещё не занятые ячейки. Тогда при обходе пути который привёл бы к значению 4, спорная ячейка уже содержит 3 и просто не попадает в рассмотрение.


Один из вариантов построения волны предполагает рекурсивный характер алгоритма перебора точек. Однако, я предлагаю другое решение. Оно позволяет обойтись без рекурсии, и основано на следующем рацпредложении.


Расчёт волны будет состоять из поочерёдного обхода строк и обхода столбцов. Рассмотрение начинается с верхней строки, во все ячейки которой ставятся 1. Далее строка обходится повторно (этап «поиск»), и для каждой ячейки, содержащей 1 проводится цикл вниз по столбцу (этап «сканирование-заполнение»), в ходе которого все свободные ячейки помечаются числом на 1 большим, чем предыдущее. 


После этого столбцы и строки меняются местами, и теперь ведётся поиск столбцов содержащих число 2, и из каждого столбца производится сканирование-заполнение соответствующей строки.


Указанная операция повторяется до тех пор, пока не будет отмечена хоть одна ячейка в столбце 0. Учитывая то, что пути обхода может не быть вовсе, обход прекращается в том случае, если при очередном обходе (поиск + сканирование-заполнение) не найдено ни одной новой ячейки.


При таком обходе каждая строка проходится по 2 раза – во время поиска и во время сканирования-заполнения. Точнее, поскольку после просмотра по горизонтали начинается сканирование по вертикали, будет правильнее перечислить в другом порядке: во время сканирования-заполнения и во время последующего поиска.


Но поскольку на момент каждой итерации неизвестно, какая строка содержит число, например 3, а какая не содержит его, для поиска каждой строки нужно просмотреть все строки, и это приводит к тому, что каждая строка просматривается один раз во время поиска числа 3 и ещё много раз при поиске остальных чисел (которых там нет).


Обращаю внимание, что итерации «поиск + сканирование-заполнение» чередуются для строк и столбцов, поэтому во время поиска по строкам, не будут искаться чётные, и соответственно наоборот. В этом случае количество просмотров каждой ячейки сокращается вдвое.


Но, обратите внимание на одну характерную вещь. В силу алгоритма заполнения каждая строка будет иметь вид.


Обращаю внимание, что алгоритм не рекурсивный. Если бы он был рекурсивный, это означало бы, что исследуется на максимальную глубину один путь, после чего алгоритм возвращается назад (на одну позицию за раз, но может получится так, что до самого начала), после чего исследуется следующий путь. В результате такого характера обхода, добавление чисел в таблицу идёт не в возрастающем порядке, но младшие числа могут добавляться на поле, в котором уже полно старших.


При обходе таблицы способом, описанным выше, каждая строка будет содержать число 
N-1 полученное при сканировании-заполнении предыдущей итерации, и числа N которые будут занесены в неё при сканировании-заполнении текущей итерации. Причём чисел N-1 может оказаться несколько, в силу того, что каждая итерация подразумевает сканирование-заполнение ряда линий, но в строке не будет ни одного числа отличного от N-1, N.


Более того, в случае рис. 20 число 4 относятся не к строке, а к столбцу, таким образом, если рассматривать только числа относящиеся к строке, получается что строка содержит только числа 5. Со столбцами ситуация аналогичная.


Это больше чем просто наблюдение это основание для дальнейшей разработки. Проиллюстрированный рис. 16-19 алгоритм подразумевает предварительный просмотр ячеек каждой строки, с целью определить содержит ли она искомое число (например, для рис. 21 возьмём поиск числа 7). Учитывая то, что строки содержат пару чисел N-1 и N просмотр первой строки можно прекратить, дойдя до ячейки с единицей. Вторую строку тоже можно не рассматривать до конца, встретив число 5, но её уже придётся прокручивать почти до конца. В то же время, поскольку, как я показал, с каждой заполненной строкой ассоциировано только одно число, и с каждым столбцом тоже одно число, можно интерпретировать задачу следующим образом. В дополнение к таблице создаётся пара массивов – один для строк и один для столбцов.


Поскольку из правого верхнего синего квадрата можно двигаться только по горизонтали (вся вертикаль исключена по правилу дублирования) поиск начинается со строки, отмеченной стрелкой NH. В ячейку H_header[0] заносится 1 ещё при инициализации. Далее следует процедура, которую можно описать диаграммой состояний показанной на рис. 23


Почему я считаю, что диаграмма состояний это альтернативная, более удобная форма записи программных алгоритмов? Она — удобный инструментом «стенографирования» мыслей, по мере того как они приходят в голову, поскольку построена из идентичных блоков, которые можно соединять любыми вообразимыми линиями.


Как по такой диаграмме создать алгоритм? В общем-то элементарно, даже проще и механистичней, чем если бы я описал всё словами. Те действия которые помечены как перебираем…, соответствуют циклам, остальные стрелки условиям, стрелки слева, помеченные 1 и 2 подразумевают цикл while(1) . 


Результат работы алгоритма показан на рис 24. В саму таблицу никакие пометки не заносятся. Кроме того, для удобства метод Make_wave возвращает число, начиная с которого ведётся построение трассы, в данном случае – 10.


После того, как получена волна, строится трасса. Алгоритм построения трассы можно описать диаграммой состояний. 


Сам проект можно увидеть на bitbucket.
Модуль Table_calculator содержит описание класса tCalculator, который и несёт весь функционал.

Программа


Пару слов о путях дальнейшего усовершенствования описанной программы. Рассмотрим задачу:


В таком виде для поиска решения задачи с помощью описанной программы, придётся одного из волков обозначить буквой w а другого W, потому что парсеримён персонажей игнорирует повторы имён. Соответственно правила запрета нужно задавать как wd, Wd.


Относительно несложно сделать так, чтобы можно было решать задачу для произвольного количества одноимённых участников, перечисляя их в виде 2w,3g,5c и задавать правила запрета в простом виде wd для любого волка, или g5c (запрет оставлять козу наедине с 5 мешками капусты, а то съест за раз и помрёт), однако разница будет в деталях, а суть решения останется прежней. Поэтому я пока не стану делать этого.


В заключение добавлю, что ранее я показывал процесс практической декомпозиции как.


Рассмотренный сегодня вариант, это тоже пример декомпозиции, но другого рода – конвейер операционных автоматов.


Поскольку выбор ОА так важен, я буду приводить ещё примеры решения задач автоматным проектированием. На сегодня всё.
Пользователь
=====
Разбираемся, что же там нового открыли в задаче о ферзях
2017-12-04, 09:08
Пару месяцев назад появилась занятная статья с анализом классической задачи о расстановке ферзей на шахматной доске (см. детали и историю ниже). Задача невероятно известная и вся уже рассмотрена под микроскопом, поэтому было удивительно, что появилось что-то действительно новое.

Сможете поставить ещё шесть? А найти все решения?
(картинка из статьи)
Далее, к сожалению, произошла какая-то совершенно невразумительная история из цепочки вот таких вот превращений:
Стоит отметить, что пять наугад открытых ссылок на русском ещё меньше проясняли картину происходящего. 
Я тут подумал — надо бы кому-то эту странную цепочку прервать и нормальным языком изложить суть событий.
О чём пойдёт речь:
Задача известна еще с древности (~ средних веков), необходимо расставить буквы таким образом, чтобы ни в одной строке и ни в одной колонке не было одинаковых, как например здесь:

Задачу придумал в 1848 году шахматный композитор Макс Беззель: суть задачи в том, чтобы расставить 8 ферзей на шахматной доске так, чтобы они не атаковали друг друга. С тех пор многие математики, например Гаусс, работали над задачей, а алгоритмисты и программисты, такие как Дейкстра, придумали множество подходов к поиску и подсчету решений.
В задаче, о которой мы будем говорить, не 8 ферзей, а N и доска, соответственно, не обычная шахматная, а NxN. 
Есть три наиболее популярных постановки задачи о ферзях
Задача формулируется очень прямолинейно.
Дано: пустая доска NxN, например 8х8

(в принципе понятно, что достаточно просто указать N, но так наглядней)
Найти: расстановку максимально возможного числа ферзей

Задача ставится тоже достаточно просто:
Дано: размер пустой доски N
Найти: H число возможных расстановок N ферзей на доске
Например, размер доски N = 1, тогда число возможных расстановок H = 1.
N = 8 => H = 92. 
Вот тут формулировка чуть-чуть коварней:
Дано: размер доски N и M позиций уже установленных ферзей
Найти: позиции оставшихся N — M ферзей
Визуально все как на КПДВ:

(картинка также из оригинальной статьи)
Вообще говоря, вариаций задачи больше: см. например: расстановку белых и черных ферзей
http://www.csplib.org/Problems/prob110
однако здесь мы рассматриваем только основной классический вариант.
В подобной вариации решения существенно отличаются (белые не бьют белых, а черные черных: в случае путаницы — см. комментарии тут):

(здесь максимальное число ферзей, причем на месте крестика можно поставить белого, а на месте точке черного — но не обоих сразу; взято из статьи)
Пришло время собственно обсудить: а как это вообще все решать и насколько быстро это вообще можно сделать?
Самый интересный момент, что даже специалисты иногда путаются и думают, что для решения N-ферзей нужен комбинаторный поиск и думают, что сложность задачи выше P. Про то, что такое P и NP, когда-то уже писал на Хабре: Зачем нам всем нужен SAT и все эти P-NP (часть первая) и вторая вот тут. Однако, задача решается без перебора вариантов! Т.е., для доски любого размера можно всегда расставить ферзей один за одним лесенкой:
Существует целый ряд алгоритмов расстановки, например см. вот эту статью или даже вот тут в Вики.
Отсюда вывод, для N = 1 и N > 3 решение всегда есть (см. алго), а для N = 2 или N = 3
всегда нет (тривиально следует из доски). Это значит, что задача разрешимости для N ферзей (где нужно сказать есть решение или нет) решается тривиально за константное время (ну ок, конструктивно за линейное — расставить/проверить).
Самое время перепроверить прочитанное, читаем типичный заголовок "задачу о N ферзях признали NP-полной задачей" — у вас замироточили глаза?
Вот тут начинается самое интересное: у количества решений задачи о расстановке ферзей даже есть своё имя — "последовательность A000170". На этом хорошие новости заканчиваются. Сложность задачи: выше NP и P#, на практике это означает, что оптимальное решение — это скачать данные последовательности в словарь и возвращать нужное число. Так как для N=27 оно уже считалось на параллельном кластере сколько там недель.
Решение: выписываем табличку и по n, возвращаем а(n)
n a(n)
1: 1
2: 0
3: 0
4: 2
5: 10
6: 4
7: 40
8: 92
9: 352
10: 724
…
21: 314666222712
22: 2691008701644
23: 24233937684440
24: 227514171973736
25: 2207893435808352
26 22317699616364044
27: 234907967154122528
Однако, если у вас какая-то хитрая разновидность задачи и все-таки нужно посчитать решения (а их количество неизвестно и раньше их никто не посчитал), то лучший вариант прототипа обсуждается чуть ниже. 
Тут начинается самое интересное: в чём же состоит новый результат статьи? Задача о дополнении до N ферзей — NP-полна! (Интересно, что про NP-полноту дополнения латинского квадрата было известно ещё в 1984-ом году.)
Что это означает на практике? Самый простой способ решишь эту задачу (или вдруг, если нам нужно её вариацию) — использовать SAT. Однако, мне больше нравится следующая аналогия:
SAT — это ассемблер для комбинаторных NP-задач, а Answer Set Programming (ASP) — это С++ (у ASP тоже загадочная русская душа: он временами запутан и непредсказуем для непосвященных; кстати, теория, лежащая в основе современного ASP, была придумана в 1988ом году Михаилом Гельфондом и Владимиром Лифшицем, работавших тогда в университетах Техаса и Стэнфорда соответственно).
Если говорить упрощенно: ASP — это декларативный язык программирования ограничений (constraints в англоязычной литературе) с синтаксисом Prolog. То есть мы записываем, каким ограничениям должно удовлетворять решение, а система сводит всё к варианту SAT и находит нам решение.
Детали решения здесь не столь важны, и Answer Set Programming достоин отдельного поста (который лежит у меня в черновике уже неприлично долго): поэтому разберем концептуальные моменты
Строка 1 { queen(X,Y) : column(Y) } 1 :- row(X). — называется choice rule, и она определяет, что является допустимым пространством поиска.
Последние три строки называются integrity constraints: и они определяют каким ограничениям должно удовлетворять решение: не может быть ферзя в одном и том же ряду, не может быть ферзя в одной и той же колонке (опущено, в силу симметрии) и не может быть ферзя на одной и той же диагонали.
В качестве системы для экспериментов рекомендую Clingo.
И для начала стоит посмотреть их tutorial и попочитать блог на www.hakank.org.
Безусловно, если впервые писать на ASP, то первая модель не выйдет невероятно эффективной и быстрой, но скорее всего будет быстрее перебора с возвратом, написанным на скорую руку. Однако, если понять основные принципы работы системы, ASP может стать "regexp для NP-полных задач".
Проведем простой численный эксперимент с нашей ASP моделью. Я добавил 5 коварных ферзей в модель и запустил поиск решения для N от 1 до 150 и вот, что вышло (запущено на обычном домашнем ноутбуке):

Итого, наша ASP модель примерно в течении минуты может найти решения задачи о дополнении при N <= 150 (в обычном случае). Это показывает, что система отлично подходит для прототипирования моделей сложных комбинаторных задач.
Data Scientist, PhD in AI
=====
Security Week 48: Root-доступ за усердие, майнер-консультант и пробный макрос-зловред
2017-12-03, 03:17
Пользователь
=====
Интегрируем смартконтракт в веб-приложение на Nodejs
2017-12-03, 09:08
Если вам интересна тема разработки продуктов использующих смартконтракты, но вы хотите понять полный цикл создания таких приложений, то этот урок специально для вас (надеюсь). Из него вы узнаете как разработать, оттестировать, залить в сеть и интегрировать в ваше приложение смартконтракт для блокчейна Ethereum.

Для примера я взял знакомый всем с детства финансовый инструмент — копилку. Для того чтобы продемонстрировать всю мощь смарт-контрактов, я добавил возможность указать лимит, который не позволит снять деньги пока на счету не накопится определенная сумма. Все материалы урока вы можете найти в репозитории PiggyBank, который содержит скрипты и UI для запуска примера. 
Цель урока показать полный цикл разработки, поэтому код местами значительно упрощен. В повседневной разработке я советую применять иструменты вроде Truffle. 
Помимо написания самого смарт-контракта, необходимо проделать следущие шаги:
Чтобы запустить приложение вам понадобится запустить тестовую сеть. Сделать это можно командой node bin/testnet.js.
Для начала напишем контракт. Алгоритм работы контаркта следующий:
Приведу код контракта из файла contract.sol:
После того, как контракт создан можно приступить к решению инфраструктурных задач. 
Файл 1-account.js.
Данный скрипт создает тестовый аккаунт с определенным балансом. При вызове из консоли файла 0-account.js вам будет предложенно ввести пароль и сумму в ether на вашем счету. После успешного выполнения секретный ключ и сумма будут записаны в файл account.json. 
расскажу подробнее о ключах. Для создания аккаунта необходимо создать секретный ключ. Из секретного ключа будут в дальнейшем получен адрес кошелька и публичный ключ. Секретный ключ это шестнадцатиричное число размером 256 бит, представленное в виде строки длинной 64 символа, содержащей префикс 0x.
Лучше всего получить подобное значение с помощью генератора случайных чисел:
Но для тестовых нужд мы будем получать sha3-хеш из введенного пользователем пароля:
Что на выходе даст нам:
Чтобы работать с кошельком созданным вручную вам понадобится добавить его через web3 API. И хотя в данном уроке вам это не понадобится я все же покажу как это делается:
После этого вы сможете отправлять транзакции с помощью сгенерированного вами ключа. Это может понадобится если ваше приложение будет самостоятельно создавать аккаунты, особенно, если их много. Стандартный метод web3.eth.personal.newAccount будет записывать ключи на диск в директорию ~/.ethereum/keystore, что может быть по каким-либо причинам не желательно. 
Про менеджмент ключей и использование собственных ключей, я постараюсь рассказать отдельно.
Файл 2-compile.js.
Данный скрипт компилирует исходный код из файла contract.sol и сохраняет результат в code.json, который будет использоваться в дальнейшем для деплоя и взаимодействия с контрактом.
Контакты в сети Ethereum хранятся в бинаром представлении, поэтому перед тем как использовать контракт нам необходимо скомпилирвоать исходный код. Делается это с помощью инструмента solc и в случае nodejs пакета solc (это скомпилированный с помощью emscripten solc).
После компиляции мы получим на выходе бинарный код bytecode, а так же описание интерфейса контракта. Вот как будет представлен метод withdraw в интерфейсе:
На выходе solc возвращает все контракты, которые найдет в исходном коде. Нам понадобится выбрать один, в нашем случае это PiggyBank:
Файл 3-deploy.js.
Скрипт берет скомплированный код из code.json. А затем создает контракт и заливает код в тестнет от имени пользователя. Полученный в результате адрес и интерфейс контракта записываются в файл contract.json.
Сначала создается пустой инстанс с интерфейсом и настройками по умолчанию (from и gas).
From — адрес от имени которого будут вызыватсья методы контракта.
Gas или бензин — это топливо для контракта, которое тратится в процессе работы приложения. Он нужен для того, чтобы избежать бесконечных циклов, способных остановить работу сети.
И так, все готово к заливке контракта:
Вызов конструктора происходит в момент деплоя, поэтому мы сразу передаем в него аргументы. В случае с PiggyBank конструктор содержит один аргумент uint _limit. После выполнения данного кода с нас списали средства отдельно за проведение транзакции и отдельно за выполнение кода конструктора.
Все готов к запуску, останется только сохранить адрес контракта:
Файл 4-run.js. Запуск npm start.
Скрипт запускает веб-сервер на порту $PORT или 8080 с простым интерфейсом для взаимодействия с контрактом. Открыв в браузере http://localhost:8080 вы сможете перечислить деньги на счет (deposit) или перевести на счет владельца (withdraw).
Рассмотрим что происходит немного подробнее. Для начала мы создаем инстанс контракта ссылающийся на тот, что мы задеплоили ранее:
К вызову конструктора добавился еще один аргумент — address, который указывает на то, что это действующий контракт. Давайте посмотрим что мы можем с ним сделать. Как вы помните у нас есть методы deposit, canWithdraw и withdraw. Чтобы пополнить счет нам необходимо вызвать метод deposit и отправть несколько монет в копилку.
Ethereum использует в расчетах 18 знаков после запятой и при этом не поддерживает типы с плавающей точкой. Рассчеты производятся в веях, а затем конвертируются в эзеры. Для этого перед отправкой мы конвертируем ether в wei с помощью метода web3.utils.toWei. Которая в свою очередь использует библиотеку BigNumber.js, для
рассчетов со значениями превышающими макисмально допустимые для типа Number.
Вызов метода canWithdraw будет отличаться, так как этот метод не вносит никаких изменений (constant), то для вызова вместо send используется call. Такая операция не вызовет списания средств и расходование бензина:
Метод для отправки монет в копилку может выглядеть так:
Файл 5-destroy.js.
Скрипт уничтожает контракт и удаляет из блокчена данные контракта. Не смотря на то, что вы все еще можете перречислить деньги на контракт, выполнить иные операции вы уже не сможете.
Файл test/test.spec.js. Запуск npm test.
Для тестирвоания используется билиотека mocha. Перед тем как запустить тесты нам понадобится запустить изолированный тестнет с предустановленными данными. Для этого необходимо:
Вот как это может выглядеть инициализация новой сети:
Мы создаем тестнет с двумя пользователями, а затем инициализируем инстанс web3. Тестнет готов. Можно приступать к тестированию. Например оттестируем конструктор:
В данном примере мы написали очень простое приложение, не обладающее сверхсложным поведением, но наглядно иллюстрирующее жизненный цикл контракта. Надеюсь это будет полезно тем, кто только начинает осваивать разработку для Ethereum.
Node.js, Standards, Distributed technologies
=====
Сборка мусора и время жизни объектов
2017-12-03, 11:26
Net Developer
=====
Релиз второй версии плагина Nodejs для Sublime Text
2017-12-03, 15:32
Разработчик
=====
Метод Уэлфорда и многомерная линейная регрессия
2017-12-04, 06:57
Многомерная линейная регрессия — один из основополагающих методов машинного обучения. Несмотря на то, что современный мир интеллектуального анализа данных захвачен нейронными сетями и градиентным бустингом, линейные модели до сих пор занимают в нём своё почётное место.
В предыдущих публикациях на эту тему мы познакомились с тем, как получать точные оценки средних и ковариаций методом Уэлфорда, а затем научились применять эти оценки для решения задачи одномерной линейной регрессии. Конечно, эти же методы можно использовать и в задаче многомерной линейной регрессии.

Линейные модели часто используют благодаря нескольким очевидным преимуществам:
Кроме того, есть ещё одно классическое "преимущество" линейных моделей, о котором обычно говорят: интерпретируемость. На мой взгляд, оно несколько переоценено, однако очевидно, что определить, какой знак имеет коэффициент признака в линейной модели существенно проще, чем проследить его использование в огромной нейронной сети.
В задаче многомерной линейной регрессии требуется восстановить неизвестную зависимость наблюдаемой вещественной величины (которая является значением неизвестной целевой функции)  от набора вещественных же признаков .
Обычно для каждого отдельного наблюдения записываются значения признаков и целевой функции. Тогда из значений признаков для всех наблюдений можно составить матрицу признаков , а из значений целевой функции — вектор её значений :




Здесь  — некоторое конкретное наблюдение. Таким образом, строчки нашей матрицы соответствуют наблюдениям, а столбцы — признакам, а общее количество наблюдений и, следовательно, строк в матрице, — .
Например, целевой функцией может быть значение температуры в конкретный день. Признаками — значения температуры в предшествующие дни в географически соседних точках. Другой пример — предсказание курса валюты или цены на акцию, в простейшем случае в качестве факторов опять же могут выступать значения той же величины в прошлом. Множество наблюдений формируется путём вычисления целевых значений и соответствующих им признаков в разные моменты времени.
Найти решение задачи означает построить некоторую решающую функцию . Сейчас мы говорим о задаче линейной регрессии, поэтому мы будем считать, что решающая функция является линейной. На самом деле, это значит, что решающая функция представляет из себя просто скалярное произведение вектора признаков на некоторый вектор весов:


Стало быть, решением нашей задачи является вектор весов признаков .
Изобразить многомерную линейную зависимость графически достаточно сложно (для начала, представим себе 100-мерное пространство признаков...), однако иллюстрации для одномерного случая вполне достаточно, чтобы понять происходящее. Позаимствуем эту иллюстрацию из Википедии:

Чтобы закончить формулировку задачи машинного обучения, требуется лишь указать функционал потерь, который отвечает на вопрос, насколько хорошо конкретная решающая функция предсказывает значения целевой функции. Сейчас мы будем использовать традиционный для регрессионного анализа среднеквадратический функционал потерь:


Итак, задачей многомерной линейной регрессии является нахождение набора весов  такого, что значение функционала потерь  достигает на нём своего минимума.
Для анализа обычно удобно отказаться от радикала и усреднения в функционале потерь:


Для дальнейшего изложения будет удобно отождествить наблюдения с наборами соответствующих им факторов:


Нетрудно видеть, что функционал потерь теперь записывается просто как скалярный квадрат: вектор предсказаний можно записать как , а вектор отклонений от правильных ответов — как . Тогда:


Из регрессионного анализа хорошо известно, что вектор-решение в задаче многомерной линейной регрессии находится как решение системы линейных алгебраических уравнений:


Получить доказательство этому несложно. Действительно, давайте рассмотрим частную производную функционала  по весу -го признака:


Если приравнять эту производную нулю, получим:




Теперь можно поменять порядок суммирования:


Теперь понятно, что коэффициент при  слева — это соответствующий элемент матрицы :


В свою очередь, правая часть построенного уравнения — это элемент вектора :


Соответственно, одновременное приравнивание нулю частных производных по всем компонентам решения приведёт к искомой системе линейных алгебраических уравнений.
При решении СЛАУ  возникают различные проблемы, например, мультиколлинеарность признаков, которая приводит к вырожденности матрицы . Кроме того, необходимо выбрать метод для решения указанной СЛАУ.
Я не буду подробно останавливаться на этих вопросах. Скажу лишь, что в своей реализации я использую гребневую регрессию с адаптивным выбором коэффициента регуляризации, а для решения СЛАУ использую метод LDL-разложения.
Однако сейчас нас будет интересовать намного более приземлённая проблема: как сформировать уравнения для СЛАУ? Если эти уравнения сформированы корректно, а вычислительные погрешности невелики, можно надеяться, что выбранный метод приведёт к хорошему решению. Если же ошибки появились уже на этапе формирования матрицы системы и вектора в правой части, никакой метод решения СЛАУ не достигнет успеха.
Из сказанного выше ясно, что для составления СЛАУ необходимо вычислять коэффициенты двух типов:




Если бы выборка была центрированной, это были бы выражения для ковариаций, о которых говорили в самой первой статье!
С другой стороны, выборку можно центрировать самостоятельно. Пусть  — матрица центрированной выборки, а  — вектор ответов для центрированной выборки:




Обозначим через  среднее значение -го признака, а через  — среднее значение целевой функции:




Пусть решением центрированной задачи является вектор коэффициентов . Тогда для -го события величина предсказания будет вычисляться следующим образом:


При этом  приближает центрированную величину. Отсюда можем окончательно записать решение исходной задачи:


Таким образом, коэффициенты линейной регрессии могут быть получены для центрированного случая, используя формулы вычисления средних и ковариаций по методу Уэлфорда. Затем они тривиальным образом преобразуются в решение для нецентрированного случая по выведенной сейчас формуле: коэффициенты всех признаков сохраняют свои значения, а вот свободный коэффициент вычисляется по формуле


Реализацию методов многомерной линейной регрессии я поместил в ту же программу, о которой рассказывал в прошлый раз.
Когда впервые задумываешься о том, что фактически вся матрица метода многомерной линейной регрессии представляет собой большую ковариационную матрицу, очень хочется сделать каждый её элемент отдельной реализацией класса, реализующего вычисление ковариации. Однако это ведёт к неоправданным потерям в скорости: так, для каждого признака будет  раз вычисляться среднее и сумма весов. Поэтому в результате раздумий родилась следующая реализация.
Решатель задачи многомерной линейной регрессии по методу Уэлфорда реализован в виде класса TWelfordLRSolver:
Поскольку в процессе обновления величин ковариаций постоянно приходится иметь дело с разностями между текущим значением признака и его текущим средним, а также средним на следующем шаге, логично все эти величины вычислить заранее и сложить в несколько векторов. Поэтому класс содержит такое количество векторов.
При добавлении элемента первым делом вычисляются новый вектор средних значений признаков и величины отклонений.


Уже после этого обновляются элементы основной матрицы и вектора правых частей. От матрицы хранится только главная диагональ и один из треугольников, т.к. она симметрична.
Поскольку разности между текущими значениями факторов и их средними уже посчитаны и сохранены в специальном векторе, арифметически операция обновления матрицы оказывается достаточно простой:


Экономия на арифметических операциях здесь более чем оправдана, т.к. эта процедура обновления матрицы является наиболее затратной с т.з. ресурсов CPU с асимптотикой . Остальные операции, в т.ч. обновление вектора правых частей, асимптотически линейны:


Последний элемент, с реализацией которого надо разобраться — это формирование решающей функции. После того, как получено решение СЛАУ, осуществляется его преобразование к исходным, нецентрированным, признакам:
Так же, как и в прошлый раз, реализации на основе метода Уэлфорда (welford_lr и normalized_welford_lr) сравниваются с "наивным" алгоритмом, в котором матрица и правая часть системы вычисляются по стандартным формулам (fast_lr). 
Я использую тот же набор данных из коллекции LIAC, доступный в директории data. Используется такой же способ внесения шума в данные: значения признаков и ответов умножаются на некоторое число, после чего к ним прибавляется некоторое другое число. Таким образом мы можем получить проблемный с точки зрения вычислений случай: большие средние значения по сравнению с величинами разбросов.
В режиме research-lr выборка изменяется несколько раз подряд, и каждый раз на ней запускается процедура скользящего контроля. Результатом проверки является среднее значение коэффициента детерминации для тестовых выборок.
Например, для выборки kin8nm результаты работы получаются следующими:
Таким образом, даже относительно небольшая модификация выборки (масштабирование в сто раз с соответствующим сдвигом) приводит к неработоспособности стандартного метода.
При этом решение, основанное на методе Уэлфорда, оказывается весьма устойчивым к плохим данным. Среднее время работы методу Уэлфорда благодаря всем оптимизациям оказывается всего на 50% больше среднего времени работы стандартного метода.
Досадно лишь то, что "нормированный" метод Уэлфорда также работает плохо, и, кроме того, оказывается самым медленным.
Сегодня мы научились решать задачу многомерной линейной регрессии, применять в ней метод Уэлфорда и убедились в том, что его использование позволяет достигать хорошей точности решений даже на "плохих" наборах данных.
Очевидно, если в вашей задаче требуется автоматически строить огромное количество линейных моделей и у вас нет возможности внимательно следить за качеством каждой из них, а дополнительное время исполнения не является определяющим, стоит использовать метод Уэлфорда как дающий более надёжные результаты.
User
=====
[Питер] Встреча JUG.ru с Олегом Ненашевым из CloudBees — Groovy DSL в Jenkins и Pipeline. Реализации и подводные грабли
2017-12-03, 12:30
Организатор конференций для программистов
=====
Используем Bash в SQL-стиле
2017-12-04, 07:35
Программист
=====
Программирование на телефоне используя эмулятор терминала Termux
2017-12-03, 15:38
User
=====
Системы ИИ научились создавать умные модели для ML: дайджест для начинающих
2017-12-03, 15:32
Пользователь
=====
Дайджест интересных материалов для мобильного разработчика #232 (27 ноября-3 декабря)
2017-12-03, 15:50
Пользователь
=====
Когда запустил стартап и узнаешь, что это уже и не стартап вовсе
2017-12-03, 15:48
Пользователь
=====
Лекция Яндекса: Advanced UI, часть первая
2017-12-03, 16:40
Интернет щей
=====
«Я слежу за тобой» или как из CADa сделать SCADA (MultiCAD.NET API)
2017-12-03, 22:22
Технический писатель
=====
Дыра у хостинг провайдера или как получить доступ к удаленному(не активному) аккаунту
2017-12-03, 17:45
Web-разработчик
=====
Идеальный каталог, оптимизация выборки данных
2017-12-03, 18:19
программист эникейщик
=====
Шаблон проектирования Entity-Component-System — реализация и пример игры
2017-12-05, 11:04
Переводчик-фрилансер
=====
Виртуальная пентест лаборатория
2017-12-03, 20:08
ИТ аудитор
=====
Два года успешного использования Edition-Based Redefiniton в базах Oracle
2017-12-07, 07:36
User
=====
C++ креши в WebAssembly на разных браузерах
2018-04-25, 11:07
Заметка задумывалась как продолжение предыдущей заметки о том, как собираем C++ креши на различных платформах включая asm.js и wasm. По количеству материала, это тянет только на заметку, а не полноценную статью, да и нужно быть наркоманом, что бы делать нативный клиент на C++, а потом засовывать его в браузер. 
Но! Мы недавно делали доклад об опыте использования wasm на cppconf. Оказалось, что наркоманов больше чем я думал, да и новость Beta for Qt for WebAssembly Technology Preview. Данная заметка может быть полезна, если вы захотите сделать отлов крешей в production окружении.
Под катом:
Отлов происходит через глобальный обработчик window.onerror.
В asm.js сообщение об ошибке и стек вызовов передается в параметре messageOrEvent. В случае wasm в messageOrEvent причина, что-нибудь типа Error: Out of bounds memory access (evaluating 'dynCall(rawFunction, a1, a2, a3)'), RuntimeError: index out of bounds и т.д
А в error попадает стек вызовов.
Мы используем ключ --emit-symbol-map при компиляции, что минимизирует имена функций. После компиляции получаем так называемые файлы символов.
Так выглядит файл символов для asm.js:
Для wasm это номер функции и имя функции:
Стек в разных браузерах выглядит по своему
Safari:
Firefox:
Chrome:
Chrome выдает не только номер функции 2007, но и смещение в ней 11. Так же chrome позволяет просматривать код в текстовом виде. На снимке экрана код 276 функции.

Это бывает полезно, например если выстрелил undefined behavior .
Остается только вытащить номера функций, сопоставить с функциями в файле, пропустить через abi::__cxa_demangle, что-бы получить читаемый стек вызовов.
Golang Developer
=====
Пирожки в Go
2017-12-03, 23:32
Инженер-программист
=====
Настройка аутентификации в SAP Netweaver AS Java (Часть 3 из 3)
2017-12-03, 22:57
SAP Basis Administrator
=====
Digital-мероприятия в Москве c 4 по 10 декабря
2017-12-03, 23:16
Подборка Telegram-канала @mоs_events

Он-лайн курс для event-менеджеров
The Future of Collaboration
Бизнес-встреча «Как и зачем предоставлять качественную клиентскую поддержку»
Открытая лекция Ильи Варламова "История моих провалов"
05.12.2017
Человек и машина — выгодный союз или жесткая конкуренция?
Машинное обучение как услуга для бизнеса
Pizza Pitch
27-й Международный фестиваль рекламы Red Apple
Бесплатная беговая тренировка с чемпионами мира и Decathlon
McKinsey Hackathon (совместно с Gett)
Говори и властвуй: философия массмедиа
User
=====
Кроссплатформенный IoT: Выявление неисправностей
2018-01-15, 18:41
Автор
=====
Как быстро спроектировать сайт с помощью CSS Grid
2017-12-04, 00:14
Пользователь
=====
Основы регулярных выражений в JavaScript
2017-12-04, 12:01
Пользователь
=====
Вероятностная интерпретация классических моделей машинного обучения
2017-12-04, 16:17
Этой статьей я начинаю серию, посвященную генеративным моделям в машинном обучении. Мы посмотрим на классические задачи машинного обучения, определим, что такое генеративное моделирование, посмотрим на его отличия от классических задач машинного обучения, взглянем на существующие подходы к решению этой задачи и погрузимся в детали тех из них, что основаны на обучении глубоких нейронных сетей. Но прежде, в качестве введения, мы посмотрим на классические задачи машинного обучения в их вероятностной постановке.
Две классические задачи машинного обучения — это классификация и регрессия. Давайте посмотрим ближе на каждую из них. Рассмотрим постановку обеих задач и простейшие примеры их решения.
Задача классификации — это задача присвоения меток объектам. Например, если объекты — это фотографии, то метками может быть содержание фотографий: содержит ли изображение пешехода или нет, изображен ли мужчина или женщина, какой породы собака изображена на фотографии. Обычно есть набор взаимоисключающих меток и сборник объектов, для которых эти метки известны. Имея такую коллекцию данных необходимо автоматически расставлять метки на произвольных объектах того же типа, что были в изначальной коллекции. Давайте формализуем это определение.
Допустим, есть множество объектов . Это могут быть точки на плоскости, рукописные цифры, фотографии или музыкальные произведения. Допустим также, что есть конечное множество меток . Эти метки могут быть пронумерованы. Мы будем отождествлять метки и их номера. Таким образом  в нашей нотации будет обозначаться как . Если , то задача называется задачей бинарной классификации, если меток больше двух, то обычно говорят, что это просто задача классификации. Дополнительно, у нас есть входная выборка . Это те самые размеченные примеры, на которых мы и будем обучаться проставлять метки автоматически. Так как мы не знаем классов всех объектов точно, мы считаем, что класс объекта — это случайная величина, которую мы для простоты тоже будем обозначать . Например, фотография собаки может классифицироваться как собака с вероятностью 0.99 и как кошка с вероятностью 0.01. Таким образом, чтобы классифицировать объект, нам нужно знать условное распределение этой случайной величины на этом объекте .
Задача нахождения  при данном множестве меток  и данном наборе размеченных примеров  называется задачей классификации.
Чтобы решить эту задачу, удобно переформулировать ее на вероятностном языке. Итак, есть множество объектов  и множество меток .  — случайная величина, представляющая собой случайный объект из .  — случайная величина, представляющая собой случайную метку из . Рассмотрим случайную величину  с распределением , которое является совместным распределением объектов и их классов. Тогда, размеченная выборка — это сэмплы из этого распределения . Мы будем предполагать, что все сэмплы независимо и одинаково распределены (i.i.d в англоязычной литературе).
Задача классификации теперь может быть переформулирована как задача нахождения  при данном сэмпле .
Давайте посмотрим, как это работает на простом примере. Положим , , , , . То есть, у нас есть две гауссианы, из которых мы равновероятно сэмплируем данные и нам нужно, имея точку из , предсказать, из какой гауссианы она была получена.
Рис. 1. Плотности распределения  и .
Так как область определения гауссианы — вся числовая прямая, очевидно, что эти графики пересекаются, а значит, есть такие точки, в которых плотности вероятности  и  равны.
Найдем условную вероятность классов:
Т.е.

Вот так будут выглядеть график плотности вероятностей :
Рис. 2. Плотности распределения ,  и .  там, где две гауссианы пересекаются.
Видно, что близко к модам гауссиан уверенность модели в принадлежности точки конкретному классу очень высока (вероятность близка к нулю или единице), а там, где графики пересекаются модель может только случайно угадывать и выдает .
Большая часть практических задач не может быть решена вышеописанным способом, так как  обычно не задано явно. Вместо этого обычно имеется набор данных  с некоторой неизвестной совместной плотностью распределения . В таком случае для решения задачи используется метод максимального правдоподобия. Формальное определение и обоснование метода можно найти в вашей любимой книге по статистике или по ссылке выше, а в данной статье я опишу его интуитивный смысл.
Принцип максимизации правдоподобия говорит, что если есть некоторое неизвестное распределение , из которого есть набор сэмплов , и некоторое известное параметрическое семейство распределений , то для того, чтобы  максимально приблизило , нужно найти такой вектор параметров , который максимизирует совместную вероятность данных (правдоподобие) , которое еще называют правдоподобием данных. Доказано, что при разумных условиях эта оценка является состоятельной и несмещенной оценкой истинного вектора параметров. Если сэмплы выбраны из , то есть данные i.i.d., то совместное распределение распадается на произведение распределений:

Логарифм и умножение на константу — монотонно возрастающие функции и не меняют положений максимумов, потому совместную плотность можно внести под логарифм и умножить на :

Последнее выражение, в свою очередь, является несмещенной и состоятельной оценкой ожидаемого логарифма правдоподобия:

Задачу максимизации можно переписать как задачу минимизации:

Последняя величина называется кросс-энтропией распределений  и . Именно ее и принято оптимизировать для решения задач обучения с подкреплением (supervised learning). 
Минимизацию на протяжении этого цикла статей мы будем проводить с помощью Stochastic Gradient Descent (SGD), а точнее, его расширения на основе адаптивных моментов, пользуясь тем, что сумма градиентов по подвыборке (так называемому “минибатчу”) является несмещенной оценкой градиента минимизируемой функции.
Давайте попробуем решить ту же задачу, что была описана выше, методом максимального правдоподобия, взяв в качестве параметрического семейства  простейшую нейронную сеть. Получившаяся модель называется логистической регрессией. Полный код модели можно найти тут, в статье же освещены только ключевые моменты.
Для начала нужно сгенерировать данные для обучения. Нужно сгенерировать минибатч меток классов и для каждой метки сгенерировать точку из соответствующей гауссианы:
Определим наш классификатор. Он будет простейшей нейронной сетью без скрытых слоев:
И запишем функцию потерь — кросс-энтропию между распределениями реальных и предсказанных меток:
Ниже приведены графики обучения двух моделей: базовой и с L2-регуляризацией:
Рис. 3. Кривая обучения логистической регрессии.
Видно, что обе модели быстро сходятся к хорошему результату. Модель без регуляризации показывает себя лучше потому, что в этой задаче не нужна регуляризация, а она слегка замедляет скорость обучения. Давайте взглянем поближе на процесс обучения:
Рис. 4. Процесс обучения логистический регрессии.
Видно, что обучаемая разделяющая поверхность постепенно сходится к аналитически вычисленной, при чем, чем она ближе, тем медленнее сходится из-за все более слабого градиента функции потерь.
Задача регрессии — это задача предсказания одной непрерывной случайной величины  на основе значений других случайных величин . Например, предсказание роста человека по его полу (дискретная случайная величина) и возрасту (непрерывная случайная величина). Точно так же, как и в задаче классификации, нам дана размеченная выборка . Предсказать значение случайной величины напрямую невозможно, ведь она случайная и, по сути, является функцией, поэтому формально задача записывается как предсказание ее условного ожидаемого значения:
Давайте посмотрим, как решается задача регрессии на простом примере. Пусть есть две независимые случайные величины . Например, это высота дерева и нормальный случайный шум. Тогда мы можем предположить, что возраст дерева является случайной величиной . В таком случае по линейности математического ожидания и независимости  и :
Рис. 5. Линия регрессии задачи про линейно зависимые величины с шумом.
Давайте сформулируем задачу регрессии через метод максимального правдоподобия. Положим ). Где  — новый вектор параметров. Видно, что мы ищем  — математическое ожидание , т.е. это корректно поставленная задача регрессии. Тогда
Состоятельной и несмещенной оценкой этого матожидания будет среднее по выборке
Таким образом, для решения задачи регрессии удобно минимизировать среднеквадратичную ошибку на обучающей выборке.
Давайте попробуем решить ту же задачу, что была выше, методом из предыдущего раздела, взяв в качестве параметрического семейства  простейшую возможную нейронную сеть. Получившаяся модель называется линейной регрессией. Полный код модели можно найти тут, в статье же освещены только ключевые моменты.
Для начала нужно сгенерировать данные для обучения. Сначала мы генерируем минибатч входных переменных , после чего получаем сэмпл исходной переменной :
Определим нашу модель. Она будет простейшей нейронной сетью без скрытых слоев:
И запишем функцию потерь — L2-расстояние между распределениями реальных и предсказанных значений:
Ниже приведены графики обучения двух моделей: базовой и с L2-регуляризацией:
Рис. 6. Кривая обучения линейной регрессии.
Рис. 7. График изменения первого параметра с шагом обучения.
Рис. 8. График изменения второго параметра с шагом обучения.
Видно, что обе модели быстро сходятся к хорошему результату. Модель без регуляризации показывает себя лучше потому, что в этой задаче не нужна регуляризация, а она слегка замедляет скорость обучения. Давайте взглянем поближе на процесс обучения:
Рис. 9. Процесс обучения линейной регрессии.
Видно, что обучаемое математическое ожидание  постепенно сходится к аналитически вычисленному, при чем, чем оно ближе, тем медленнее сходится из-за все более слабого градиента функции потерь.
В дополнение к изученным выше задачам классификации и регрессии есть и другие задачи так называемого обучения с учителем, в основном сводящиеся к отображению между точками и последовательностями: Object-to-Sequence, Sequence-to-Sequence, Sequence-to-Object. Так же есть и большой спектр классических задач обучения без учителя: кластеризация, заполнение пробелов в данных, и, наконец, явная или неявная аппроксимация распределений, которая и используется для генеративного моделирования. Именно о последнем классе задач будет идти речь в этом цикле статей.
В следующей главе мы посмотрим, что такое генеративные модели и чем они принципиально отличаются от рассмотренных в этой главе дискриминативных. Мы посмотрим на простейшие примеры генеративных моделей и попробуем обучить модель, генерирующую сэмплы из простого распределения данных.
Спасибо Olga Talanova  за ревью этой статьи. Спасибо Sofya Vorotnikova за комментарии, редактирование и проверку английской версии. Спасибо Andrei Tarashkevich за помощь в верстке.
Data Science/Machine Learning
=====
Кроссплатформенный IoT: Операции с устройствами
2018-01-11, 16:27
Автор
=====
Немного про .NET Framework и .NET Core [плюс полезные ссылки]
2017-12-20, 10:26
Автор
=====
Дайджест свежих материалов из мира фронтенда за последнюю неделю №291 (27 ноября — 3 декабря 2017)
2017-12-04, 01:07

Просим прощения за возможные опечатки или неработающие/дублирующиеся ссылки. Если вы заметили проблему — напишите пожалуйста в личку, мы стараемся оперативно их исправлять. 
User
=====
Где моя оплата? Как мошенники зарабатывают на фрилансерах
2017-12-04, 03:28
Web-разработчик
=====
Нейросеть для определения лиц, встроенная в смартфон
2017-12-04, 10:20
автор, переводчик, редактор
=====
DotNext 2018 Piter Release Notes
2017-12-04, 12:44

Развитие конференции очень похоже на развитие программной платформы. Наши языки и динамические рантаймы постоянно конкурируют между собой: например, Java гонится за .NET по синтаксису основного языка, а .NET пытается догнать Java по кроссплатформенности.
IDE для наших платформ эволюционируют более непосредственно. Представьте обновление Rider — одного из основных IDE для .NET, построенного на основе IntelliJ Platform. Каждый раз, когда обновление получает базовая платформа, обновляются и все связанные проекты. Теоретически, если улучшить механизм PSI, апгрейд будет не только у Rider, но и у IntelliJ IDEA — самой популярной IDE для Java. Но некоторые фичи, возможно, потребуют дополнительного допиливания. 
Наши конференции устроены еще лучше. Каждый раз, когда обновление получает главная .NET-конференция (DotNext) или главная Java-конференция (Joker), обновления совершенно кроссплатформенно распространяются на все продукты. 
Если в DotNext мы вводим дополнительные механики социального взаимодействия участников конференции, то ими в дальнейшем будет пользоваться и Joker. Если в записи и прямой трансляции JPoint мы повышаем качество картинки до 1440p и добавляем возможность одновременно видеть спикера и слайды, эту фичу получает DotNext. Правильная архитектура платформы — залог успеха.
Интересно, что, участвуя в конференции на одной из сторон, ты попадаешь прямо в этот стремительный поток непрерывно эволюционирующих технологий. Ты видишь, как спикеры (если они из Microsoft или Oracle) рассказывают о новинках, сделанных для приближения друг к другу. Ты слушаешь, как герои из JetBrains пытаются усидеть на двух стульях (на самом деле, стульев больше). И ты можешь раз за разом наблюдать, как твоя любимая конференция эволюционирует вместе с ними, получая новые фичи и оттачивая старые до зеркального блеска.
Давайте посмотрим, какие новые фичи ждут нас в 2018 году.
Мы умеем делать клевые конференции, но просто клевые конференции больше не возбуждают. Мы тщательно проанализировали фидбэк, и в этот раз решили сфокусироваться на вовлечении участников в активное взаимодействие: чтобы люди общались, задавали вопросы, предлагали свои темы и решения и так далее. Как мы будем это делать?
Будут хорошие новые доклады, которые интересно обсудить. Тут всё понятно: они были, есть и будут, это суть наших конференций. Кроме них есть еще несколько важных вещей.
Во-первых, дискуссионные зоны. Если вы у нас еще не были, то следующая пара абзацев — небольшой курс молодого бойца дискуссионных зон.
Обычно вопросы после доклада жёстко ограничены во времени: есть несколько минут, а потом приходится освобождать зал. В нашем случае, этим дело не ограничивается: каждый спикер после выступления перемещается в специально отведённую зону, где заинтересованные слушатели могут продолжать общаться с ним куда дольше.

Дискуссионные зоны — это ваш инструмент как слушателя. Ими нужно уметь пользоваться, а не просто ждать у моря погоды. Узнав о наличии дискуссионной зоны, люди думают: «Окей, я всегда смогу задать свой вопрос». Нет, зачастую желающих слишком много. Можно не успеть выведать у спикера всё, что вам нужно, за отведенный час. Даже если спикер по доброй воле решил остаться надолго, если это какой-то очень важный в сообществе человек, толпа вокруг него может не исчезать часами. Если докладчика «допрашивают» излишне долго, он может просто перегореть, и свой вопрос вы так и не зададите. Кроме того, дискуссия может зайти в неинтересное вам русло — поэтому нужно активно принимать участие в её формировании. Поднимайте интересные вам темы, подходите поближе. Часто дискуссионные зоны расположены там, где много народу, плюс вокруг спикера собирается внушительная толпа. Если стоять слишком далеко, будет очень плохо слышно, вопрос издалека не задать и к разговору не подключиться. Если честно, нужно всегда прорываться в первый ряд — во втором ряду слышимость уже хуже. Задавайте вопросы в Telegram-канале конференции — спикеры сами им пользуются и читают, что вы там пишете.
Пара слов про Telegram-канал. Он есть, и его действительно читают. Более того, когда доклад завершается, и наступает время задавать вопросы, трекоунеры зачитывают из Telegram избранные вопросы.
Во-вторых, общение между участниками. Почему этому уделяется такое внимание? Часто мы получаем отзывы типа: «Я два дня ходил, никто со мной не общается, я не знаю, как подойти к людям — это будет выглядеть глупо, что мне делать?» Поэтому мы решили помочь наладить общение и вне дискуссионных зон. Конечно же, будет вечеринка, на ней хотят выступить наши докладчики — Дилан Битти и Вагиф. В Москве это было вот так, и всем очень понравилось:

В-третьих, будет круглый стол. Мы очень долго боялись круглых столов, сознательно не включали их в программу — мало у кого они получаются хорошо. Было даже мнение, что проще сделать пустой слот, чем круглый стол. Но первые же попытки показали, что у нас это вышло замечательно. В прошлый раз на DotNext был особенно хороший круглый стол, и нам кажется, вот почему: прекрасный модератор, отличные эксперты, и, главное, — дружелюбная аудитория, задающая интересные вопросы. Кстати, там было несколько вопросов из Telegram.



В-четвертых: будет открытый микрофон. Это новая для нас практика, заключающаяся в следующем: в программе есть один свободный слот, который можно заполнить любым докладом и любой темой. Утром в фойе появляется флипчарт, размеченный сеткой всевозможных тем. Любой участник может с помощью стикера, лежащего рядом с флипчартом, отметить тему, которая ему максимально интересна. Просто вклеить стикер в квадрат с нужной темой. Тема, которая наберет больше стикеров, и займет свободный слот. Возможно, ближе к конференции мы что-то поменяем, но общий смысл — такой.
Есть другие варианты организации открытого микрофона, но они довольно сложные. Например, утром выставить несколько досок и дать спикерам возможность писать на них собственные темы, типа: «Я хочу рассказать про .NET Core 3.0». После обеда наступает вторая фаза: слушатели смотрят на эти темы и точно так же отмечают стикерами наиболее понравившиеся. Третья фаза: модераторы анализируют темы и стикеры, и из самых топовых тем формируют программу следующего дня в одном отдельно взятом зале. К сожалению, согласно мировой практике, этот формат — не самый удачный, для успеха ему требуется серьезная доводка и улучшения. Поэтому в России нигде такого не делают, и мы тоже пока не делали, и на DotNext 2018 Piter делать не будем.
Для разнообразия, расскажем о практике, которая есть повсеместно, а у нас — нет, потому что она не работает. На зарубежных конференциях ставят столики с обозначенной темой: например, на столике стоит бумажка с надписью «.NET Core». Предполагается, что люди на перерыве будут собираться вокруг такого столика и обсуждать .NET Core. На самом деле, это не работает. Люди говорят о чем угодно, кроме «темы столика» — о машинах, о снеге на улице, о политике… В результате там сидят те, кому негде больше сесть. Убого как-то, да?
Как же люди будут находить друг друга? Мы в JUG.ru Group про это думали-думали, и так ничего и не придумали.
Изучая программу конференции, вы увидите напротив каждого доклада пиктограмму — «смузи», «бородач» и «хардкор».

Многие неверно интерпретируют это как «сложность темы доклада» и в результате идут на неподходящие доклады.
Одна из лучших вещей, которые вообще можно сделать этим хабрапостом — объяснить истинный смысл обозначений.
«Смузи» — это такой доклад, для которого не нужно иметь глубоких знаний о теме обсуждения. Всё необходимое сообщается по ходу доклада. Например, слушатель может долго работать с фреймворком, но не имеет глубоких знаний в каком-то его аспекте. По ходу доклада об этом аспекте всё расскажут. Несмотря на то, что такая тема сложна, вся необходимая информация для её понимания полностью рассказывается докладчиком.
«Бородач» — это такой доклад, для которого нужно иметь опыт работы с технологией. Эти доклады созданы для практикующих инженеров, которые знают толк в обсуждаемой теме, поэтому не нужно тратить время на мелочи типа объяснения терминологии и типичных способов делать вещи. Тут дается сконцентрированная информация.
Теперь — сноски со звездочкой*, как это бывает в контрактах.
Бывает так, что некоторые доклады — «хардкорные», но почему-то они стоят под «бородачами», и наоборот. Тут есть еще такая вещь, как способ подачи. Некоторые спикеры имеют невероятно интересный контент, но очень сложно подают его. Например, спикер решил в сравнительно простой теме удариться в математическое доказательство какого-то утверждения, и хардкор возникает просто потому, что он решил объясняться таким способом. Тем не менее, его доказательство — важное, и просить у докладчика выбросить этот кусок не имеет смысла.
А есть спикеры, которые придумывают интересные способы и хаки, как сложные вещи объяснить максимально простым образом. Например, мне кажется, что Саша Гольдштейн умеет так подобрать слова, что сложные вещи кажутся не такими уж хардкорными. Хотя его доклады у нас почти всегда помечены «козой» — это соответствует их истинному уровню.
Отдельный вопрос — паззлеры. С одной стороны, паззлеры — это весело и прикольно, и даже неподготовленные слушатели могут туда приходить. Но оказывается, что по характеру доклада паззлеры — это или «бородач», или «хардкор». Если человек пришел туда просто повеселиться, и реально ничего не практикует, абсолютно всё содержание доклада пройдет мимо него.
Еще можно рассказать про четвертую категорию докладов, которой у нас никогда не бывает — маркетинг и product placement-доклады. В мировой практике принято четко обозначать такие доклады в программе: должно быть сразу видно, что это для того, чтобы вы купили или начали практиковать что-то, нужное докладчику. Но у нас таких докладов не бывает — и это ключевая фишка нашей конференции. Никакого маркетингового буллшита, только реально интересный, полезный и актуальный материал. А вот за рубежом product placement-доклады — обычное дело, и это взрывает мозг морально неподготовленному слушателю.
По поводу контента нового DotNext 2018 Piter. Он будет всё такой же огненный, и около десяти докладов проходят подготовку прямо сейчас.
Я не буду копипастить на Хабр всю программу, а перенаправлю на соответствующую страницу на официальном сайте конференции. Тем более, что эта страница сейчас стремительно изменяется, отражая регистрацию новых спикеров и работу над программой.
Вместо этого пара слов о подготовке. Многие спикеры мирового уровня могут привезти с собой сразу несколько новинок, но еще не определились с выбором.


Например, Jeremy Likness. С одной стороны, он может взять вполне очевидную для него тему «Going Serverless with .NET», рассказав про то, как оверхед бессерверной платформы можно побороть с помощью правильной обработки событий (HTTP-запросов, таймеров, загрузки файлов и т.п.). Заодним рассказав про кроссплатформенную разработку на C# и употребив множество популярных слов типа IaaS, PaaS, SaaS, Serverless, Azure Functions, Azure Logic Apps, Azure AppInsights. 
С другой стороны, он же может рассказать о CosmosDB — документной NoSQL-базе данных, с помощью которой можно начать работать в парадигме serverless data. Георепликация и понятное latency идут в комплекте, а еще — выбор API по вкусу и поддержка DocumentDB, MongoDB, Gremlin, Table Storage, Cassandra. Всё это с лайв-кодингом и демонстрациями.
Первый доклад — откровенно полезный, второй — необычный, выбрать какой-то один из них — головная боль Программного комитета. Как говорил ведьмак Геральт из Ривии про выбор из двух зол: «Меньшее, бо́льшее, среднее — всё едино, пропорции условны, а границы размыты. Я не святой отшельник, не только одно добро творил в жизни. Но если приходится выбирать между одним злом и другим, я предпочитаю не выбирать вообще». У Программного комитета проблема посложней: во-первых, им нужно выбрать один из двух заранее великолепных докладов. Во-вторых — выбирать придется, потому что на DotNext 2018 Piter будет большое количество звездных докладчиков, и упаковать их всего в два дня — задача непростая.


Jeremy — не единственный человек с двумя докладами. Например, Maarten Balliauw (основатель MyGet.org и developer advocate в JetBrains) может рассказать как о диагностике приложений для Azure, так и о DNS. Опять этот выбор: все мы делаем облачные приложения, а тема семантического логирования для Azure и вещи типа AppInsights и AppInsights Analytics актуальны как никогда. С другой стороны, есть очень интересный и необычный доклад про то, как взять DNS (казалось бы, технологию столетней давности), проанализировать её лучшие стороны (почему она так хороша, что продолжает быть актуальна в 2017 году) и показать, как DNS можно использовать для реализации вещей, для которых эта технология и не предназначена вовсе.


Кроме новых докладчиков будут и всем известные люди. Например, совершенно точно приедет Саша Гольдштейн — автор книг, текстов и постов, регулярный спикер DotNext и бесконечного количества других конференций и так далее и тому подобное, специализирующийся на перфомансе и отладке в мире .NET и C++. С выбором возможных новых докладов у него также всё очень хорошо.


Будет Дилан Битти. Кроме того, что он играет на гитаре на афтепати DotNext и носит огромную черную шляпу, Дилан до недавних пор был известен как системный архитектор в Spotlight, занимающийся разработкой огромных распределенных систем и дизайном идеально красивых API (а теперь перешёл в SkillsMatter). Будет Federico Lois с докладом «Scratched metal: Hardware intrisic at RavenDB» — он покажет, как работать с аппаратными интринсиками, параллельно помогая коллегам из Intel (или подпинывая их?) на реализацию инструкций, нужных нам для vNext.


Мы смогли уговорить Андрея Акиньшина ещё раз выступить с докладом. В прошлый раз у него случился полный аншлаг. Несмотря на то, что у него был кейноут, и он не стал лидером по времени работы с аудиторией в дискуссионной зоне, суммарное количество вложенных им в конференцию сил потрясало и ужасало. Кроме того, он участвовал в Программном комитете. После своего выступления он был как бы везде одновременно и отвечал на все вопросы сразу. К концу второго дня Андрей начал выгорать, и я боялся, что однажды он выйдет в дверь в произвольном направлении и больше не вернется. Но нет, суровые испытания делают Андрея только сильнее, и на DotNext 2018 Piter он появится с новой порцией хардкора.


Ну и на закуску, хочется рассказать об одном из наиболее известных спикеров. К нам приедет «тот самый» Андрей Александреску — соавтор языка программирования D, автор книг и статей (включая «Modern C++ Design: Generic Programming and Design Patterns», которая когда-то необратимо повлияла на помыслы C++ программистов во всём мире), эксперт во множестве связанных тем типа машинного обучения и обработки естественных языков. Андрей расскажет о современных челленджах в области оптимизации и на конкретных примерах покажет принципы разработки алгоритмов, которые можно применять ежедневно с целью их улучшения (или иногда — полного их редизайна). Бенчмаркинг, параллелизм, связность данных и так далее — в общем, всё, что и ожидается от Андрея. Более подробно содержание доклада я спойлерить пока не буду, тем более что, по мере приближения DotNext 2018 Piter, мы собираемся опубликовать еще несколько постов на Хабре, раскрывающих программу конференции и темы докладов. С несколькими спикерами будет отдельное интервью.
Уже сейчас очевидно, что программа DotNext 2018 Piter будет хороша. Но если хочется сделать её еще лучше — приходите со своим собственным докладом. Как подать заявку — написано на сайте.
кибер-ниндзя
=====
Собираем пользовательскую активность в JS и ASP
2017-12-05, 15:11
User
=====
TypeScript: Библиотека tslib
2017-12-04, 11:14
Перевод. Оригинал по ссылке.
В некоторых случаях компилятор TypeScript вставляет вспомогательные функции в сгенерированный JavaScript код, которые потом вызываются во время исполнения. Каждая такая вспомогательная функция эмулирует семантику особенности языка, которая ещё не поддерживается браузерами нативно.
В настоящее время в TypeScript существуют следующие вспомогательные функции:
Например, если следующий код:
скомпилировать в стандарт ES5, который не поддерживает ни класс, ни расширения, то на выходе получится:
Это подходит только для простых примеров, подобных тому, что описан выше. На деле же он имеет огромный недостаток.
Вспомогательная функция __extends вставляется в скомпилированный результат для каждого файла, в котором вы используете наследование класса. Это значит, что для каждого компонента React на основе класса вставляется функция-хелпер.
Для средних размеров приложения, включающего в себя десятки или сотни React компонентов, это выльется в большое количество повторяющегося кода только для функции __extends. Это увеличивает размер бандла, вследствие чего растёт время загрузки файла.
Эта проблема только усиливается, когда в скомпилированном результате используются другие хелперы. Вспомогательные функции __awaiter и __generator огромны, и делают существенный вклад в размер бандла. Запомните, они вставляются для каждого файла, в котором вы используете async/await.
Начиная с версии 1.5, TypeScript поддерживает флаг --noEmitHelpers. Если компилятор видит этот флаг, то TypeScript не будет вставлять вспомогательные функции в скомпилированном коде.
React компонент, описанный выше, скомпилированный с флагом --noEmitHelpers:
Обратите внимание, что функция __extends всё ещё вызывается. В конце концов, главное, чтобы компонент React работал. Если вы используете флаг --noEmitHelpers, то ваша обязанность предоставить все необходимые функции-хелперы. TypeScript предполагает, что они будут доступны во время исполнения.
Однако, слишком трудозатратно делать это вручную. Вы должны проверить какие вспомогательные функции нужны и затем каким-то образом сделать их доступными в приложении. Перспектива совсем не радужная! К счастью, команда TypeScript разработала решение.
В версии 2.1 появился новый флаг --importHelpers, который говорит компилятору импортировать функции из внешней библиотеки помощников tslib, а не встривать их для каждого файла. Вы можете установить и tslib так же, как и любой другой пакет npm:
Давайте перекомпилируем наш React компонент. На этот раз с использованием флага --importHelpers:
Обратите внимание на require("tslib") на второй строчке и использвание tslib_1.__extends на пятой. В этом файле больше нет вспомогательной функции. Вместо этого функция __extends импортируется из модуля tslib. Таким образом, каждый хелпер импортируется только один раз, и вы не будете наказаны за использование extends и async/await во многих файлах.

Frontend developer
=====
Дневник техлида: полгода разработки мобильного PvP
2017-12-04, 13:57
Ведущий разработчик мобильных игр и приложений.
=====
Рубрика «Читаем статьи за вас». Октябрь — Ноябрь 2017
2017-12-11, 14:03

Привет, Хабр! По традиции, представляем вашему вниманию дюжину рецензий на научные статьи от членов сообщества Open Data Science из канала #article_essense. Хотите получать их раньше всех — вступайте в сообщество ODS!
Статьи выбираются либо из личного интереса, либо из-за близости к проходящим сейчас соревнованиям. Напоминаем, что описания статей даются без изменений и именно в том виде, в котором авторы запостили их в канал #article_essence. Если вы хотите предложить свою статью или у вас есть какие-то пожелания — просто напишите в комментариях и мы постараемся всё учесть в дальнейшем.
Статьи на сегодня:
Авторы статьи: Bo Tian, 2016
→ Оригинал статьи
Автор обзора: BelerafonL 
Идея в добавлении к весам нейронов гауссовского шума, чтобы получилось что-то типа дропаута. Только если дропаут действует обычно на выходы узлов, то здесь шум добавляется к весам. Но не это главное, а то, что автор находит аналитическое решение для выхода сети при бесконечном ансамбле таких сетей с разным сочетанием семплов шума в весах. Т.е. задается желаемый уровень шума весов (сигма), а найденное решение показывает, что выдала бы сеть если бы долго семплировать выход такой шумной сети и усреднять результат.
Автор для задачи разделения двух спиралей на плоскости показывает, как отличается решение обычной сети и его. Он рисует карту активации, и для обычной сети и она получается изрезанная и рваная, а для его решения гладкая и точно повторяет форму спиралей.
К сожалению, автор не проверяет предложенный способ на других задачах и использует сеть всего с одним скрытым слоем. Кроме того, он использует метод оптимизации LMA (Алгоритм Левенберга—Марквардта), но говорит, что можно использовать любой другой, в том числе обычный бекпроп. 
Посыл статьи простой — бесконечный ансамбль сетей, отличающихся на величину шума в весах отлично обобщает и находит лучший минимум, кроме того, уровень шума весов можно изменять по мере обучения сети, и тогда можно найти желаемое соотношение точность/обобщение. И так как решение найдено аналитическое, то вычислительных затрат на это почти никаких. 
В статье много формул, отчего я не понял применимость метода для практических сложных задач, есть ли там какие подводные камни. Поэтому прошу более опытных исследователей посмотреть и прокомментировать статью, уж больно красивый получается результат эдакой регуляризации.
Также есть заметка автора на stackexchange где он в двух словах объясняет концепцию.
Авторы статьи: Zhou Z, Feng J, 2017
→ Оригинал статьи
Автор обзора: Dumbris 
В статье AutoEncoder by Forest, авторы Zhou Z, Feng J предложили альтернативный метод построения авто-энкодеров, основанный на ансамбле (Random Forest, Gradient boosted tree) деревьев. Предложенный метод они назвали eForest. 
Согласно экспериментам авторов, eForest показывает лучшие результаты на задачах MNIST и CIFAR-10, в сравнении с авто-энкодерами, построенными на основе Multilayer Perceptron и Convolutional Neural Network. А в задаче восстановления текста на датасете IMDB — eForest в 200 раз превзошел по точности MLP.

Основные преимущества метода eForest:
Как работает предложенный метод
Автоэнкодер имеет две основные функции кодирование и декодирование.
Кодирование. Для получения закодированного представления, нужно построить лес F деревьев T и сохранить индексы листов из всех деревьев для каждого x. Таким образом для входных данных x получим вектор длинной T.

Декодирование — имея лес F и индексы листов для всех x можно восстановить путь в дереве от листа к вершине. Этот путь авторы записывают в виде конъюнкции логических выражений, взятых из узлов решающего дерева. Пример (x1=>0)^(x2=>5)^:(x3==RED)^:(x1>2:7)^:(x4 == NO). Такое выражение они называют RULE. 
Множество RULEs для конкретного сэмпла из всех деревьев T можно упростить, до одного правила, которое Zhou Z, Feng J называют: Maximal-Compatible Rule (MCR). Авторы утверждают, в форме теоремы, что все оригинальные сэмплы буду находиться внутри регионов определенных MCR.
По правилам записанным в MCR, можно сгенерировать данные, которые будут похожи на исходные. В статье есть описание алгоритмов, в формате псевдокода.
Судя по Table 4, процедура построения MCR и декодирования занимает сильно больше времени, по сравнению с фазой декодирования в NN. Но авторы надеются этот момент оптимизировать в будущем.
В начале года, от этих же авторов был более общий paper: Deep Forest: Towards An Alternative to Deep Neural Networks.
Авторы статьи: Anh L. T. Arkhipov M. Y, Burtsev M. S, 2017
→ Оригинал статьи
Автор обзора: Dumbris 
Первая публикация в рамках проекта iPavlov. По утверждениям авторов — SOTA для NER на русских текстах. 
Датасеты
Для экспериментов были использованы датасеты Gareev’s (топ цитируемых статей из Yandex News), Person-1000 и FactRuEval. В качестве бейзлайна, была взята Bi-LSTM сеть с Conditional Random Fields (CRF) слоем. Применение CRF дает существенный прирост точности по сравнению с чистым Bi-LSTM на задаче NER.
Архитектура сети, эксперименты
На входе для каждого слова вычисляется его word level и char level представление, оба представления конкатенируются в один вектор и подаются на вход Bi-LSTM. После Bi-LSTM, CRF слой присваивает каждому слову тэг, по которому можно определить является ли слово именем человека, или названием организации. 

В ходе экспериментов, также были исследованы расширения известной архитектуры NeuroNER при помощи highway network. C highway network сеть получает возможность учиться “отключать” некоторые из внутренних слоев (carry gate), пропуская низкоуровневые представления входных данных ближе к выходным слоям. Реализуется этот механизм с помощью sigmoid layer. Получается, что сеть может сама управлять сложностью архитектуры в зависимости от входных данных.

Вместе с тем, на практике было выяснено, что использование правильно приготовленных word embeddings дает прирост точности в комбинации с Bi-LSTM-CRF. Для тренировки эмбедингов, были использованы корпуса News by Kutuzov и Lenta.
Результаты
Наиболее точной оказалась модель Bi-LSTM-CRF + word embedding, созданные на основе корпусов русских новостей. Комбинация NeuroNER + highway network тоже показала себя хорошо. Одно из перспективных направлений использование character level CNN архитектуры, вместо LSTM. В рамках данной работы оно не было исследовано.
Авторы статьи: Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten, 2016
→ Оригинал статьи
Автор обзора: N01Z3 
Относительно свежая идея архитектуры, которая прошла как-то незаметно, но показала в нашем чатике лучшую single model performance на Kaggle Amazon from Space.

Итак, парни предложили Dense Block. Каждый базовый элемент это BN -> ReLU -> Conv. И каждый базовый элемент прокидывает свои фичи к всем следующем элементам в блоке. Фичи конкатенируются, а не суммируются, поэтому линейно числу фильтров (авторы называют это Growth rate) растет количество каналов. Но это компенсируется тем, что сами базовые элементы более узкие. Также чтобы уменьшить рост числа каналов после каждого Dense Block’a втыкают базовый элемент с свертками (1, 1) и количеством фильтров в два раза меньше, чем каналов к этому моменту в основном бранче. А затем, обычный Pooling с Stride’ом 2.
Все. А дальше идет размахивание руками, что фичи более грамотно переиспользуются, что за счет этого эффективно все считается. Еще в этом случае дропаут в свертках выглядит как-то более осмысленным. Также в следующих работах парни показали, что можно получать оценочную классификацию уже после первых блоков и тем самым не гнать инференс дальше в случае уверенных предсказаний.
Авторы статьи: Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng, 2017
→ Оригинал статьи, → Исходный код
Автор обзора: N01Z3 
Очередное переосмысления связи между сверточными и рекурентными сетями. Для начала авторам понравился Resnet и Densnet. Первый это суть рекурентная сеть развернутая вдоль основного бранча, а вторая это рекурентная сесть высокого порядка (HORNN) (серия картинок 1). И парни решили, что надо слить обе идеи в одну. Так родилась DPN.
Чтобы понять как они объединили, серия картинок 2 

a) классический Resnet — основная бранч фиксированного размера, фичи складываются
b) обычный Densenet — основной бранч растет, фичи конкатинируются
c) обычный Densenet — эквивалентная схема. Переосмысление в том плане, что все-таки есть основной бранч фиксированного размера от входа в DenseBlock и вспомогательный, которые растет.
d) моднейший Dual Path блок — есть бранч фиксированного размера, есть бранч который растет. Фичи берутся из каждого, прогоняются через слой сверток, разделяются и докидываются в каждый бранч. В фикcированный бранч — складываются, в растущий — конкатенируются
e) моднейший Dual Path блок — эквивалентная схема Dual Path.
В каждом блоке на входе и выходе используются свертки (1,1), чтобы каналы сошлись. А основные свертки (3,3) используются не обычные, а Separable. То есть по сути за базу взят не Resnet, а ResNeXt.
Авторы статьи: Mikhail Khodak, Nikunj Saunshi, Kiran Vodrahalli, 2017
→ Оригинал статьи
Автор обзора: yorko 
Распознавание сарказма – очень нетривиальная задача, с которой не то что сетки, а люди далеко не всегда справляются. Если уж человек с его 10^10 нейронов порой спрашивает “Погодь… а ты меня случаем не троллишь?“, то можно представить, что распознавание сарказма – это действительно один из большущих челленджей машинного обучения. 
Чуваки разметили самый большой на текущий момент датасет по сарказму – 1.3 млн. саркастических утверждений с Reddit. К тому же, рядом валяются еще полмиллиарда несаркастических фраз, так, чтобы на досуге попрактиковаться в imbalanced classification. 
Ребята сразу заявили, что их основной вклад – это датасет, а не методология выявления сарказма. Зато датасет на порядок больше прежних и намного качественней, по их словам, чем с Твиттера, где люди сами ставят теги #sarcasm или #irony. На Реддите юзеры тоже отмечают, что троллят тегом “/s”, и авторы признают, что такая разметка тоже шумная.

Любопытно, что чуваки не собирали сообщения, идущие в треде после саркастического утверждения. Типа там дальше идет треш-угар-троллинг, сарказмом отвечают на сарказм, и разметка слишком шумной получается.
Авторы оценили качество полученной разметки, выбрав по 500 случайных саркастических и обычных сообщений и проверив разметку самостоятельно. Зарепортили 1% False Positive Rate (когда обычное сообщение пришло с меткой “сарказм“) и 2% False Negative. В случае сбалансированного обучения это норм, а вот если обучаться с кучей не-сарказма (99.75%), то вот 2% False Negative Rate – многовато. 
От себя добавлю: прикинул матрицу ошибок для разметки с Reddit и ассесорской разметки, при таком дисбалансе точность (precision) разметки с Reddit получается очень низкой: всего лишь в 11% случаев ассесоры подтверждают сарказм, если с Reddit пришел сарказм. Это вызывает вопросы: чем датасет так уж лучше остальных, того же Твиттера? Авторы утверждают, что все-таки лучше: на реддите намного чаще попадается сарказм (~ 1% против 0.002% в Твиттере) и вообще сообщения на Реддите, очевидно, длиннее. Ну ок… уговорили
Далее, авторы репортят несколько бейзлайнов в задаче определения сарказма на данных Реддита. Задача формулируется так: имеется сообщение на реддите и все комментарии к нему. Надо определить, какие из комментариев несут в себе сарказм. Насобирали 8.4 млн таких сообщений (как я понял объект – это исходное сообщение + коммент к нему), сарказма – 28% (сбалансировали выборку). Далее взяли по одному сообщению из треда с сарказмом и без, причем на основе GloVe-эмбеддингов отсекали слишком похожие друг на друга саркастические фразы (лавинноподобное дрожжевое метание говн).
Строили 3 логистических регрессии, признаки очень простые: Bag of Words, Bag of bigrams и GloVe-эмбеддинги (Amazon product corpus, размерность – 1600, представление документа (сообщения) – простое усреднение представлений отдельных слов). Также посадили бедных ассесоров размечать сарказм – набралось 5 человек, разметили по 100 сообщений, скудновато будет, можно было бы и механического турка напрячь (Amazon Mechanical Turk), ведь написание научной статьи – это такое доходное дело! 

Получилось, что биграммы дали лучший результат среди трех моделей (~76% accuracy в политике и 71% – в остальных сабреддитах), но человеческая разметка, конечно, лучше (83% и 82% – в среднем, 85% и 92% – ансамбль 5 ассесоров, голосование большинством). Авторы заключают, что эмбеддинги сыграли хуже, но возможно, именно с их помощью мы научимся передавать контекст, в котором применяется сарказм. 
В целом, и людям, и алгоритмам сложно обнаруживать сарказм. Ожидаю прогресса в этой задаче со стороны нейросетей – надо как-то научиться правильно учитывать контекст, в котором употребляется сарказм, то есть правильно этот контекст закодировать в скрытое представление, как оно сейчас делается в conditioned рекуррентных сетках. Эксперименты в описанной статье так себе, но то что ребята выложили большой датасет по сарказму – это, безусловно, здорово! Боюсь представить, что будет, когда машины реально научатся чувствовать сарказм лучше людей.
Авторы статьи: Han Xiao, Kashif Rasul, Roland Vollgraf, 2017
→ Оригинал статьи
Автор обзора: Ivan Bendyna
У ребят из Zalando тоже пригорает от фразы "state-of-the-art на MNIST", поэтому они сделали свой Fashion-mnist, который полностью повторяет структуру оригинального MNIST. В нем тоже 10 классов (одежда и обувь: shirt, t-shirt, sneakers, ...), 28x28 пикселей, 60000 train, 10000 test. В этом и есть основная идея датасета — можно просто подменить урл и проверить качество на чуть более сложном датасете. SOTA top-1 error около 3.5%, что на порядок больше ошибки на MNIST.

Есть бенчмарк основных классификаторов из sklearn, топовые результаты можно найти в Readme.md. Также там можно найти результат человека, который составляет всего 0.835, что впринципе неудивительно, если взглянуть на картинки глазами.
Датасет подготовлен по товарам с сайта zalando, процедура создания датасета хорошо описана в работе. Датасет уже есть в pytorch.
Еще узнал из статьи, что есть расширенный датасет EMNIST (62 класса рукописных символов) 
Мои мысли: 
Авторы статьи: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, 2017
→ Оригинал статьи
Автор обзора: Ruslan Grimov
В данной статье раскрывается внутреннее устройство капсул и описывается механизм роутинга – способ передавать выход от капсулы в текущем слое только на определённые капсулы в вышележащем слое. Это они называют построением Parse tree, где каждой node сопоставлена одна капсула.
Результат статьи: построение енкодера/декодера, обученного на MNIST и призыв изучать капсулы дальше. Енкодер — сеть CapsNet, состоит из обычного конволюционного слоя и двух слоёв с капсулами, один из которых конволюционный (см. далее). Декодер — просто fullconnected сеть. Код подаваемый в декодер — 10 капсул с 16 нейронами каждая. По одной на цифру.
Капсула — обособленная группа нейронов на слое. Между собой внутри капсулы они не связаны, но выход каждого нейрона зависит от выхода других в капсуле (см. формулы далее). Отдельная капсула отвечает за один объект. Каждый нейрон в капсуле учится представлять какое-то свойство объекта. В конце статьи будет картинка где они показывают как меняется форма цифр при изменении свойств (похоже на то, что действительно эти нейроны что-то знают). 
Нейроны в одинаковой позиции, но в разных капсулах одного слоя отвечают за разные свойства объектов (на сколько я понял). Само наличие объекта на сцене определяется длинной выходного вектора капсулы (корень от суммы квадратов активностей нейронов в капсуле). Соответственно. если на сцене два одинаковых объекта — то свойства будут смешаны.
Размерность капсул растёт от входного слоя сети к выходному. При этом информация из «позиционной» превращается в rate-coded (As we ascend the hierarchy more and more of the positional information is "rate-coded"). Простые свёрточные слои в начале сети рассматриваются как капсулы имеющие один нейрон.
Капсулы
В статье всё отталкивается от капсул. Потому дальше все действия описываются в отношении капсул, а не отдельных нейронов. То, есть рассматривается не input/output нейрона и weights между нейронами, а input/output и weights между капсулами. Просто имеем ввиду, что капсула — это вектор из нейронов.
j — индекс капсулы в текущем слое, i — в нижележащем (тот что ближе к входу).
Вход капсулы (вектор размерности m) s[j] = сумма по всем i от c[i, j]U[i, j], где
c[i, j] — некий коэффициент связанности (скаляр) между капсулами двух слоёв — результат работы роутинга (см. далее). Для капсулы из i сумма по всем j от c[i, j] = 1. То есть одна капсула из нижележащего слоя распределяет своё «влияние» на вышележащие неравномерно. Перед циклами роутинга все c[i, j] равны.
U[i, j] — в статье называется prediction (вектор размерности m) U[i, j] = W[i, j]u[i], где W[i, j] веса связей между нейронами капсул i и j (размерность n x m), u[i] — output капсулы i (вектор размерностью n). U[i, j] также потом понадобится для роутинга.
Далее к s[j] применяется нелинейная функция активации, которая приводит вектор s[j] к вектору v[j] такой же размерности, но длинной меньше 1 (это тот момент где нейроны одной капсулы влияют друг на друга, в остальном специальными связями между собой внутри капсулы они не связаны). Элементы этого вектора после последнего цикла роутига и есть выходы нейронов капсулы j.
Роутинг
Длина выходного вектора капсулы меньше 1. А раз длина выходного вектора капсул нормализована, то можно выяснить какие из капсул вышележащего слоя более «отзывчивы» и передавать данные от нижележащей капсулы только на те капсулы, для которых скалярное произведение U[i, j] и v[j] больше. Собственно это и есть построение parsing tree (как я понял).
Parsing tree между слоем l и (l + 1) строится на лету (заново при подаче каждого изображения).
Для построения этого дерева служат специальные веса b[i, j] (скаляр) между капсулами двух слоёв (используются при вычислении того самого c[i, j] — коэффициента связанности, но постоянно не хранятся, при подаче нового входа на сеть обнуляются).
c[i, j] рассчитываются как exp(b[i, j])/(сумма по k от exp(b[i, k])), где k — кол-во групп в вышележащем слое. Т.е. это softmax.
После последней итерации значение v[j] — и есть выход нейронов капсулы j.
Функция ошибки
Максимизируем длину выходного вектора у той капсулы, которая должна активироваться на текущую картинку и минимизируем у тех, которые не должны. А именно высчитываем сумму по всем k от 0 до 9
L[k] = Tmax(0, 0.9 — ||v[k]||)^2 + 0.5(1 — T)*max(0, ||v[k]|| — 0.1) ^ 2
где T = 1 при наличии цифры k на картинке и равно 0 при отсутствии.
В качестве дополнения приплюсовываем попиксельную ошибку восстановления картинки декодером (см далее), но приплюсовывали с очень небольшим коэффициентом 0.0005.
Архитектура CapsNet

Энкодер состоит из:
Роутинг используется только между вторым и третьим слоем. Все b при инициализации равны 0. 
Декодер состоит из трёх fullyconnected слоёв с 512, 1024 и 784 нейронами и ReLU в качестве активации. То есть декодер берёт данные из тех 10х16 нейронов последнего слоя энкодера и выдаёт картинку 28x28.
Результаты
Для определения какая цифра активна они выбирали вектор самой большой длины из последнего слоя. Без ансамблей сетей и аугментации данных (кроме смещения на 2px) достигли 0.25% ошибку на сети с 3 слоями.
Тренировали свою сеть только на чистом MNIST пока та не достигла ошибки 99.23%. Потом применили её на affNIST, результат 79% accuracy. Проделали то же самое с традиционной CNN — она достигла 66%.
Показали что выучили нейроны в капсуле
Для этого меняли значения отдельных нейронов активной капсулы последнего слоя (все остальные капсулы занулены были, насколько я понял) в диапазоне −0.25, 0.25 и восстанавливали картину через декодер. Тут надо смотреть картинку. Нейроны в капсуле явно выделили вполне интерпретируемые свойства (наклон, размер, интенсивность штриха, всякие отдельные элементы специфичные для конкретного класса, например, закругление верхней части у шестёрки).

Сегментация. Две разные перекрывающиеся цифры из MNIST
Выбирали два самых длинных вектора. Тут тоже надо смотреть картинки. У них получилось разделить где какая цифра и обвести каждую контуром отдельно через декодер( обнуляя все остальные кроме каждой из двух по очереди). При этом цифры были нарисованы довольно плотно друг к другу.
P.S. по новой архитектуре: по сути это обычная архитектура со слоями где каждый нейрон связан со всеми нейронами предыдущего слоя. С четырьмя отличиями:
Авторы статьи: Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana, 2017
→ Оригинал статьи
Автор обзора: Arseny_Info 
Статья о том, как сгенерировать тест-кейсы для DL задачи, обладая хотя бы двумя обученными нейросетями и датасетом.
Т.к. тестировать сети нужно, а классическое software testing с его классическими же метриками типа code coverage выглядит малоприменимым, авторы ввели свою метрику — доля нейронов, активированных набором тест-кейсов, и придумали решение для генерации этих тестов.

Основная идея:

Задача полностью дифференцируема, тест-кейсы генерируются за разумное время на консьюмерском железе. Есть имплементация на TF + Keras.
Полученные тест-кейсы могут использоваться:
Алгоритм тестировался аж на пяти датасетах: три картиночных (MNIST, Imagenet, Driving) и два некартиночных (Contagio/Virustotal, Drebin). Их оригинальная метрика про активированные нейроны выглядит отлично (кто бы сомневался!), но и по остальным показателям (скорость генерации, точность сети, дообученной на проблемных семплах) вроде ок.
Авторы статьи: Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, Byron Boots, 2017
→ Оригинал статьи
Автор обзора: Vadzim Piatrou
В работе предлагается подход, который позволяет сегментировать объекты новых классов по одному или более образцу.
Предложенная нейронная сеть работает следующим образом. На нижнюю сегментирующую ветку подается изображение с новым классом объектов, которое требуется сегментировать. На выходе получается 4096 выходных каналов. На верхнюю управляющую ветвь подается изображение-образец с маской класса. Данная ветвь задает 4096 весов и 1 смещение для классификатора пикселей 4096 каналов выходного снимка, полученного в ветви сегментации. Классификатор пикселей выдает финальную маску нового класса объекта на входном изображении.

Обучение сети выполнялось на базе PASCAL VOC 2012 путем симуляции сегментации одного класса по одному примеру. На каждой итерации случайно выбиралась пара снимок-маска и формировалась маска для одного класса путем случайного выбора класса из присутствующих на снимке. В качестве образца в управляющую ветвь подавался другой случайный снимок-маска для того же класса объектов.
Точность работы (mIoU) сети на новых классах составляет около 40%, что выше предыдущих результатов для обучения по одному примеру (31-33%). Однако финальный результат ниже в 2 раза точности сегментации для полного обучения (PSPNet, 83%).
Авторы статьи: Yuchin Juan, Damien Lefortier, Olivier Chapelle, 2017
→ Оригинал статьи
Автор обзора: Fedor Shabashev
Factorization machines частенько используется в продакшене для предсказания CTR, но поведение юзеров в интернете подвержено изменениям, поэтому модель нужно периодически обучать на новых данных.
Но когда данных много, то каждый раз переобучать модель заново становится вычислительно затратно.
Поэтому, вместо того чтобы обучать модель с нуля, ее инициализируют предыдущей обученной моделью.
Как утверждают авторы, такой подход хорошо работает для логистической регрессии, но плохо для factorization machines. Они пишут что если взять обученную factorization machine и пытаться ее дообучать, то она сразу начинает переобучаться и точность ее от дообучения будет только падать (на картинке график naive). 
Замечу что все эксперименты они делают на кликовых датасетах где нужно предсказывать CTR.
Однако дообучать factorization machines все-таки можно и для этого нужно брать за основу не полностью обученную модель, а недообученную. То есть такую, у которой обучение было остановлено до схождения в локальный минимум.
Если делать так, то, как как утверждается, модель при дообучении не начнет сразу переобучаться и ее точность за счет новых данных повысится (на картинке график pre-mature).
Подобный подход позволяет значительно экономить время на регулярное обучение моделей.
Авторы статьи: Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht, 2017
→ Оригинал статьи
Автор обзора: BelerafonL 
В статье сравнивают работу разных оптимизаторов: SGD, SGD+momentum, RMSProp, AdaGrad, Adam. Общий посыл статьи такой – адаптивные методы (RMSProp, Adam, AdaGrad) работают у авторов хуже, чем обычный SGD и он же с моментумом. Они проверяют на разных доступных в интернете сетках и задачах диплернинга, таких как распознавание картинок, рекуррентных языковых моделях и т.п. И везде у них SGD типа лучше всех ведет себя и на тесте, и на трейне – и дает лучшую точность, и даже быстрее сходится. 
Адаптивные методы могут быть сравнимы с SGD на трейне, но точно проигрывают на тесте. Причем адам ведет себя хуже всех. Объясняют они это тем, что адаптивные методы сильнее оверфитятся и хуже обобщают. Даже что-то там доказывают про это в начале статьи на задаче линейной регрессии. Однако при сравнении они делают для каждого метода очень точный подбор лернингрейта и кривой для снижения лернингрейта. И говорят, что именно правильный подбор этого делает SGD таким успешным. А адаптивные методы, как я понял, работают и с более грубым подходом к заданию learning rate и его decay, сходятся быстро и почти хорошо, но все равно всегда хуже, чем правильно настроенный SGD. Статья, конечно, холиварная и доверять ей сходу стрёмно, но в каждой сказке, как говорится, есть намёк…
Авторы статьи: Gustav Larsson, Michael Maire, Gregory Shakhnarovich, 2016
→ Оригинал статьи
Автор обзора: BelerafonL 
Баян, но всё же. Взахлеб прочитал статью FractalNet: Ultra-Deep Neural Networks without Residuals. На первый взгляд строят ещё одну упоротую архитектуру для обучения имейджнету, в один ряд с ремейками ResNet и DenseNet. Однако внутри оказалось всё интереснее, и меня это впечатлило сильно гораздее, чем, скажем, нашумевшие капсулы. Главная идея сети показана на картинке. 

Они говорят – давайте сделаем структуру сети фрактальной. А именно: сначала поставим простую сверточную сеть от входа на выход. Параллельно ей поставим еще две такие же, соединенные последовательно. Параллельно каждой из тех поставим еще две такие же, соединенные последовательно, и так далее. Получается сеть, которая растет одновременно и в длину, и в толщину. Зачем это надо и чем лучше ResNet?
Для начала надо пояснить, как работает у них механизм объединения Join (зелененький на рисунке). Они говорят, что можно придумать много вариантов, суммирование, конкатенацию, макспуллинг или че-нить еще. Но они берут среднее арифметическое. И этот выбор завязан на то, как они делают регуляризацию своей сети при обучении. Аналогично с дропаутом или дропконнектом, они дропают ветки своей фрактальной сети. Например, одну из двух параллельных, и так для каждого уровня вложенности, как показано на рисунке
 
Выключенная ветка просто не участвует в расчете среднего арифметического, в результате чего параллельные ветки учатся делать одно и то же. Т.е. более простая сеть (на верхнем уровне) учится делать тоже самое, что делают более кучерявые параллельные ей ветки, и наоборот. 
Что это дает? Как показывают авторы, по ходу обучения сначала учатся более простые сетки, т.е. более короткий путь градиентов сети от входа и выхода, и после того, как они выучиваются, начинают за ними, как за учителем, подтягиваться более сложные вложенные слои. Т.е. используется концепция student-teacher.
Далее, в отличие от ResNet, ветки которой выучивают, что нужно прибавить к residual соединению, у FractalNet можно выкинуть любую часть сети, и она продолжит работать на остатках. В ResNet же выкинуть residual связь нельзя, так как эта ветка сломается – можно выкидывать только блоки целиком, residual и параллельные ей ветки. В FractalNet можно после обучения выкинуть, например, все верхние слои и оставить только самый длинный и самый вложенный.

И сеть будет работать, и, что самое интересное, показывать такую же точность, как показывала сеть целиком! В отличие от ResNet, которая без своих residualов не работает.
И это самый интересный результат: вся эта архитектура на самом деле просто фреймворк на тему «как обучить глубокую сеть», т.е. самую глубокую часть сети. Подход student-teacher позволяет протекать градиентам во время обучения так, чтобы обучить самые глубокие слои. Они делают сеть 160 слоев (самый глубокий путь) и она не теряет в точности. И эта сеть опровергает некоторые попытки объяснить успех ResNet как ансамбля сетей (действительно, ResNet можно развернуть как кучу параллельных мелких сеток). Здесь же они показывают – выучили, отлично, откидываем всех «учителей» и вот у нас работающая глубокая сеть с последовательными соединениями, без всяких residual. И они показывают, что если взять сразу такую глубокую сеть, то она не учится ничему вообще, градиенты не протекают так глубоко. А вместе с «учителями» сверху – учится.
Интересная картинка обучения этой штуки на имейджнете: 

Помимо лосса выхода сети, они строят лосс для каждого «столбца» — т.е. график col#4 будет соответствовать лоссу, который дает участок сети с прошлой картинки, т.е. самый длинный путь. А штрихованным показано что будет, если учить такую же сеть без учителей, т.е. голую. Они показали, что в самом начале самый глубокий слой в их сети учится так же, как и «голый», но потом, когда его вышестоящие учителя разобрались как чего, лосс вложенного столбца пошел быстренько падать.
Авторы всю статью рассуждают о том, чем их сеть похожа и не похожа на ResNet, но не говорят о перспективах. А они интересные – архитектура сети, судя по всему, позволяет наращивать сложность сети прямо на ходу, во время обучения. Т.е. сделали фрактал глубиной 3, обучили – что-то мало точности. Накинули еще вложенности, сохраняя веса уже обученного – и «учителя» быстренько доубучат новые вложенные глубокие слои, чтобы те далее превзошли их и показали большую точность.
Конечно, может это сказки всё, и завтра изобретут еще 10 архитектур и эту забудут, но читать было интересно.
В одном из следующих выпусков мы планируем опубликовать обзоры нескольких топовых статей, прошедшей недавно конференции NIPS. А пока, можете узнать от Павла Мезенцева
о первых впечатлениях с NIPS 2017.
Пользователь
=====
MySQL и партицирование
2017-12-04, 11:53
Пользователь
=====
Первый в России митап по Apache Ignite, 12 декабря
2017-12-04, 11:59
Digital Consultant
=====
Пишем простой модуль ядра Linux
2017-12-04, 16:48
автор, переводчик, редактор
=====
Внедрение зависимостей в .Net Марка Симана 1 — Зависимости между слоями приложения
2017-12-04, 13:32
Инженер
=====
Проектирование системы для считывания данных с устройств ввода (Часть вторая)
2017-12-04, 13:01
User
=====
Введение в обучение с подкреплением: от многорукого бандита до полноценного RL агента
2017-12-04, 13:23


Education program designer
=====
EU GDPR: соблюдение требований регуляторов в сфере облачных вычислений
2017-12-04, 16:47
Пользователь
=====
Анализируй это — Lenta.ru
2017-12-04, 15:31

What — анализ статей новостного ресурса Lenta.ru за последние 18 лет (с 1 сентября 1999 года). How — средствами языка R (с привлечением программы MySterm от Yandex на отдельном участке). Why… В моем случае, коротким ответом на вопрос "почему" будет "получение опыта" в Big Data. Более развернутым же объяснением будет "выполнение какого-либо реального задания, в рамках которого я смогу применить навыки, полученные во время обучения, а так же получить результат, который я бы смог показывать в качестве подтверждения своих умений". 
Конечно, определившись внутри себя, что мне нужны практика и портфолио, стоило бы схватить пару датасетов, коих сейчас море, и анализировать, анализировать, анализировать… Но прикинув в голове свои скилы по непосредственному анализу и вспомнив, что анализ это только 20-30% времени и остальное это поиск-сбор-очиска-подготовка данных, решил взяться за второе. Да и хотелось что-то особенное, возможно даже интересное, в отличии от анализа проданных в США авиабилетов за последние 30 лет или статистику арестов.
В качестве объекта исследования была выбрана Lenta.ru. От части, потому что я являюсь ее давним читателем, хоть и регулярно плююсь от того шлака, который проскакивает мимо редакторов (если таковые там вообще имеются). От части, потому что она показалась относительно легким для data mining. Однако если быть честным, то подходя к выбору объекта я практически не учитывал вопросы "а что я буду с этой датой делать" и "какие вопросы буду задавать". И связано это с тем, что на текущий момент я более-менее освоил только Getting and Cleaning Data и мои знания в части анализа очень скудны. Я конечно представлял себе, что как минимум могу ответить на вопрос "как изменилась среднедневное количество публикуемых новостей за последние 5-10 лет", но дальше этого я не задумывался. 
И так, эта статья будет посвящена добыванию и очистке данных, которые будут пригодны для анализа Lenta.ru. 
Первым делом мне необходимо было определиться, как сграббить и распарсить содержимое страниц ресурса. Google подсказал, что оптимальным для этого будет использование пакета rvest, который одновременно позволяет получить текст страницы по ее адресу и при помощи xPath выдернуть содержимое нужных мне полей. Конечно, продвинувшись дальше, мне пришлось разбить эту задачу на две — получение страниц и непосредственный парсинг, но это я понял позже, а пока первым шагом было получение списка ссылок на сами статьи.
После недолгого изучения, на сайте был обнаружен раздел "Архив", который при помощи простого скрипта переадресовывал меня на страницу, содержащую ссылки все новости за определенную дату и путь к этой странице выглядел как https://lenta.ru/2017/07/01/ или https://lenta.ru/2017/03/09/. Оставалось только пройтись по всем этим страницам и получить эти самые новостные ссылки.
Для этих целей (граббинга и парсинга) так или иначе использовал следующие пакеты:
Не хитрый код, который позволил получить все ссылки на все статьи за последние 8 лет:
Сгенерировав массив дат с 2010-01-01 по 2017-06-30 и преобразовав archivePagesLinks, я получил ссылки на все так называемые "архивные страницы":
При помощи метода read_html я в цикле "скачал" содержимое страниц в буфер, а при помощи методов html_nodes и html_attr получил непосредственно ссылки на статьи, коих вышло почти 400К:
После получения первых результатов я осознал проблему. Код, приведенный выше, выполнялся примерно 40 мин. С учетом того, что за это время было обработано 2738 ссылок, можно посчитать, что для обработки 379862 ссылок уйдет 5550 минут или 92 с половиной часа, что согласитесь, ни в какие ворота… Встроенные методы readLines {base} и download.file {utils}, которые позволяли просто получить текст, давали схожие результаты. Метод htmlParse {XML}, который позволял аналогично read_html скачать и продолжить парсинг содержимого, также ситуацию не улучшил. Тот же результат с использованием getURL {RCurl}. Тупик.

В поисках решения проблемы, я и гугл решили посмотреть в сторону "параллельного" исполнения моих запросов, так в момент работы кода ни сеть, ни память, ни процессор не были загружены. Гугл подсказал копнуть в сторону parallel-package {parallel}. Несколько часов изучения и тестов показали, что профита с запуском даже в двух "параллельных" потоках почему нет. Обрывочные сведения в гугле рассказали, что с помощью этого пакета можно распараллелить какие-нибудь вычисления или манипуляции с данными, но при работе с диском или внешним источником все запросы выполняются в рамках одного процесса и выстраиваются в очередь (могу ошибаться в понимании ситуации). Да и как понял, даже если бы тема и взлетела, ожидать увеличение производительности стоило только кратно имеющимся ядрам, т.е. даже при наличии 8 штук (которых у меня и не было) и реальной параллельности, курить мне предстояло примерно 690 минут. 
Следующей идеей было запустить параллельно несколько процессов R, в которых бы обрабатывалась своя часть большого списка ссылок. Однако гугл на вопрос "как из сессии R запустить несколько новых сессий R" ничего не сказал. Подумал еще над вариантом запуска R-скрипта через командную строку, но опыт работы с CMD были на уровне "набери dir и получишь список файлов в папке". Я снова оказался в тупике. 
Когда гугл перестал мне выдавать новые результаты, я с чистой совестью решил обратиться за помощью к залу. Так как гугл довольно часто выдавал stackoverflow, я решил попытать счастье именно там. Имея опыт общения на тематических форумах и зная реакцию на вопросы новичков, попытался максимально четко и ясно изложить проблему. И о чудо, спустя какие несколько часов я получил от Bob Rudis более чем развернутый ответ, который после подстановки в мой код, практически полностью решал мою задачу. Правда с оговоркой: я совершенно не понимал как он работает. Я первый раз слышал про wget, не понимал, что в коде делают с WARC и зачем в метод передают функцию (повторюсь, семинариев не кончал и в моем предыдущем языке таки финты не использовались). Однако если долго-долго смотреть на код, то просветление все таки приходит. А добавив попытки выполнять его по кускам, разбирая функцию за функцией, можно добиться определенных результатов. Ну а с wget мне помог справиться все тот же гугл. 
В итоге суть решения свелась к следующему — предварительно подготовленный файл, содержащий ссылки на статьи, подсовывался команде wget:
Непосредственно код выполнения выглядел так:
После выполнения, я получал кучу файлов (по одному на каждую переданную ссылку) с html содержимым веб-страниц. Также в моем распоряжении был запакованный WARC, который содержал в себе лог общения с ресурсом, а так же то самое содержимое веб-страниц. Именно WARC и предлагал парсить Bob Rudis. То что нужно. Причем у меня оставилсь копии прочитанных страниц, что делало возможно их повторное чтение.
Первые замеры производительности показали, что для закачки 2000 ссылок было потрачено 10 минут, методом экстраполяции (давно хотел использовать это слово) получал 1890 минут для всех статей — "олмост три таймс фастер бат нот энаф" — почти в три раза быстрее, но все равно недостаточно. Вернувшись на пару шагов назад и протестив новый механизм с учетом parallel-package {parallel}, понял, что и здесь профита не светит.
Оставался ход конем. Запуск нескольких по-настоящему параллельных процессов. С учетом того, что от "reproducible research" (принцип, о котором говорили на курсах) шаг в сторону я уже сделал (использовав внешнюю программу wget и фактически привязав выполнение к среде Windows), я решил сделать еще один шаг и снова вернуться к идее запуска параллельных процессов, однако уже вне R. Как заставить CMD файл выполнять несколько последовательных команд, не дожидаясь выполнения предыдущей (читай параллельно), рассказал все тот же stackoverflow. Оказалось, что замечательная команда START позволяет запустить команду на выполнение в отдельном окне. Вооружившись этим знанием, разродился следующим кодом:
Этот код разбивает массив ссылок на блоки по 10000 штук (в моем случае получилось 38 блоков). Для каждого блока в цикле создается папка вида 00001-10000, 10001-20000 и т.д, в которую складывается свой собственный файл "articles.urls" (со своим 10-тысячным набором ссылок) и туда же попадут скачанные файлы. В этом же цикле собирается CMD файл, который должен одновременно запустить 38 окон:
Запуск запуск сформированного CMD файла запускает ожидаемые 38 окон с командой wget и дает следующую загрузку компьютера со следующими характеристиками 3.5GHz Xeon E-1240 v5, 32Gb, SSD, Windows Server 2012

Итоговое время 180 минут или 3 часа. "Костыльный" параллелизм дал почти 10-кратный выигрыш по сравнению с однопоточным выполнением wget и 30-кратный выигрыш относительно изначального варианта использования read_html {rvest}. Это была первая маленькая победа и подобный "костыльный" подход мне пришлось применить потом еще несколько раз.
Результатом выполнения на жестком диске был представлен следующим:
Что означает 379703 скачанных веб-страниц общим размером 66713.61MB и 38 сжатых WARC-файлы общим размером 18770.40MB. Нехитрое вычисление показало, что я "потерял" 159 страниц. Возможно их судьбу можно узнать распарсив WARC-файлы по примеру от Bob Rudis, но я решил списать их на погрешность и пойти своим путем, распарсивая непосредственно 379703 файлов.
Прежде чем что-то выдергивать со скачанной страницы, мне предстояло определить что именно выдергивать, какая именно информация мне может быть интересна. После долго изучения содержимого страниц, подготовил следующий код:
Для начала заголовок. Изначально я получал из title что-то вида Швеция признана лучшей страной мира для иммигрантов: Общество: Мир: Lenta.ru, в надежде разбить заголовок на непосредственный заголове и на рубрику с подрубрикой. Однако потом решил под подстраховки подтянуть заголовок в чистом виде из метаданных страницы.
Дату и время я получаю из <time class="g-date" datetime="2017-07-10T12:35:00Z" itemprop="datePublished" pubdate=""> 15:35, 10 июля 2017</time>, причем для подстраховки решил получить не только 2017-07-10T12:35:00Z, но и текстовое представление 15:35, 10 июля 2017 и как оказалось не зря. Это текстовое представление позволило получать время статьи для случаев, когда time[@class='g-date'] по какой-то причине на странице отсутствовал.
Авторство в статьях отмечается крайне редко, однако я все равно решил выдернуть эту информацию, на всякий случай. Также мне показалось интересным ссылки, которые появлялись в текстах самих статей и под ними в разделе "Ссылки по теме". На правильный парсинг информации о картинках и видео в начале статьи потратил чуть больше времени, чем хотелось бы, но тоже на всякий случай. 
Для получения рубрики и подрубрики (изначально хотел выдергивать из заголовка) я решил подстраховаться и сохранить строку chapters: ["Мир","Общество","lenta.ru:_Мир:_Общество:_Швеция_признана_лучшей_страной_мира_для_иммигрантов"], // Chapters страницы, показалось, что и нее выдернуть "Мир" и "Общество" будет чуть легче, чем из заголовка.
Особый интерес у меня вызвало количество расшариваний, количество комментариев к статье и конечно сами комментарии (словарное содержимое, временная активность), так как именно это было единственной информацией о том, как читатели реагировали на статью. Но именно самое интересное у меня и не получилось. Счетчики количества шар и камменнтов устанавливаются скриптом, который выполняетя после загрузки страницы. А весь мой суперхитрый код выкачивал страницу до этого момента, оставляя соответствующие поля пустыми. Комметарии также подгружаются скриптом, кроме того они отключаются для статей спустя какое-то время и получить их не представляется возможным. Но я еще работаю на этим вопросом, так как все-таки хочется увидеть зависимость наличия слов Украина/Путин/Кандолиза и количества срача в камментах.
Вот в принципе и вся информация, которую я посчитал полезной.
Следущий код позволил мне запустить парсинг фалов, находящейся в первой папке (из 38):
Получив адрес папки вида 00001-10000 и ее содержимое, я разбивал массив файлов на блоки по 1000 и в цикле при помощи map_df запускал свою функцию ReadFile для каждого такого блока.
Замеры показали, что для обработки 10000 статей, требуется примерно 8 минут (причем 95% времени занимал метод read_html). Все той же экстраполяцией получил 300 минут или 5 часов.
И вот обещанный второй ход конем. Запуск понастоящему параллельных сессий R (благо опыт общения с CMD уже имелся). Поэтому при помощи этого скрипта, я получил необходимый CMD файл:
Вида:
Который в свою очередь запускал на выполнение 38 скриптов:

Подобное распараллеливание, позволило 100% загрузить сервер и выполнить поставленную задачу за 30 мин. Очередной 10-кратный выигрыш.
Остается только выполнить скрипт, который объединит 38 только что созданных файлов:
И в итоге у нас на диске 1.739MB неочищеной и неприведенной даты:
Что же внутри?
Парсинг завершен. Осталось привести эту дату к состоянию, пригодному для анализа. Но перед этим добавим обещанный кусок про камменты и репосты.
На самом деле, этот шаг был выполнен практически в конце первой части моего исследования (когда я почти получил пригодную для анализа дату). Если вы помните, веб-страница скачивалась без данных о комментариях и реакций соцсетей, так как эта информация заполнялась скриптами динамически. После подсказки, я решил проверить что показывает инспектор в Google Chrome в момент загрузки страницы и в разделе Network нашел следующее:
И в качестве ответа там было:
Схожие запросы были обнаружены и для Вконтакта, Одноклассников и Рамблера, где хранились данные о количестве комментариев к каждой статье (сами комментарии мне получить так и не удалось). Как оказалось, достаточно было выполнить подобные запросы для каждой из статей. Так как количество запросов ожидалось Количество статей Х 4, то решил сразу воспользовать проверенным проверенным методом "параллелизации". 
Код, который подготавливает 4 CMD файла, которые которые запускают параллельное выполение запосов к социальным сетям:
Как и в предыдущих примерах, массив ссылок разбивается на куски по 10К, преобразуются в строку запроса (к каждой соцсети отдельно), складывается в соответствующие папки. После запуска командного файла и выполнения всех запросов, в соответствующих папках окажутся WARC файлы, содержащие ответы сервисов. Отдельно пришлось повозиться с фейсбуком, так как он не обрабатывал больше 100 запросов за раз. Чтобы увеличить лимит, пришлось зарегистрироваться как ФБ разработчик, зарегистрировать собственное приложение, получить токен и слать запросы уже с его указанием. А так как ФБ мог обрабатывать до 50 значений параметра в одном запросе, то строки запроса для него готовились чуть по другому.
Парсинг ответов был уже делом техники:
На этом сбор данных окончен. Приступаем к обработке.
Как видно из предыдущего текста, работать пришлось с датой 379746 obs. of 21 variables and size of 1.739MB, чтение которой было делом не быстрым. Однако гугл и стаковерфлоу довольно быстро подсказали выход в качестве fread {data.table}. Разница:
Ну а дальше предстояло проверить каждую колонку таблицы, есть ли в ней что-нибудь вменяемое или там только NA. И если что-то есть — привести это что-то к читаемому виду (на этом этапе мне пришлось несколько раз подпиливать Parsing). В итоге код, который приводил дату в вид, готовый для анализа стал таким:
Так как внезапно столкнулся с длительным исполнением кода, добавил секции и time stamp в виде print(paste0("1 ",Sys.time())). Результаты на собственном макбуке 2.7GHz i5, 16Gb Ram, SSD, macOS 10.12, R version 3.4.0:
Результаты на сервере 3.5GHz Xeon E-1240 v5, 32Gb, SSD, Windows Server 2012:
Внезапно (а может и ожидаемо), выполнение функции UpdateAdditionalLinksDomain (которая выдергивает домен и доменную зону для формирования ключа источника) стало самым времязатратным местом. Причем все упиралось в метод tldextract {tldextract}. На самом деле уделять время на дополнительную оптимизацию я не стал и если кто-нибудь сходу тыкнет пальцем куда копнуть — обязательно выделю время и попробую оптимизировать.
В целом по камментам думаю понятно, какие преобразования происходят над данными. Результат:
Все готово для финального шага.
И да, файл потолстел до:
Однако финальный шаг (а именно STEMMING) придется немного отложить, так как необходимо внести кое-какие поправки в исследование. В какой-то момент (ближе к концу шага STEMMING) решил проверить насколько мой ресерч является репродюсибл. Для этого в качестве начально даты указал 1 сентября 1999 года и повторил все шаги но уже с почти вдвое большей выборкой.
Парсинг архивных страниц занял 2 часа и его итогом стал список из 700К ссылок:
Граббинг 700000 статей (за неполные 18 лет) в виде 70 одновременных процессов закончился за 4.5 часа. Результат:
Парсинг 123GB скачанных веб-страниц в виде тех же 70 процессов занял 60 мин (при полной загрузке сервера). Результат 2.831MB неочищеной и неприведенной даты:
Приведение даты заняло больше 8 часов (проблемное место все тоже, секция 7). Время выполнения на сервере 3.5GHz Xeon E-1240 v5, 32Gb, SSD, Windows Server 2012:
В результате получена таблица весом 4.5GB, практически готовая к анализу:
Финальный блок. Осталось привести заголовок, описание и текст статей к нормальному виду, когда Путин/Путина/Путину/Путиным (мир ему и благословение) будет приведено к единообразному Путин (мир ему и благословение). В этом мне помогла статья Стемминг текстов на естественном языке и программа MyStem. Код приведен ниже:
Итог первой части, а именно майнинга даты:
В принципе вот и все. Следующим шагом будет анализ всего этого. Уже подобрал с десяток вопросов, которые можно будет "задать", но будут очень рад услышать ваши идеи и вопросы. Код и процедуры можно найти в моем репо.
P.S.
Дополнительные вопросы для зала:
P.P.S где-то в этом месте я хотел выложить свой датасет в общий доступ, чтобы желающие датасаентисы кинулись его анализировать, нашли бы кучу инсайтов, поделились ими, а я бы по горячим следам повторил их подвиги, наклепал кучу графиков и таблиц и таки завершил бы свой курсовой проект. Но… Пошел чуть дальше.
В какой-то момент (ближе к концу своего проекта) почувствовал, что неплохо было посмотреть на линукс чуть ближе. Поиск подходящего курса на различных учебных площадках в итоге привел на рутрекер, где я и увидел Linux Foundation Certified System Administrator (LFCS) в исполнении Sander van Vugt (рекомендую, акцент преподавателя прочто чумовой). 15 часовой курс удалось осилить за неделю (часть в голове осталось, а часть типа Kernel вылетела почти сразу). Как итог, новые знания захотелось примерить на практике и немного пересмотреть подход к первоначальной задаче, а соответственно и к решению. На момент, когда я приступил к работе цель представляла из себя что-то типа "получить веб-страницу, которая, обновлясь раз в сутки, показывала бы текущее состояние Ленты.ру".
Для начала было принято решение отказаться от Мака и полностью переехать в облако. Для этого был арендован Линукс сервер на убунту с 2Гб рам и 2 ядрами, на котором сразу же был развернут RStudio Server и вся дальнейшая работа продолжилась на нем. Дальше было необходимо выбрать как хранить все полученные данные, как понималось, что работать с файлами это моветон. Выбор пал на объектную МонгоДБ, которая тут же была установлена (хотя как оказалось удобных пакетов для работы с МонгоДБ из Р было не так уж и много).
Были созданы 4 коллекции:
И 4 скрипта:
Схема работы подразумевалась следующая:
Таким образом, закинул в первую коллекцию массив дат и запустив последовательно данные скрипты (которые по сути были модифицированными версиями скриптов первой части) можно было получить итоговый датасет, готовый для анализа. Также, если при помощи cron ежедневно закидвать в коллекцию новые даты, можно получить регулярно пополняемый дадасет. Но… Ни о какой параллельности тут говорить не приходилось. Закинув более менее большой массив дат (не говоря уже про c("1999-09-01", as.Date(Sys.time())) можно было повесить сервер на неделю. 
Немного подумав, пришел к следующей схеме:

Такам образом, достаточно было закинуть в первую коллекцию весь период дат c("1999-09-01", as.Date(Sys.time()) и просто наблюдать. Конечно, с конфигурацией 2Гб рам и 2мя ядрами наблюдать пришлось бы долго, но хостинг позволял легко увеличить количество ядер до 16, а памяти до 8гб. Причем такая мощность нужна была на ограниченный период времени, 700т статей обрабатывались менее чем за сутки, после чего сервер можно было вернуть к базовым настройках.
А дальше как и описывал раньше — достаточно было раз в сутки докидывать новые даты в первую коллекцию, и все остальное проиходило бы автоматически.
Тоже самое и с автообновляемой страницей отчета. Раз в сутки запускается скрипт, которые выгружает данные из монго в CSV, обрабатывает, строит графики, помещает на вебстраницу и выкладывает в общий доступ.
Результат доступен по ссылке LENTA-ANAL.RU.
Сам датасет (период 01-09-1999 — 04-12-2017) доступен по ссылке lenta-ru-data-set_19990901_20171204.zip.
Пример одного документа из сета*:
*в нем могут присутствовать не все поля, рекомендую посмотреть на sample (100 документов) или на весь сет
Надеюсь, что кто-то посчитает эти данные интересными и сможет найти им применение. Сам же вернуть к ним позже, когда прокачаю скилы в анализе.

Пользователь
=====
Команда разработчиков Renga: как мы достигли идиллии, работая без менеджеров
2017-12-09, 11:17
Пользователь
=====
Компания WD (Western Digital) планирует создать диск на 40 ТБ, но хватит ли этого?
2017-12-04, 13:46
Пользователь
=====
Как программисты с PVS-Studio ошибки в проектах искали
2017-12-04, 14:14







Программист
=====
Боты на .Net Core для Telegram, Slack и Facebook
2017-12-04, 16:06
.NET Architect
=====
Подарки от М.Видео: что под капотом?
2017-12-05, 10:33
Разработчик ПО
=====
Об итогах конкурса MERC-2017: победители и комментарии
2017-12-04, 20:56
User
=====
Интерфейсы в C#
2017-12-04, 14:52
User
=====
ASO Monthly #16: Август 2017
2017-12-04, 15:04
Маркетинг, ASO
=====
ITSM-дайджест: 20 материалов об оптимизации ИТ-процессов
2017-12-07, 12:24
Пользователь
=====
Обзор услуги виртуальный рабочий стол
2017-12-04, 15:18

Привет, уважаемые хабра жители! Случилось так, что мне потребовался удаленный рабочий стол для 5 сотрудников своего небольшого проекта и так как этот проект на текущем этапе практически не приносит нам денег, то выбор провайдера услуги виртуального рабочего стола был крайне важен с точки зрения соотношения адекватная цена/качество. Мне попался интересный обзор облачных провайдеров, но услуга виртуальный рабочий стол в обзоре отсутствовала, поэтому я решил провести свое исследование с подведением итогов. 
В результате моих поисков на запрос откликнулось 10 компаний. У каждой компании цены сильно разнились, поэтому потребовалось разбираться почему так дешево или почему так дорого. Результаты моего разбора, задачи, требования и методологию оценки смотрите под катом. 
+бонус: в ходе отбора кандидатов я обнаружил, что у 2 разных провайдеров один и тот же калькулятор и биллинг, что навело на мысль о том, что это одна и та же компания...
Для нашей компании необходимо организовать доступ к удаленному рабочему столу (DaaS – Desktop as a Service, рабочий стол как услуга) по RDP для 5 пользователей с помесячной оплатой.
Если провайдер удовлетворяет требования, то ставится +1 балл. Если не удовлетворяют требованиям, то ставиться отрицательный балл -1. Если не предоставлена информация, то баллы не начисляются.
Обращения в компании осуществлялись от моего юридического лица. После того, как я отправил запрос кандидатам, мне стало интересно, как быстро смогу получить ответ. В итоге не все мне ответили, несмотря на звонки и обращения в онлайн чаты. Некоторые кандидаты не предоставляют виртуальный рабочий стол с числом пользователей менее 20. Данные показатели не вошли в оценку, но возможно, кому-то скорость ответа на запрос будет важна.
Жирным выделены самые медленные и самые быстрые компании.
Компания Deaс выбыла из списка, так как у них оказался невероятно сложный процесс получения бесплатного теста. Сначала просили подписать договор, потом решили упростить процедуру и попросили прислать письмо с заявлением на имя председателя правления. Ни у одной компании из обзора не возникло подобных требований. Всем хватило того, что у меня есть корпоративная почта и сайт компании. Некоторым предоставлял номер мобильного телефона.
Компания ActiveCloud и IT-Grad тоже выбыли на старте, так как не предоставляют рабочий стол с пользователями менее 20 и менее 100, соответственно.
Там, где указан символ “?” означает, что либо отказались отвечать, либо проигнорировали вопрос. 
Мы проводили peformance тесты дисков, CPU и RAM. В тестировании использовались CrystalDiskMark 5.2.1 и PassMark PerformanceTest 9.0, а так же speedtest.net
Наверняка кто-то скажет, что результаты данных тестов – это сферический конь в вакууме, так как замеры в разное время могут отличаться, например, при переносе виртуальной машины в другую ноду производительность может упасть и тд. Тем немее, некоторое представление о производительности в целом такие тесты дают.
В оценке участвовал только тест CrystalDiskMark. Напомню, нас интересует скорость чтения/записи у дисков не менее 500 Мб/с, в частности 1 строка – Seq Q32T1 – проверяется запись и чтение файла размером в 1 Гб, с глубиной в 32 используя 1 поток:
Сводная таблица с результатами выглядит так: 
Результаты speedtest, а так же производительности CPU, RAM и HDD по каждой компании смотрите ниже. 
Cloud4Y
http://beta.speedtest.net/result/6398137154
Cloud Office
http://beta.speedtest.net/result/6397902121
Encility
http://beta.speedtest.net/result/6397310386
Invs (Parking)
http://beta.speedtest.net/result/6415199966
ITCloud
http://beta.speedtest.net/result/6392409440
Невозможно установить PassMark, так как не предоставляют админские права.
Itelis
http://beta.speedtest.net/result/6392537565
IT-Lite
http://beta.speedtest.net/result/6392401677
Maxiplace
http://beta.speedtest.net/result/6409331382
MyVDI / VMLab
http://www.speedtest.net/my-result/6395537360
Stockwell
http://beta.speedtest.net/result/6397970441
Невозможно установить PassMark, так как не предоставляют админские права.
Xelent
http://beta.speedtest.net/result/6412194498
Напомню, у нас 5 пользователей, каждому пользователю минимум 10Гб пространства, не считая системы. Мы не хотим тратить более 1500 руб за пользователя удаленного рабочего стола. Нам нужен Libre Office или Open Office, но опционально рассматривали MS Office. И отсутствие тормозов при работе с виртуальным рабочим столом, поскольку тормоза – это субъективная оценка, поэтому я оставил свои комментарии и в оценке этот параметр не участвовал. 
Итоговые баллы
ТОП-3 провайдеров обзора: 
Encility http://encility.ru/ 10 баллов
IT-Lite http://www.it-lite.ru/ 10 баллов
Xelent https://www.xelent.ru/ 6 баллов
А теперь обещанный бонус – мое мини расследование
Есть ощущение, что компании 1cloud и IT-Grad – это одна и та же компании, которая еще к тому же как-то связана с Xelent (в прошлом Xelent назывался SDN). Я не могу однозначно утверждать это, но есть серьезные предпосылки.
Все началось с того, что, когда я делал этот обзор, я случайно обратил внимание на favicon на сайте xelent.cloud:
Увидев эту иконку, она мне сразу напомнила другую компания 1cloud. Зайдя на сайт 1cloud.ru сразу стало понятно, что это один и тот же логотип: 
Далее в разделе “О компании” мы видим скан сертификата на компанию ИТ-Град:
Сравнив личные кабинеты Xelent и 1cloud мы видим идентичную панель входа и калькулятор, отличается только логотип и цвет. Найдите 3 отличия :)
https://1cloud.ru/services/vps-vds 
https://www.xelent.ru/services/vds-vps/


Надеюсь мой обзор поможет кому-то сэкономить время и деньги. Буду рад вашим комментариям. Спасибо за внимание.
UPD:
Мне написал представитель компании Смарт Офис, который заметил, что его компании нет в списке, точнее указан статус нет ответа. Совместными усилиями мы выяснили, что ответ на мой запрос был, но попал в спам. Поэтому я добавлю в обзор данную компанию после проведения тестирования, о чем сообщу позже. Так будет честнее.
Пользователь
=====
Цифровой маркетинг в режиме одного окна: разбираем облачные продукты SAS
2017-12-05, 14:34
Консультант по клиентской аналитике
=====
Стартап дня (ноябрь 2017-го)
2017-12-05, 12:54

Продолжая серию дайджестов «Стартап дня», сегодня я представляю самые интересные проекты за ноябрь. Если хотите ознакомиться с остальными, то прошу в мой блог. Записи доступны в Facebook, ICQ и Телеграм. 
Стартаперы во всём мире пытаются запустить сервис по аренде одежды, я видел такие презентации десятки раз, идея лежит на поверхности. Китайский YCloset её реализовал и довёл до значимых масштабов.

Механика — классическая подписка: девушка платит 500 юаней (75 долларов) и держит у себя одновременно три вещи. Перебирать гардероб можно неограниченно, хоть каждый день курьера вызывай, пока она платит подписку, это бесплатно. Выбор одежды — 60 тысяч наименований, в Ламоде, для сравнения, чуть больше, но масштаб такой же. Из известных мне брендов с YCloset работает Zara, а журналисты пишут, что средняя розничная цена вещи в сервисе составляет 1500 юаней — на дилетантский взгляд сумма кажется завышенной, но спорить не буду. 
Для тех, кто уже тратит много на одежду, сделка выглядит отлично. Подписчица платит за год цену четырёх предметов, держит дома три и меняет их хоть к каждому выходу в свет — это явно эффективнее, чем обычный магазин. Стартап пока берёт вещи у брендов бесплатно — для них это способ рекламы, но, допустим, будет когда-нибудь покупать с трёхкратной скидкой от розницы. Предположим также, что платья живут год, а потом мода меняется. При таких условиях и идеальной утилизации ассортимента YCloset будет тратить на саму одежду ровно четверть выручки, в реальности, наверное, половину. Оставшиеся 37,5 доллара в месяц с запасом окупят две-три-четыре поездки курьера и столько же стирок, да и на больших объёмах менять будут реже, наиграются и надоест, экономика в масштабе выглядит рабочей. 
Любопытно, что самые большие операционные сложности возникли именно со стиркой и чисткой, в типовой презентации стартапера-идеалиста о них не было ни слова. YCloset полностью берет эти хлопоты на себя, и ему это организационно тяжело, в итоге они даже слились с одной из специализированных компаний.
Стартапу два года, инвестиций получил 70 миллионов долларов, 50 из них в недавнем раунде. О планах расширения на мужскую аудиторию пока не говорит.

Бизнес-планы мобильных игр строятся по одной схеме: стоимость привлечения, конверсия в платящего, средний чек и возвращаемость комбинируются друг с другом по нехитрой формуле и уверенно предсказывают успех или его отсутствие. Braavo Capital автоматизировал такой анализ для предложения финансирования.
Игровая компания дает BC доступ к учётным записям в аналитике и кабинетам рекламодателя, алгоритмы Braavo прикидывают, что маркетинг отбивается, и немедленно одобряют кредитную линию. Как только CTR или ретеншен переползают красную черту — всё, стоп машина, больше денег не даём. Выплаты по кредиту автоматически списываются из денег, перечисляемых сторами, бизнес клиента Braavo понимает обычно лучше, чем сам владелец, — риски минимальны. Как результат уверенности — условия финансирования предлагаются лучше, чем у банка, который в экономике мобильной игры не разбирается и всего боится. Тем более BC дешевле, чем традиционный венчурный фонд — тот-то в конечном итоге хочет возврат 3x, а то и 10x, как это ни оформляй. Ещё одно преимущество нового подхода — скорость принятия решений, в рекламе Braavo обещает 24 часа от получения данных до одобрения кредита.
Автоматического финансирования стартапов по автоматическому финансированию пока не изобрели, сам BC инвестиции поднимал традиционно. Последний раунд составил 70 миллионов долларов, но это число не так велико, как кажется: большая часть суммы — это коммерческий кредит в рабочий капитал, а не инвестиции в развитие компании.

Повесить камеру сейчас стоит копейки, гораздо дороже в неё постоянно смотреть, чтобы не пропустить интересующий момент. Много лет напрашивался стартап с наблюдателями из условного Бангладеш, смотрящих за порядком в США. Местный дорогой сотрудник безопасности выбегал бы только при проблемах, т. е. почти никогда, из десятерых девятерых бы спокойно уволили. Не знаю, запустили ли такой в реальности, а Matroid сделал уже более прогрессивное решение.
В его модели вместо живого человека на видеопоток смотрит AI. Универсальных настроек не существует, каждый клиент ищет свои собственные интересности, кому-то важны любые люди около запретной двери, кому-то чьё-то конкретное лицо. В Matroid покупатель загружает собственные образцы для тренировки своей уникальной нейронной сети и включает её для своей камеры. Удачными настройками сеток можно делиться с миром, пока вроде бы на чистом альтруизме, но в будущем, наверное, и заработать можно будет.
Прайс стартапа выглядит по-современному, с открытыми тарифами, бесплатным минимумом на пробу и оплатой по карте. Оптом 1000 секунд распознавания стоит 40 центов, в переводе на восьмичасовой рабочий день выходит 250 долларов. Сейчас это сравнимо с конечной ценой живого человека в Бангладеш, но куда лучше масштабируется и со временем будет дешеветь, а не дорожать. Со стоимостью аренды сервера эти числа можно даже и не сравнивать, unit-маржинальность у Matroid отличная. 
Хотя клиенты приходят с улицы с произвольными задачами, сам Matroid целенаправленно ищет потребителей в определённых секторах. Кроме очевидной безопасности, сейчас он активно предлагает мониторинг телевидения для крупных компаний в поисках показов их логотипов и первых лиц.
Любопытно, что стартап выглядит очень круто и современно, но прорывных технологий внутри нет, библиотеки используются чужие, веб-интерфейс с биллингом каждый повторить может. Продукт уже готов, а инвестиций пока потрачено всего 3,5 миллиона долларов, в последнем раунде поднято 10, но это на будущий маркетинг, а не на разработку.

Телемедицина с трудом находит себе B2C-ниши с реальным лечением пациентов, зато отлично борется со странной бюрократией. Американские правила разрешают покупку контактных линз только по рецепту, причём с ограниченным сроком действия. Близорукие и дальнозоркие люди даже со стабильным зрением регулярно ходят к врачу, тратят на это своё время и деньги лишь затем, чтобы получить каждый раз совершенно идентичную бумажку. 
Simple Contacts переносит визит в онлайн: пациент ставит множество галочек об отсутствии у него осложнений и жалоб, после чего на экране смартфона или компьютера появляются английские аналоги ШБ и МНК. Результаты автоматического теста подписывает лицензированный врач, если всё ок, то старый рецепт подтверждается, пользователь покупает любимую марку линз прямо в Simple Contacts. 
За проверку зрения стартап берет 30 долларов. Если считать, что окулист расписывается за 5 минут, то долларов 20 из них — маржа. Интернет-магазин работает с рыночными ценами и комиссией, но потенциально зарабатывает больше конкурентов — лояльность покупателей у него должна быть выше, они деньги и, главное, время на визит к офлайн-врачу сэкономили. 
Инвестиций Simple Contacts поднял 10 миллионов долларов; кроме маркетинга, много денег съедает расширение лицензии на тест — сокращение списка противопоказаний и т. п.

Тренд последнего десятилетия — объединение всего и вся в смартфоне. Раньше такси вызывали рукой, в магазине платили карточкой, а еду вообще не фотографировали — теперь всё это делают приложения в смартфоне. Onyx идет против течения и материализует одно из стандартных приложений — общение в режиме рации, воки-токи.
Устройство выглядит как круглый диск размером с ладонь, пользователи цепляют его на одежду, обычно на грудь, но можно и на плечо или ещё куда-то. Аналогичный форм-фактор мы видели в десятках фантастических и фэнтези-фильмов, у Captain Power от похожей кнопки костюм выключался, например. Сигнал Onyx идёт до смартфона по Bluetooth, а дальше к собеседнику через интернет. 
Выигрыш по сравнению с приложением — количество кликов. Чтобы что-то сказать, достаточно коснуться рукой груди — и можно говорить, сравните это с «вытащить телефон из кармана, разблокировать, найти нужное приложение, запустить, нажать кнопку внутри него». С другой стороны, тренд на объединение всего в смартфоне не на пустом месте появился, минусы Onyx тоже очевидны: отдельное устройство и отдельная зарядка, да и цена больше 100 долларов за штуку вместо рекламной модели или пары баксов.
Окупаются такие неудобства только при постоянном использовании, среди частных лиц на них идёт разве что какая-то суперромантичная пара или очень тревожная мама, для 99,9 % населения достаточно обычного мессенджера. Основной рынок стартапа — B2B. Компании покупают Onyx для линейных сотрудников, работающих руками, типичный разговор — это что-то типа «Эй, кто-нибудь, на складе остались ещё красные трубки?» Экономия секунд может быть оправдана даже прямо экономически, выигранное время грузчика — действительно деньги. Кроме того, если речь идёт, например, о кассирах, то использование ими Onyx выглядит куда выигрышнее для глаз людей из очереди. 
Развиваться стартап хочет в сторону автоматического перевода для общения испаноязычных сотрудников с англоязычными и ботов с ответами на стандартные вопросы. Выглядит это как попытка примазаться к AI-хайпу, но раунд в 18 миллионов долларов под эти планы компания получила.
Инвестиции
=====
F# на Linux как лекарство для души
2017-12-11, 14:06
А у вас никогда не возникало ощущения, что «вот это» уже надоело? Что хочется чего-то нового? «Вот этим» может быть что угодно: игра, работа, машина. Что-то любое, что повторяется изо дня в день. А в программировании? Под катом вы найдете историю об усталости от C# и выборе более интересного подхода. 

Передаю слово автору. 
В последнее время я немного устал от C#, бесконечные строчки однотипного кода перестают со временем греть душу. В такие моменты иногда хочется заняться чем-то для души. В моем случае это Linux и F#.
По большей части для меня важно, чтобы я вышел из зоны комфорта. В общем-то я так и сделал, сменил систему и язык.
Организуя свой отдых, я столкнулся с тем, что немного непонятно с чего начать. Давайте же немного разберемся, как дело обстоит с F# на Linux.
Основное:
Искушенные хаброжители уже смекнули, что "легковесный редактор" + Microsoft = Visual Studio Code. Надеюсь, она у вас уже стоит :)
Итак, с выбором редактора разобрались, теперь со всем остальным по порядку:
В принципе, чтобы что-то уже запустить хватит и этого, но тогда статья быстро бы закончилась.
Поставим расширения для поддержки синтаксиса F#, сборки и управлением пакетами Nuget.

Для полноты можно поставить Nuget manager совместимый с .Net Core.

Поставим расширение для отладки (Да, все правильно, написано C#).

После этого клоним вот этот реп, и далее по инструкции делаем: 
Теперь перезагружаем VS Code и ждем пока расширение "отладчика" докачает свои пакеты и нормально развернется.
На самом деле, сейчас уже почти все готово, осталось только протестировать.
Получаем заветные Hello World from F#!

запустится долгожданный F# Interactive.

Его также можно использовать для более полезных целей:

Далее посмотрим сборку и отладку.

Автоматически сконфигурированный файл нам менять не нужно.



Итак, ваша машина настроена и готова к новым свершениям на замечательном функциональном языке. Приятным бонусом будет то, что ваши наработки можно будет встроить в C# проект (посредством подключения .dll).
Ну мало ли кто не знал)
Максимилиан Спиридонов — разработчик C#, студент МАИ, Microsoft Student Partner. В профессиональную разработку на .NET пришёл ещё в школе. Более 2,5 лет работает с реальными проектами на WPF(MVVM)+C#, MySQL, более 5,5 лет разрабатывал на C#. Основная сфера интересов сейчас — это мобильная разработка на Xamarin. Также, по воле случая в сфере интересов оказались С/С++ и Linux.
Также приглашаем вас в чат по F# в Telegram.
Автор
=====
Электричество в дата-центре
2017-12-06, 11:16
Технический писатель
=====
Самые популярные книги по программированию на Reddit
2017-12-06, 16:30
Пользователь
=====
GraphQL — новый взгляд на API. Ч.1
2017-12-04, 17:33
Senior JavaScript Developer
=====
Установка Solaris 10 на SunFire V240 по сети, с помощью загрузочного сервера на базе Debian Stretch
2017-12-04, 17:43
Приветствую всех читателей.
Прошу прощения за столь одиозный заголовок в виде вольной трактовки известной басни Крылова, но именно такое ощущение я получил при распаковке из пузырьковой пленки сервера SunFire V240.
По тексту немного фото будут иметь место быть.
Я достаточно давно хотел "пощупать" настоящий SUN на UltraSPARC именно в железе.
Solaris 10 я увидел на x86 достаточно давно, еще в то время когда SUN, при заполнении простой формы, рассылала установочные DVD по всему миру. Попробовал, не впечатлило. Может не понял философию OS, может Debian на тот момент был логичнее и понятнее для меня, да и для x86 в частности, но диск был заброшен на полку и забыт, а мысль попробовать SUN именно в "железе" периодически всплывала в голове.
И если не в даваться в подробности, как то случайно я стал владельцем SunFire V240.
Так как железка досталась "условно бесплатно", я особо не вникал в особенности комплектации, и получив ответ — "рабочая", согласился стать ее хозяином.
Сервер оказался без… видеокарты, интерфейса PS/2 (видимо были "дернуты" вместе со 2-м блоком питания) и DVD-ROM (его видимо не подразумевалось комплектацией, так как заводская "заглушка" была на месте). Но 4 HDD имели место быть и в душе затеплилась надежда...
"Погуглив" просторы интернета, как что и где, нашел в хозяйстве кабель для CISCO — "RS232-RJ45", подключил к хосту с putty и… надежда пропала.
Система проходила POST, но с HDD не грузилась, выпадая в ошибку (я думаю система была намеренно испорчена, так как потом выяснилось через информацию в ALOM, что сервер использовался в достаточно знакомом на слух банке), пароль для входа в ALOM предыдущий хозяин не помнил или не знал, батарейка BIOS была севшая, настроение ни к черту.
Гуглим...
Поиск показал, что есть возможность установки OS по сети, но разрозненность вариаций установки очень сильно смущала, да и в конечном итоге все статьи сводились к установке из под solaris, когда в наличии был только Debian. Желание "громоздить" даже на "виртуалку" Sunos отсутствовало напрочь. Ну не выполнимых задач не бывает, начнем изучать материал, имея в хозяйстве домашний сервер под управлением Debian Stretch.
Для загрузки по сети на сервере под управлением Debian необходимы следующие пакеты:
Для того, что бы получить исходные данные для загрузки (MAC-адрес) и запустить саму загрузку, нам нужен доступ к OpenBoot Prompt (получаем доступ нажимая в подключенной "putty-сессии" ctrl + Break) и выпадаем в приглашение:
набираем: boot net -v — install
Теперь у нас есть MAC-адрес интерфейса с которого идут запросы на загрузку.
Принимаем имя "SUN-железяки" для дальнейшей работы с ней — "v240"
правим файл /etc/ethers добавив строку:
правим /etc/hosts добавив строку:
Естественно сервис должен быть перезапущен:
Теоретически, при наборе команды boot net -v — install в среде OpenBoot Prompt "SUN-железяка" уже должна получать IP-адрес посредством RARP:
Далее проконтролируем, что в файле /etc/inetd.conf присутствует строка, приведенная ниже. 
Это говорит о том, что tftpd как минимум, корректно установлен:
проверяем, что порт 69 прослушивается:
Далее создаем папку /exports/v240/ и монтируем туда образ загрузочного DVD-диска. Как создать образ или где его скачать останавливаться не буду, приведу только команду монтирования для сохранения логики изложения:
Теперь необходимо скопировать из точки монтирования загрузочного DVD файлы, которые будут нужны "SUN-железяке" для первоначальной загрузки по сети, посредством tftp-протокола, в каталог, который используется по умолчанию tftpd — /srv/tftp. В нашем случае нам понадобится файл inetboot из каталога — /exports/v240/Solaris_10/Tools/Boot/platform/sun4u
Далее создаем symlink файла inetboot с именем в виде HEX транскрипции IP-адреса, котрый будет присваиваться "SUN-железяке".
Вот какой вывод команды ls /srv/tftp/ у меня:
C0A80132 — это и есть symlink на файл inetboot.sun4u (который называется inetboot и находиться на DVD диске в каталоге /Solaris_10/Tools/Boot/platform/sun4u) в виде HEX-транскрипции IP-адреса 192.168.1.50:
C0 — 192
A8 — 168
01 — 1
32 — 50
Теперь если попробовать снова загрузиться с помощью команды boot net -v — install
в syslog загрузочного сервера мы должны увидеть:
Это говорит о том, что запрос на IP-адрес был обработан, далее прошло соединение по TFTP и далее был запрошен IP-адрес на получение bootparams.
Настраиваем /etc/bootparams:
где gate — имя загрузочного сервера под управлением Debian
root — откуда тянем необходимые файлы для дальнейшей загрузки
install — "корень" установочного диска
Параметры boottype и rootopts необходимы "SUN-железяке" как служебные, без описания которых загрузка не пойдет.
далее добавляем в /etc/eхports следующую строку:
Проверяем загружен ли NFS-сервеер:
Проверяем доступ к дистрибутиву через NFS:
Доступ есть.
Теперь правим /etc/dhcp/dhcpd.conf добавив нижеприведенную секцию (dhcp-сервер должен быть предварительно настроен и проверен на факт раздачи IP-адресов клиентам)
Теперь момент истины, в среде OpenBoot Prompt набираем boot net -v — install и в syslog bootup-сервера наблюдаем: 
Далее должно "проскочить" в syslog bootup-сервера:
"И он сказал поехали..."
И вуаля! Нам предлагают выбрать язык и продолжить установку.
За сим откланиваюсь, но не прощаюсь.
denyhosts, битый switch, который "режет" ARP и тихонько молчит об этом (у D-LINK это вообще тема), и iptables (при некоторых настройках) вам могут изрядно попортить крови. Отсюда лишний раз следует, что если хотите предсказуемости результата, то проверьте hardware и выключите не нужный, на данный момент software, тем самым, вы сохраните свои нервные клетки и время.
Так же, можно создать папку /exports/v240_cfg и в ней создать файл sysidcfg со следующим содержимым:
и добавить в /etc/exports строку:
Это избавит вас от ответов на некоторые вопросы в процессе установки OS, но это уже немного другая история...
Типа вид сверху
Ну очередная "доска" из оперы — "выдерни шнур, выдави стекло"
Модуль SCC вид сверху
Модуль SCC (кусок) и "кусок" интерфейса HDD0 — HDD1 — CD(DVD)ROM
"Кусок" интерфейса HDD2 — HDD3
"Доска" из оперы — "выдерни шнур, выдави стекло" №2
Общий план "ливера"
Батарейка, мать ее...
Кулеры процессора — берегите их, они маленькие :)
Кулеры основные, конструкция конечно… "писюковая" во всем смысле этого слова
Плата дистрибьютора питания
Жопа платы дистрибьютора питания
Модуль SCC с наружи — ключик "out"
Модуль SCC с наружи — ключик "in"
Жесткие диски — seagate
Пользователь, Системный администратор-любитель :)
=====
Алгоритмы антиалиасинга в реальном времени
2017-12-06, 10:10
Переводчик-фрилансер
=====
Бот добра для Slack
2017-12-05, 11:20
User
=====
Как мы делали приложение под Windows 10 с Fluent Design (UWP/C#)
2017-12-11, 09:15
User
=====
Неопределённое поведение != Небезопасное программирование
2018-01-24, 08:10
Программист
=====
Автоматизация процессов разработки: как мы в Positive Technologies внедряли идеи DevOps
2017-12-05, 15:10
Пользователь
=====
Анализ производительности React 16 приложений с помощью инструментов разработчика Chrome
2017-12-05, 12:12
Пользователь
=====
Дебаты об Отличном Сервере Приложений Java c Tomcat, Jboss, GlassFish, Jetty и Liberty Profile
2017-12-04, 19:19
Редактор
=====
Двенадцать советов по повышению безопасности Linux
2017-12-06, 12:46
Пользователь
=====
Анализ шести веб-фреймворков: плюсы, минусы и особенности выбора
2017-12-07, 12:27
Пользователь
=====
Развертывание контейнеров Windows в Azure Container Instances (ACI). Коннектор для Kubernetes
2017-12-05, 09:00

Azure Container Instances (ACI) позволяют запускать контейнеры, не беспокоясь об инфраструктуре. Мы можем дать образ контейнера, и ACI с радостью запустит контейнер и даже обеспечит внешним IP-адресом. Когда ручное вмешательство необходимо только при запуске контейнеров, это называется «беcсерверные контейнеры». ACI отлично подходит для пакетных рабочих нагрузок или долгосрочных контейнеров, где мы не хотим иметь дело с инфраструктурой.
ACI обеспечивает низкоуровневый блок построения инфраструктуры для запуска контейнеров. Мы можем думать о нем как о ВМ (виртуальная машина), но вместо запуска образа виртуальной машины он запускает образ контейнера.
Один интересный пример того, как ACI можно использовать в сочетании с контейнерным оркестром, – это экспериментальный ACI-коннектор для Kubernetes. Когда он установлен в кластере Kubernetes, ACI-коннектор создает виртуальные узлы в кластере. Они ведут себя как узлы с неограниченной мощностью. На них мы можем планировать запуск подов (pods), но фактически они будут выполняться как группы контейнеров в ACI.
Возможно, однажды ACI Connector станет основой, позволяющей «бессерверному Kubernetes»… строить кластер в Kubernetes Azure Container Service (AKS), который не имеет физических узлов
Недавно в ACI Connector для Kubernetes была добавлена поддержка контейнеров Windows, и сегодня мы рассмотрим, как использовать ACI Connector для запуска контейнеров Windows.

Создать управляемый кластер Kubernetes в Azure с использованием AKS невероятно просто. Запустите эти команды CLI Azure:
Это создает группу ресурсов и ресурс AKS. Мы установили размер пула агентов в 1, его местоположение в eastus и версию Kubernetes 1.8.2.
После того как кластер готов, мы можем использовать CLI Azure для установки последнего CLI Kubernetes (kubectl) и загрузить файл конфигурации для нашего кластера:
Теперь мы видим один узел в кластере.
Перед установкой ACI-коннектора необходимо создать группу ресурсов, в которую будут развертываться ресурсы ACI:
Обратите внимание на идентификатор новой группы ресурсов.
Затем нужно создать первичный сервис, который ACI-коннектор будет использовать для создания экземпляров контейнера, управления и их удаления во вновь созданной группе ресурсов. Первичному сервису необходимо назначить роль контрибьютора в группе ресурсов. Чтобы создать первичный сервис и назначить ему роли, мы запускаем следующую команду, используя полный идентификатор группы ресурсов на предыдущем шаге:
Обратите внимание на возвращаемые значения свойств appId, password и tenant.
ACI-коннектор доступен как образ на Docker Hub. Чтобы получить поддержку Windows, нужно использовать сборку canary. Создайте следующий файл aci-connector.yaml. Он определяет deployment Kubernetes с одним контейнером, который запускает контейнер из образа microsoft/aci-connector-k8s:canary:
Замените переменные среды значениями, полученными из предыдущих команд. Затем создайте deployment в Kubernetes:
Теперь, если посмотрим состояние кластера, мы увидим ещё два новых виртуальных узла:
Планирование запуска контейнеров на aci-connector-0 будет запускать контейнеры Linux; aci-connector-1 будет запускать контейнеры Windows.
Чтобы Kubernetes случайно не загрузил на них поды, для узлов ACI-коннектора был добавлен azure.com/aci:NoSchedule taint. Мы можем это увидеть, если посмотрим на свойства узла:
Создайте файл iis-pod.yaml со следующим содержимым. В нем описывается один контейнер, в котором отображается содержимое контейнера Windows microsoft/iis:windowsservercore.
Обратите внимание: мы явно указываем Kubernetes, что этот под должен запускаться на узле с именем aci-connector-1. Теперь мы создаем под:
И если мы запросим список наших подов, они появятся в списке:
ACI понадобится несколько минут, чтобы загрузить образ и запустить его. В настоящее время в ACI-коннекторе есть ошибка: он покажет под в состоянии «Running», даже если под все еще создается. Мы должны увидеть статус экземпляра контейнера, выполнив команду Azure CLI:
Когда состояние изменится на «Succeeded», мы можем перейти по IP-адресу контейнера. Получить IP можно через выполнение kubectl get -o wide или вывод команды az, указанный выше.

Посмотрите это видео от Ria Bhatia – менеджера, работающего в ACI и ACI Connector. Отличная демонстрация технологий, о которых мы говорили.
Оригинал: Deploying Windows Containers with Azure Container Instances (ACI) Connector for Kubernetes.
CEO в Southbridge
=====
Внешние ИТ-услуги: «а мы раньше и так работали»
2017-12-05, 10:04
Пользователь
=====
Не было печали, апдейтов накачали
2017-12-04, 23:26
У меня дома используется Debian Sid. Большей частью он весьма и весьма хорош, но местами он слишком Bleeding слишком Edge. Например, когда отгружает пакеты, ломающие работоспособность системы. Вчера приехал wpasupplicant, который сломал мне wifi. Я его откатил, но в процессе я подумал, что многие пользователи не умеют этого делать. Рассказ "как откатить плохой apt-get install/upgrade" — в этом посте.
Мы сделали apt-get install что-то, или apt-get upgrade, или даже apt-get dist-upgrade, и после перезагрузки (или даже сразу же) обнаружили, что так нельзя. Сервис не стартует, убрана важная нам фича, кто-то падает и т.д. Мы хотим откатиться. Но вот, незадача — куда именно мы не знаем, потому что какая была версия до обновления мы не знаем.
apt ведёт очень подробные логи в /var/log/apt. Там есть, в частности, history.log — там будут строчки вида:
Там же будет написано кто это сделал и какой командой.
Чтобы понять, какая версия была предыдущей, придётся посмотреть на лог установки. Он рядом — /var/log/apt/terminal.log. К сожалению, с esc-последовательностями.
Там мы увидим строчку вида:
Чтобы поставить обратно пакет по версии, мы указываем её в явном виде:
К сожалению, 90% это не сработает. Почему? Потому что пакета такой версии в репозитории нет.
В этом случае нам надо найти файл для установки. Он может быть в двух местах.
Когда мы имеем файл на диске, установка тривиальная:
… и у меня снова есть интернет. Есть вероятность, что придётся разбираться с зависимостями, но хорошая новость состоит в том, что если пакеты не конфликтовали до обновления, то их можно откатить в тот же набор версий. И какие именно это версии видно в выводе terminal.log.
Если же архива нет (потому что кто-то сделал upgrade, а потом тут же apt-get autoclean), то надо искать версию в интернетах. Для этого есть архив миррора дебиана. http://snapshot.debian.org/
Как его подключать — написано там, хотя чаще проще просто скачать нужный пакет. Важно: когда ищите снапшот, проверяйте по версиям в логах. Если зависимости противные и сложные, то имеет смысл автоматизировать парсинг terminal.log.
Альтернативный путь: можно попробовать apt-cache policy wpasupplicant чтобы посмотреть на доступные версии и выбрать одну их них (тоже через знак равно). В моём случае это было бесполезно, ибо интернетов не было. Важно: как только вы начинаете откатываться на старые версии, вы рискуете сделать себе такую конфигурацию по зависимостям, которую так просто уже не разберёшь, так что я рекомендую при наличии зависимых пакетов в первую очередь пробовать откатиться на предыдущие версии, а не искать себе приключений посредством даунгрейда на антиквариат.
Дальше мы не хотим апдейтиться до исправления проблемы. Чтобы проблему исправили, её надо зарепортить. https://bugs.debian.org — и там, возможно, оно есть. Если нет — надо репортить. reportbug имя_утилиты.
Чтобы случайно снова не проапдейтиться, пакет можно поставить на hold (запретить апдейты):
(unhold снимает hold).
Я давно для себя выработал привычку делать "dpkg -l >/var/log/dpkg_date" перед апдейтом. В таком виде его легче парсить, если нужно делать большое количество даунгрейдов.
оператор ЭВМ
=====
Spring Websocket + SockJs. How it works?
2017-12-04, 23:53
User
=====
Измерение vs Иллюзии
2017-12-05, 00:18
Биоробот
=====
Стоит ли свое хобби развить в стартап?
2017-12-05, 00:22
Пользователь
=====
История переезда системного администратора в Германию. Часть первая: поиск работы и виза
2017-12-05, 10:30

Системный администратор
=====
Как оценить эффективность рекламы в приложении: ARPDAU, сезонность и несколько секретов
2017-12-05, 12:16
Product Marketing Manager
=====
ASO Monthly #17: Сентябрь 2017
2017-12-05, 08:34
Маркетинг, ASO
=====
Производительность как восприятие: управление восприятием
2017-12-05, 09:55



Время можно анализировать с двух разных точек зрения: объективной и психологической (субъективной). Когда мы говорим о времени, которое измеряется при помощи часов, мы говорим об объективном времени или времени, измеряемом по часам. Оно обычно отличается от времени в восприятии пользователей, ожидающих реакции на свои действия на сайте или в приложении.

В первой части этой статьи мы говорили об объективном времени, и обсуждали подходы к управлению им. Мы изучили, что представляют собой некоторые широко распространенные в веб-индустрии понятия, скажем, «время загрузки страницы», или время ответы системы. Мы также привели рекомендации по выбору бюджета производительности, а заодно разобрались, что делать, когда нам нужно улучшить производительность веб-сайта, или мы хотим догнать по отзывчивости сайты конкурентов. Тем не менее, учет объективного времени имеет свои сложности ввиду ограниченности технических методов такого учета.

Каждый вариант учета времени имеет свои ограничения.
В дополнение к техническим ограничениям абсолютное значение объективного времени имеет особенность (несмотря на то, что мы привыкли воспринимать эту величину как неотъемлемую часть того, что называем «производительность сайта»): её значение сами по себе не позволяет сказать, «быстр» ли конкретный веб-сайт. В подтверждение этих слов ниже приведены абсолютные значения времени пары веб-сайтов. Чтобы сохранить интригу, их названия и адреса закрыты, однако взгляните на цифровые значения:

Абсолютные значения времени этих двух веб-сайтов совершенно не впечатляют. Вы можете провести те же тесты сами, вот ссылки на замеры для сайтов первого и второго.
Время загрузки 12.436 секунд? Визуальное завершение через 12,2 секунды? Значения не впечатляют, не так ли? Сравнивая только эти цифры, мы можем заключить, что производительность этих веб-сайтов отчаянно нуждается в улучшении. Но можете ли вы поверить, что вот эти самые сайты в 2014 году принесли почти 89 и 19 миллиардов долларов дохода, соответственно? Как это может быть?
Не волнуйтесь! Просто введите amazon.com или ebay.com в адресной строке вашего браузера (да, значения времени были измерены именно на этих сайтах) и заметьте, что они отображаются намного быстрее, чем можно было бы ожидать, если смотреть только на приведенные выше цифры. Мы рассмотрим, как достичь этого, чуть ниже.

Производительность не связана с математикой.
Производительность никогда выражается в миллисекундах, килобайтах или количестве запросов; она не связана с математикой. Производительность связан с восприятием и психологией.
В этой части мы рассмотрим производительность в другой перспективе:
«Воображение — единственное оружие в войне против реальности» — фраза, приписываемая Чеширскому коту Льюиса Кэрролла
Время в восприятии нашего мозга может (и обычно так и случается) отличаться от времени, которое мы измеряем при помощи часов. Восприятие и ценность времени колеблются в зависимости от многих факторов, включая уровень тревоги, возраст, время суток и даже культурный бэкграунд. Время, воспринимаемое нашим мозгом, называется субъективным, или психологическим временем.
Чтобы понять, как мы воспринимаем время, давайте рассмотрим некоторые из его основных свойств.
В 1927 году немецкий философ Мартин Хайдеггер написал в своей книге «Бытие и время», что «время сохраняется только как следствие событий, происходящих [в пространстве]». Согласно Хайдеггеру, время конечно, имеет начало и конец, и, в свою очередь, состоит из множества событий с их собственными началами и концами — которые составляют то, что мы называем «время». Давайте рассмотрим простое событие:

Мысленно люди различают события, имеющие четкие начало и конец.
Мы обозначим начало события маркером начало события; его же для простоты будем порой называть маркером начала. Момент завершения события будет, соответственно, обозначен маркером конец события, или маркером конца.
В дополнение к конечному характеру времени, почти любое событие может быть выражено двумя разными фазами: активной и пассивной. Активная фаза или активное ожидание характеризуется наличием сознательной деятельностью пользователя. Это может быть какая-то физическая активность или процесс чистого мышления, например, решение головоломки или поиск пути на карте. Период, в течение которого пользователь не может выбирать, что ему делать, и не может повлять на процесс (например, стояние в очереди, или ожидание любимого человека, который опаздывает на свидание), называется пассивной фазой или пассивным ожиданием. Люди склонны оценивать пассивное ожидание как более длительный период времени, чем активное, даже если временные интервалы на самом деле равны. Исследование Якова Хорника, а затем обширные исследования Ричарда Ларсона (человека, получившего в MIT прозвище Доктор Очередь), показывают, что в среднем люди в состоянии пассивного ожидания переоценивают время своего нахождения в ожидании примерно на 36%.

Люди в состоянии пассивного и активного ожидания воспринимают одинаковые отрезки времени по-разному.
Итак, два основных принципа времени, которые помогут нам дальше в этой статье, таковы:
Никому не нравится ждать (за исключением некоторых граничных случаев). Но когда мы говорим о слишком долгом ожидании, мы на самом деле имеем в виду только пассивную фазу ожидания; в большинстве случаев активная фаза ожидания вообще не принимается во внимание, из-за того, что в течении её мы заняты осмысленной деятельностью. Следовательно, чтобы управлять психологическим временем, и заставить мозг воспринимать событие как менее длительное, чем на самом деле, мы должны максимально уменьшить пассивную фазу события — как правило, путем увеличения активной фазы события. Существует множество методов для достижения этого, но большинство из них сводится к двум простым практикам: упреждающий старт и раннее завершение. Давайте рассмотрим их оба.
Техника упреждающего старта — это процесс создания активной фазы ожидания в начале вашего события, и как можно более долгое оттягивание момента перехода пользователя к пассивному ожиданию. Все это делается без ущерба для продолжительности исходного события. Как уже упоминалось, большинство людей не считают, что активное ожидание — как таковое ожидание вообще; следовательно, мозг пользователя воспримет упреждающий старт как смещение маркера начала события ближе к концу (к тому моменту, когда активная фаза завершена), что поможет пользователю воспринимать событие как более короткое.
Перенести активную фазу в начало можно разными способами, в т.ч. и при помощи некоторых уловок. Это даже может выглядеть как волшебство; но ваши пользователи не должны ничего узнать! Давайте посмотрим на такой процесс на примерах.

При упреждающем старте мы начинаем с активной фазы, удерживая пользователя в ней как можно дольше, прежде чем перейти к более короткой пассивной фазе в конце.
В 2009 году команда управления в аэропорту в Хьюстоне, штат Техас, столкнулась с необычным типом жалобы. Пассажиры не были удовлетворены долгими ожиданиями получения багажа по прибытии. В связи с этим руководство аэропорта увеличило число служащих, занятых обработкой багажа. Итогом стало сокращение времени ожидания багажа до восьми минут, что было очень хорошим результатом по сравнению с другими аэропортами США. Удивительно, но число жалоб не уменьшилось.
Руководство изучило ситуацию, и выяснило, что, действительно, первые сумки появляются на ленте багажной карусели примерно через восемь минут. Однако пассажиры оказывались у багажной карусели всего за одну минуту. Таким образом, в среднем, пассажиры ждали семь минут, прежде чем появлялись первые сумки. Говоря в психологических терминах, активная фаза составляла всего одну минуту, а пассивное ожидание — семь минут.
Используя свои знания в области управления восприятием, руководство использовало нетривиальное решение. Они выбрали ворота прибытия, расположенные дальше от главного терминала, и начали выдавать сумки на самой дальней карусели. Это увеличило время ходьбы пассажиров до шести минут, оставив только пару минут для пассивного ожидания. В результате, несмотря на более длительную прогулку к месту выдачи багажа, пассажиры почти перестали жаловаться.
]
Багажная карусель в аэропорту.
Обработка багажа в аэропорту Хьюстона (как и в любом аэропорту) можно рассматривать как пример техники упреждающего запуска. С психологической точки зрения, начиная процесс обработки багажа как можно раньше, в то время, когда пассажиры находятся в активной фазе ожидания, мы «перемещаем» маркер начала события для пассажиров с реального старта (когда они покидают самолет и начинается обработка багажа) в новую (более позднюю) точку на временной шкале. Это то, что мы называем упреждающим началом: начинай работу, прежде чем пользователь это осознает.
Чтобы справиться с жалобами, команде управления аэропорта Хьюстон остался единственный способ, чтобы уменьшить пассивное ожидание (ожидание около багажной карусели) — увеличить активную фазу ожидания (заставляя пассажиров дальше и дольше идти за своим багажом). И этот подход сработал — без изменения объективного времени обработки багажа.

Чтобы справиться с жалобами, руководители аэропортов Хьюстона увеличили активную фазу ожидания, уменьшив пассивное ожидание.
Другой пример этой техники можно встретить в мобильном браузере Safari на iOS. Когда вы вводите URL-адрес, браузер предлагает вам страницы, используя страницы, на которых вы были, или результаты поиска из поисковой системы. В этом списке есть «часто запрашиваемые» пользователем ссылки. Не многие заметили, что, когда Safari предлагает каждый подобный вариант, он запускает предварительную загрузку такой страницы в фоновом режиме, чтобы ее можно было показать как можно быстрее после выбора из списка (включить и отключить такую предзагрузку можно в настройках мобильного Safari).
То же самое происходит, когда вы открываете ссылку в новой вкладке: пока мобильный Safari анимирует появление вкладки, он параллельно загружает страницу в фоновом режиме, так что, с точки зрения пользователя, после появления вкладки на экране, страница будет загружена и отображена почти мгновенно.

Мобильный Safari предварительно загружает страницы, когда вы производите поиск или открываете вкладку.
Мы можем использовать тот же метод в Интернете — например, в работе функции поиска. Предположим, что поле поиска отображается на всех страницах вашего сайта, но сама страница результатов поиска требует загрузки некоторого дополнительного функционала (сортировка, фильтрация результатов, что несет с собой загрузку некоторых дополнительных файлов библиотек с сервера). Вместо того, чтобы загружать эти файлы на каждой странице (что замедлит загрузку и увеличит трафик), где она может даже не понадобиться, мы можем загружать их только на странице результатов поиска; однако, в результате мы получим замедление загрузки страницы с результатами поиска.
Вместо этого мы можем начать предварительную загрузку требуемых библиотек, как только пользователь начнет вводить текст в поле поиска; это делается в предположении, что результаты поиска окажутся следующей страницей, которую увидит пользователь. Таким образом, необходимые библиотеки, скорее всего, будут загружена в браузер к моменту запроса пользователем результатов поиска. Вы также можете использовать эту технику, чтобы начать предварительную загрузку ресурсов на первой странице корзины покупок, или на первой странице мастера оформления покупки предварительно загрузите скрипты, которые будут использоваться на дальнейших шагах, что сделает переход между этими страницами почти мгновенным.
Чтобы оптимизировать производительность в браузере, группа экспертов отрасли, возглавляемая инженером веб-производительности Google Ильей Григориком, работают над спецификацией W3C под названием «Resource Hints» («Подсказки о ресурсах»). В конечном итоге, спецификация будет охватывать технические решения для браузеров, поддерживащих технологию упреждающего старта. Как пишет Илья в своей книге High Performance Browser Networking, мы сможем «встроить дополнительные подсказки для браузера в сам документ, чтобы браузер мог узнать о дополнительной оптимизации, которые он может выполнять от нашего имени».

Эти подсказки уже поддерживаются в некоторых браузерах.
Все эти подсказки должны быть помещены в элемент <head> документа, чтобы как можно скорее включить предварительную загрузку ресурсов. Давайте кратко рассмотрим некоторые подсказки:
dns-prefetch
Полезна для предварительного ресолвинга (разрешения) имен доменов, которые используются далее на странице. Он также может использоваться для предварительного разрешения имен доменов, куда будет произведен дальнейший переход со страницы.
Пример: <link rel="dns-prefetch" href="//jug.ru/">
preconnect
Подсказка позволяет не только предварительно разрешать доменные имена, но и начать устанавливать связь с этими хостами. Необязательный атрибут «crossorigin» используется для настройки запросов CORS; Илья Григорик написал интересный комментарий по этому поводу:
«Одно небольшое уточнение по поводу атрибута «crossorigin» для preconnect… Это не способ указать, что вам нужен «кросс-доменный сокет» — это уже видно просто по виду URL-а. Скорее, это атрибут, который контролирует политику CORS для этого сокета. Скажем, запросы шрифтов указываются так, чтобы не использовать файлы cookie, и, следовательно, вам нужно указать «crossorigin» в preconnect, чтобы указать клиенту, что он должен открыть сокет, который будет использоваться для извлечения таких ресурсов. Знаю, это немного запутанно.»
Пример: <link rel="preconnect" href="//cdn.example.com" crossorigin>
prefetch
Подсказка имеет самый низкий приоритет и используется для предварительной выборки файлов или ресурсов, которые пригодятся для дальнейшего перемещения по сайту. Дополнительный атрибут as используется для оптимизации процесса выборки для указанного типа содержимого.
Например: <link rel="prefetch" href="/library.js" as=”script”>
Для получения дополнительной информации и просмотра информации о возможных параметрах, прочтите рабочий проект спецификации. Чтобы лучше понять логику работы этих подсказок о ресурсах, и познакомиться с большим числом примеров их использования, обязательно ознакомьтесь со слайдами Ильи Григорика «Preconnect, Prefetch, Prerender», или со статьей «Prefetching, Preloading, Prebrowsing» автора Robin Rendle на сайте CSS-Tricks.
Кроме того, существует спецификация «Preload», объединяющая семейства подсказок ресурсов в проекте редакторов W3C. В этой спецификации предлагается подсказка «preload», внешне выглядящая почти так же, как упоминавшаяся ранее prefetch:
Однако между ними есть одно существенное различие: если «prefetch» имеет наименьший возможный приоритет и предназначен для ресурсов, которые пользователю в дальнейшем понадобятся при переходе по сайту, то «preload» предназначен для критических ресурсов, которые должны быть получены в первую очередь. Также обратите внимание, что preload заменяет подсказку subresource, с которой вы могли столкнуться в других статьях.
Возвращаясь к методике упреждающего старта, можно суммировать её следующим образом: мы начинаем событие активной фазой и сохраняем пользователя в этой фазе как можно дольше, прежде чем переключиться на нудную пассивную фазу события.
Подобно тому, как мы можем перемещать маркер начала в методе упреждающего запуска, раннее завершение перемещает маркер конца ближе к началу, давая пользователю ощущение, что процесс заканчивается быстро. При этом подходе мы открываем событие пассивной фазой, но быстро переключаем пользователя на активную фазу.

С помощью метода раннего завершения мы начинаем с короткого пассивного ожидания и переключаемся на активную фазу как можно раньше.
Наиболее привычное использование этого подхода в Интернете — службы видеовещания. Когда вы нажимаете кнопку воспроизведения на видео, вы не ожидаете загрузки всего видео. Воспроизведение начинается, когда браузеру становится доступен первый минимально необходимый фрагмент видео. Таким образом, маркер конца перемещается ближе к началу, и пользователю предоставляется активное ожидание (просмотр загруженного фрагмента), в то время как остальная часть видео загружается в фоновом режиме. Легко и эффективно.

Службы видеовещания, такие, как YouTube, являются хорошим примером метода раннего завершения.
Мы можем применить ту же технику при работе со временем загрузки страницы. В промежутке между запросом страницы и ее отображением в браузере пользователь ждет, никак не контролируя процесс, и не имеет другого варианта повлиять на него, кроме, разве что, закрытия вкладки. Это признаки пассивного ожидания — а люди ненавидят пассивное ожидание. Поэтому мы должны каким-то образом изменить ситуацию.
Одним из приемлемых решений было бы начать рендеринг страницы, как только основные части, такие как DOM, будут готовы к показу. Нам не нужно ждать загрузки каждого файла, если это не повлияет на рендеринг. Нам даже не нужны все HTML-элементы; мы можем добавлять на экран те, которые не сразу видны (например, нижний колонтитул), позже, при помощи JavaScript. Вспомним примеры Amazon и eBay. Давайте посмотрим на процесс рендеринга Amazon в раскадровке, так, как мы видим его загрузку в браузере:

Amazon достаточно быстро переводит пользователя в активное ожидание.
Здесь мы видим, что, несмотря на завершение отрисовки внешнего вида в течение как минимум 7 секунд, страница Amazon перемещает пользователя в активное ожидание буквально через 1.1 секунды, путем запуска процесса рендеринга верхней (то есть видимой) части веб-сайта, как только основные ресурсы для этой части загружены.
Рекомендация приоритезации загрузки видимого контента, которую мы видим в PageSpeed Insights и аналогичных инструментах оптимизации, должна выполняться именно с учетом такого раннего завершения. Дайте пользователям что-то как можно скорее; заставьте их думать, что страница загружается быстрее, чем на самом деле, разбив нагрузку на короткое пассивное ожидание в начале, а затем на активное ожидание, когда начальная информация была загружена и отображена. При этом очень полезно учитывать время начала рендеринга, сообщаемое инструментами наподобие WebPageTest (там он называется «Start render») — оно дает представление о том, насколько «быстры» страницы сайта в восприятии посетителей.

При использовании метода раннего завершения, как можно скорее заинтересуйте пользователя. Это переключит ожидание в активную фазу.
Управление восприятием, которое использует активные и пассивные фазы, очень эффективно и не обязательно требует дорогостоящих или трудоемких решений. Такой подход отлично работает в большинстве случаев, используя самый мощный инструмент из имеющихся в нашем распоряжении: человеческий мозг.
Кстати, на этой неделе проводится конференция HolyJS 2017 Moscow. HolyJS — наша главная и единственная в России конференция, посвященная исключительно JavaScript. Автор статьи, Денис Мишунов, стоит у её истоков:
У вас еще есть возможность успеть на HolyJS 2017 Moscow, для этого нужно приобрести билеты на официальном сайте конференции.
кибер-ниндзя
=====
Как мы расписание общественного транспорта в 2ГИС добавляли
2017-12-05, 09:48
Крайний за транспорт в 2ГИС
=====
Анонс конференции Mobius 2018 Piter
2017-12-05, 11:07
ContentProvider
=====
Хранение метрик: как мы перешли с Graphite+Whisper на Graphite+ClickHouse
2017-12-05, 13:42
Всем привет! В своей прошлой статье я писал об организации модульной системы мониторинга для микросервисной архитектуры. Ничего не стоит на месте, наш проект постоянно растёт, и количество хранимых метрик — тоже. Как мы организовали переход с Graphite+Whisper на Graphite+ClickHouse в условиях высоких нагрузок, об ожиданиях от него и результатах миграции читайте под катом. 

Прежде чем я расскажу, как мы организовали переход от хранения метрик в Graphite+Whisper на Graphite+ClickHouse, хотелось бы дать информацию о причинах принятия подобного решения и о тех минусах Whisper, с которыми мы жили в течение продолжительного времени.
1. Высокая нагрузка на дисковую подсистему
На момент перехода к нам прилетало примерно 1.5 млн метрик в минуту. С таким потоком дисковая утилизация на серверах была равна ~30%. В целом это было вполне приемлемо — все работало стабильно, быстро писалось, быстро читалось… До того момента, пока одна из команд разработки не выкатила новую фичу и не стала отправлять нам 10 млн метрик в минуту. Вот тогда-то дисковая подсистема поднапряглась, и мы увидели 100% утилизации. Проблему удалось быстро решить, но осадочек остался.
2. Отсутствие репликации и консистентности
Скорее всего как и все, кто использует/использовал Graphite+Whisper, мы лили одинаковый поток метрик сразу на несколько серверов Graphite с целью создания отказоустойчивости. И с этим особых проблем не было — до момента, когда один из серверов по какой-либо причине не падал. Иногда мы успевали поднять упавший сервер достаточно быстро, и carbon-c-relay успевал заливать в него метрики из своего кэша, а иногда нет. И тогда в метриках была дыра, которую мы затягивали rsync`ом. Процедура была достаточно долгой. Спасало только то, что происходило подобное очень редко. Также мы периодически брали рандомный набор метрик и сравнивали их другими такими же на соседних нодах кластера. Примерно в 5% случаев несколько значений различались, что нас не очень радовало.
3. Большой объем занимаемого места
Так как мы пишем в Graphite не только инфраструктурные, но и бизнес-метрики (а теперь ещё и метрики из Kubernetes), то довольно часто получаем ситуацию, при которой в метрике присутствуют только несколько значений, а .wsp-файл создается с учетом всего retention периода, и занимает предвыделенный объем места, который у нас был равен ~2Мб. Проблема усугубляется ещё и тем, что подобных файлов со временем появляется очень много, и при построении отчетов по ним на чтение пустых точек уходит много времени и ресурсов.
Сразу хотелось бы отметить что с проблемами, описанными выше, можно бороться различными методами и с разной степенью эффективности, но чем больше к вам начинает поступать данных, тем сильнее они обостряются.
Имея все вышеперечисленное (с учетом предыдущей статьи), а также постоянный рост количества получаемых метрик, желание перевести все метрики к интервалу хранения в 30 сек. (при необходимости — до 10 сек.), мы решили попробовать Graphite+ClickHouse в качестве перспективной альтернативы Whisper.
Посетив несколько митапов ребят из Яндекса, прочитав пару статей на Хабре, прошерстив документацию и найдя вменяемые компоненты для обвязки ClickHouse под Graphite, мы решили действовать!
Хотелось получить следующее:
Достаточно амбициозно, правда? 
Для получения данных по протоколу Graphite и последующей записи их в ClickHouse, был выбран carbon-clickhouse (golang). 
В качестве базы данных для хранения временных рядов был выбран последний на тот момент релиз ClickHouse стабильной версии 1.1.54253. При работе с ним были проблемы: в логи сыпало гору ошибок, и было не совсем понятно что с ними делать. В обсуждении с Романом Ломоносовым (автор carbon-clickhouse, graphite-clickhouse и еще много-много чего) был выбран более старый релиз 1.1.54236. Ошибки пропали — все стало работать на ура.
Для чтения данных из ClickHouse был выбран graphite-сlickhouse (golang). В качестве API-интерфейса для Graphite — carbonapi (golang). Для организации репликации между таблицами ClickHouse был использован zookeeper. Для маршрутизации метрик мы оставили нами горячо любимый carbon-c-relay (С) (см. прошлую статью). 
“graphite” — база данных, созданная нами для таблиц мониторинга. 
“graphite.metrics” — таблица с движком ReplicatedReplacingMergeTree (реплицируемый ReplacingMergeTree). В данной таблице хранятся имена метрик и пути до них.
“graphite.data” — таблица с движком ReplicatedGraphiteMergeTree (реплицируемый GraphiteMergeTree). В данной таблице хранятся значения метрик.
“graphite.date_metrics” — таблица, заполняемая по условию, с движком ReplicatedReplacingMergeTree. В эту таблицу записываются имена всех метрики, которые встретились за сутки. Причины создания описаны в разделе «Проблемы» в конце этой статьи.
“graphite.data_stat” — таблица, заполняемая по условию, с движком ReplicatedAggregatingMergeTree (реплицируемый AggregatingMergeTree). В эту таблицу записывается количество входящих метрик, с разбивкой до 4 уровня вложенности.

Как мы помним из ожиданий от данного проекта, переход на ClickHouse должен быть без даунтаймов, соответственно, мы должны были каким-то образом переключить всю нашу систему мониторинга на новое хранилище максимально прозрачно для наших пользователей.
Сделали мы это так. 
В carbon-c-relay добавили правило отправлять дополнительный поток метрик в carbon-clickhouse одного из серверов, участвующих в репликации ClickHouse таблиц.
Написали небольшой скрипт на python, который с помощью библиотеки whisper-dump вычитывал все .wsp-файлы из нашего хранилища и отправлял эти данные в вышеописанный carbon-clickhouse в 24 потока. Количество принимаемых значений метрик в carbon-clickhouse, достигало в 125 млн/мин., и ClickHouse даже не вспотел.
Создали отдельный DataSource в Grafana с целью отладки функций, использующихся в существующих дашбордах. Выявили список функций, которые мы использовали, но они не были реализованы в carbonapi. Дописали эти функции, и отправили PR`ы авторам carbonapi (отдельное им спасибо).
снизили утилизацию дисковой подсистемы с 30% до 1%; 


В нашем случае не обошлось и без подводных камней. Вот с чем мы столкнулись после перехода. 
С версии 1.1.0 Graphite стал официально поддерживать теги. И мы активно думаем над тем, что и как надо сделать чтобы поддержать эту инициативу в стеке graphite+clickhouse.
На базе описанной выше инфраструктуры, мы реализовали прототип детектора аномалий, и он работает! Но о нем — в следующей статье. 
Подписывайтесь, жмите стрелку вверх и будьте счастливы!
Пользователь
=====
Tcl/Tk. Тематические виджеты TTK и дизайнер TKproE-2.20
2017-12-05, 11:22
Программист
=====
Присвоение имён: руководство для программиста
2017-12-05, 10:45
Пользователь
=====
Почему repository в pom.xml — плохая идея
2017-12-05, 14:05
Где-то полгода назад я опубликовал туториал, посвящённый добавлению в проект библиотек, которых нет в репозиториях maven. Речь шла о маленьких проектах, и я порекомендовал ставить тег repository прямо в pom.xml, чтобы можно было собирать проект без необходимости править settings.xml.
В комментариях этот подход критиковали sshikov, igor_suhorukov, jbaruch и многие другие. Там же в комментариях мне дали ссылку на статью Брайана Фокса, в которой чётко и понятно изложено, чем чреваты repository в pom.xml. Статья 2009 года, но не потеряла актуальности до сих пор. Перевода на Хабре я не нашел — поэтому предлагаю вашему вниманию свой.
Есть один вопрос, который мне задают очень часто, и, думаю, пришло время изложить свои мысли по этому поводу в письменном виде, чтобы не приходилось повторять их снова и снова.
Вот он, этот вопрос: Куда класть пути к репозиториям: в помники или в settings.xml?
Короткий ответ: settings.xml.
Длинный ответ: По обстоятельствам.
Тут нужно рассмотреть два сценария: для энтерпрайза (программного обеспечения, которое недоступно извне) и общедоступного программного обеспечения.
Мы начнём с энтерпрайза.
Энтерпрайз
В случае с энтерпрайзом общепринятая практика — ставить какой-нибудь менеджер репозиториев наподобие Nexus. Он работает как прокси для всех внешних репозиториев, а также управляет внутренними.
Когда maven ищет артефакты, он ходит по перечню репозиториев, определённых в помниках и в settings.xml, пока не найдёт то, что ищет. Если в качестве репозитория в помнике указать ваш менеджер репозиториев, maven всё равно откатится к поиску в репозитории Central, когда не найдёт артефакты там.
Один из способов решить эту проблему — переопределить id репозитория Central и вписать туда адрес вашего менеджера репозиториев. Такое решение может подойти, но, чтобы не повторять процедуру для каждого проекта, делать это нужно в каком-то одном корпоративном pom файле. После переопределения Central вы попадаете на "Уловку-22". Для того, чтобы найти помник, в котором указан путь к репозиторию, нужно знать, где репозиторий, в котором этот помник находится. Закончится всё тем, что в любом случае придётся добавить этот репозиторий в settings.xml просто чтобы что-то вообще работало.
С простым переопределением Central есть и другие проблемы. Конкретно, этот способ не позволяет обрабатывать обращения к другим репозиториям, которые могут быть в транзитивных зависимостях проекта и редиректить запросы к ним. Это порождает проблемы: во-первых, замедляется сборка, так как maven обходит все внешниe репозитории в поисках артефакта… Возможно, он будет искать артефакт даже в ваших внутренниx репозиториях, в которых этого артефакта точно нет. Во-вторых, это означает, что что-то может собраться у одного разработчика и не собраться у другого. В-третьих, получается, что как организация вы понятия не имеете, откуда берутся ваши артефакты.
Вам, как организации, в большинстве случаев удобно, когда все разработчики используют одни и те же репозитории при сборке и все запросы проходят через один контролируемый механизм. Этого легче всего достичь с помощью записи mirrorOf * в settings.xml, которая будет редиректить все запросы на ваш менеджер репозиториев. В помнике mirrorOf определить нельзя. Примеры того, как выглядит хорошая настройка, можно посмотреть в этом разделе Nexus Book.
Если учесть все эти моменты, понятно, что определение репозиториев в pom.xml на самом деле не очень помогает и в основном только создаёт дополнительные проблемы.
Выше подразумевается, что ваши артефакты не имеют внешнего потребителя. Если это не так, то кроме первой категории, вы попадаете ещё и во вторую. (Категории не исключают друг-друга.)
Проекты с открытым кодом
Если вы выкладываете своё программное обеспечение в открытый доступ и его будет скачивать и собирать кто-то, кроме вас, тогда нужно учесть и другие моменты. Если все ваши зависимости есть в репозитории Central, тогда больше ничего делать не надо. Однако если ваши артефакты можно положить только в ваш собственный репозиторий (речь идёт, например, о SNAPSHOT версиях ваших родительских помников), или в репозитории третьих лиц, тогда со сборкой вашего кода у разработчиков будут проблемы. В этом и только в этом случае имеет смысл добавлять repository в ваши помники. Но тут есть побочные эффекты, о которых нужно знать.
Если результат сборки — утилита (как Nexus), а не компонент, который будет использоваться как зависимость, то добавление repository в помник более-менее безопасно. В этом случае, маловероятно, что кто-то ещё будет зависеть от вашего артефакта в своей сборке напрямую, и изложенные выше опасения становятся беспочвенными, так как в новые версии кода скорее всего попадут правильные пути.
Если вы публикуете джарки, которые будут использоваться в сборке кем-то другим, тогда нужно подумать о том, чтобы выкладывать их и их зависимости в репозиторий Central. Тогда они всегда будут доступны всем пользователям, независимо от того, что случится с вашим репозиторием или урлами, и нет риска случайно внести новые репозитории в сборки. По этой же причине Central рано или поздно перестанет принимать помники с repository.
Итого
В сухом остатке имеем следующее. Если вам нужны воспроизводимые сборки и хороший контроль над происходящим внутри организации, используйте менеджер репозиториев, а также задавайте mirrorOf в settings.xml у всех сотрудников, чтобы указать адрес этого менеджера.
Если вы отдаёте исходники наружу и хотите, чтобы их удобно было собирать, рассмотрите возможность добавления repository в помник, но не подходите к выбору урла легкомысленно, мыслите стратегически, и используйте урл, который всегда будет под вашим контролем. Если урл придётся изменить, убедитесь, что вы всегда сможете отследить ошибки 404 и напишите соответствующие правила mod_rewrite для того, чтобы убедиться, что будущие сборки смогут найти подходящие артефакты.
P.S.
И в заключение небольшая байка из моей собственной жизни. Когда-то давно я поучаствовал в создании концепт пруфа одной интересной идеи. Сделано всё было на java, и деплоить код надо было естественно в среде заказчика, иначе — какой же это пруф :). Сборка была оформлена согласно рекомендациям лучших собаководов, репозитории подключались в settings.xml, артефакты лежали в Nexus — всё как в этой статье.
Но во время деплоя внезапно выяснилось, что на settings.xml, который используется при сборке, нам не дадут даже посмотреть, не говоря уже о редактировании оного. Энтерпрайз, секьюрити, все дела. И пришлось класть repository в помник. Репозитории для разработки и для CI были разные, и для того, чтобы как-то обработать эту ситуацию, сделали 2 профиля: один — с репозиториями для CI, который был дефолтным, а другой — с репозиториями для разработки, который помечался как дефолтный в settings.xml на машине разработчика. Эмоций, помню, мы тогда хлебнули изрядно. Вот так вот бывает.
UPD.
Тут поступило 2 полезных совета от jbaruch, которые хорошо дополняют статью, добавлю их сюда. Он, конечно, топит за Artifactory, но советы дельные, а адвокатов Nexus в темку почему-то не набижало.
Пользуйтесь вычищением репозиториев из pom.xml в Artifactory. Если кто-то в команде таки поленился сделать правильно, и наговнякал, билд на CI упадёт, так как из pom.xml будут удалены репозитории. 
Наша компания ищет:
Software developer
=====
RubyMine 2017.3. Более быстрая IDE, WSL, Embedded Puppet, и многое другое
2017-12-05, 12:41
Supercalifragilisticexpialidocious!My army of SketchUp Ruby API cops are locked and ready! :D pic.twitter.com/Lbow1l6oUt
Hosted a Ruby User Group today at @jetbrains! Next time you're in Saint Petersburg, make sure to check if there's a meetup and join us! https://t.co/vmVC9vRdNZ #Ruby #Rails pic.twitter.com/LxM3Ny2LFQ
Product marketing manager
=====
Сдаем экзамен NS0-507 — NetApp Certified Implementation Engineer—SAN Specialist
2017-12-06, 09:48
Пользователь
=====
Дискуссия о статическом анализе кода
2017-12-05, 11:36
DevRel
=====
Автоматизация бизнеса (бизнес-процессов) простыми словами
2017-12-05, 11:44
Разработка и создание систем автоматизации бизнеса
=====
Virtuozzo 7 update 6: ресурсы стали мобильнее
2017-12-05, 12:03
Пользователь
=====
Шоу, учебник, справочник и договор: анонс бесплатной YouTube-трансляции Heisenbug 2017 Moscow
2017-12-06, 11:25

Какой самый частый вопрос в комментариях на Хабре? «Будет ли запись?» Сразу возьмём быка за рога: запись будет, как всегда, через 3-4 месяца. Но из тех, кто задавал вопрос, смотреть её будет едва ли половина.
Чтобы понять, почему так происходит, нужно разобраться — как мы смотрим видео с конференций, зачем это вообще нужно?
Видео — это не только шоу и повод что-то вместе обсудить. Можно рассматривать его еще и как один из лучших видов документации.
Будучи разработчиками, мы не так часто задумываемся, что существует несколько способов написания документации. На заре появления рунета об этом хорошо написал Влад Балин.
Документация бывает разных типов, как минимум:
Доклад с конференции совмещает сразу несколько ролей. Изначально это некое живое выступление, концентрированная, полезная для обучения информация.
Но если доклад о Selenium читает создатель Selenium и заявляет, что в будущем проект будет развиваться определенным образом — это некий договор, зафиксированный на видеозаписи. Можно показать это друзьям и сказать: «Ха, глядите, у нас есть обещание от самого Simon Stewart, что это будет работать вот так». Счастливчики, пришедшие на конференцию вживую, могут это обещание еще и обсудить в специальной дискуссионной зоне.
Если же докладчик полно и всесторонне описывает какую-то часть системы, то в будущем это можно использовать как справочник. Можно будет обращаться к этому видео раз за разом, обдумывать, пересматривать, использовать данные из него при написании кода или при составлении собственной документации.
Очевидно, что у доклада и его видеоматериала есть некий жизненный цикл. Максимальную ценность доклад имеет на старте, когда человек может быстро и просто впитать всю эту информацию и тут же применить на практике. После того, как конференция закончилась, она потихоньку превращается из учебника в еще и справочник. Пока видео лежит в закрытом доступе — это справочник с особыми ноу-хау, доступными только посетителям конференции. После открытия через 3-4 месяца и выкладывания на YouTube оно становится публичной информацией, на которую можно кинуть ссылку друзьям, с указанием точного времени просмотра. Через полгода-год после выпуска оно начинает устаревать, но эстафетную палочку подхватывает следующая конференция. Устаревание касается в большей степени фреймворков и конкретных технологий, а вот фундаментальные доклады по computer science, построению процесса разработки и тестирования и так далее — ими можно продолжать пользоваться в течение нескольких лет точно. Получается такой глобальный круговорот докладов в природе.
Основная проблема донесения информации в виде лекций и написания документаций-учебников в том, что обычно в организации нет людей, которые могут провести такую лекцию. Или они есть, но заняты чем-то другим. Или они просто не так хороши, как докладчики с конференций — мало кто может рассказать про Selenium лучше, чем Simon Stewart — живое воплощение Selenium на земле.
Именно поэтому мы хотим не смотреть видеозаписи конференций через 4 месяца, а участвовать в этой конференции вживую. Это совершенно логичное желание и способ использовать предложенный материал на полную катушку.
К сожалению, не все могут прилететь через полмира вживую. Не всех отпустят работодатели. В конце концов, кому-то могло не хватить бюджета.
Иногда гораздо полезней собраться перед большой плазмой в офисе и обсудить доклад с коллегами — в полный голос, не боясь кому-то помешать, используя всю мощь русского языка и русского мата.
А еще, иногда, хочется просто сидеть дома перед большим экраном, без необходимости вливаться в огромную толпу людей и куда-то идти. Спокойно поспать дома, а не в гостинице — как-никак, конференция идет целых два дня.
Все эти проблемы решает онлайн-трансляция. Вы можете смотреть что угодно и как угодно. Несмотря на то, что наибольшую пользу конференция приносит, если приехать туда вживую и участвовать в дискуссионных зонах, онлайн-трансляция также имеет массу уникальных преимуществ.
Раньше мы выкладывали на Хабр ссылки на всевозможные плейлисты на YouTube и предупреждали: «Вот эта ссылка — на первый день, вот эта — на второй, вот эта — еще на что-нибудь». Вам нужно было сохранить ссылку на этот хабрапост и потом следить за его обновлениями. 
Есть путь гораздо проще. Бесплатную трансляцию можно смотреть прямо с нашего сайта, вот по этой ссылке:
Эта ссылка — конечный источник истины. Именно там отображается самая свежая, актуальная программа. Там же есть и большая оранжевая ссылка «Пообщаться с участниками и задать вопрос спикеру», которая ведет на Telegram, пользоваться ей могут участники не только платной, но и открытой трансляции.
Развитие конференции очень похоже на развитие программной платформы. Наши языки и платформы постоянно конкурируют между собой: например, Java гонится за .NET по синтаксису основного языка, а .NET пытается догнать Java по кроссплатформенности. Как только появляется новая идея, реализации этой идеи начинают появляться во всех популярных технологиях.
Создание конференций очень похоже на развитие программной платформы. У JUG.ru Group есть своя платформа, список базовых фичей. Как только одна из конференций что-то улучшает, обновление получают и все остальные. За этот год, в том числе этой осенью, было множество конференций, и много всего улучшилось.
Онлайн-трансляция — это именно то, что все мы хотим и ожидаем увидеть от хорошей конференции. Чтобы улучшить трансляцию, зачастую достаточно просто добавить больше качественной техники и правильно её настроить. Поэтому трансляции получили за эту осень максимальное количество полезных обновлений. 
Конференция проводится в течение двух дней. Первый день начинается в 10.00, второй — в 10.30. Каждый день оканчивается завершающим кейноутом (в 17.35 и 18.15 соответственно). Актуальную программу первого зала можно увидеть на странице трансляции.
Никита Макаров/Одноклассники
В отрасли тестирования очень много говорится про «черный ящик», но изредка и вскользь упоминается «белый». Связано это в том числе и с тем, что тестирование «белого ящика» всегда считалась прерогативой программистов.
В этом докладе мы увидим ответы на эти и другие вопросы, с примерами и наглядной демонстрацией.
 Никита Макаров
Работал в аутсорсинге и продуктовых компаниях. Занимался автоматизацией встраиваемых операционных систем на базе Linux, комплексных VPN-решений для бизнеса, программно-аппаратных комплексов. С января 2012 года является руководителем группы автоматизации тестирования в проекте Одноклассники.

Юлия Атлыгина/ALM Works
Цель доклада: показать инструменты, которые можно будет использовать в тот же день, которые ускорят работу ручных тестировщиков, а также аналитиков-разработчиков и прочих. Многие думают, что ускорить ручные тесты можно только автоматизацией — но это не совсем так :) 
В данном докладе хотелось бы показать простые инструменты, которые бы позволили тратить меньше времени и при этом помогали бы увеличить тестовое покрытие. Целевая аудитория: ручные тестировщики, а также все, кому хоть иногда нужно что-то анализировать и делать руками. 
 Юлия Атлыгина
В тестировании более 9 лет, последние из которых провела в ALM Works, разрабатывая плагины для Atlassian JIRA и Confluence. Роль тестировщика совмещает с ролями Product Owner и SAFe консультанта. Если есть вопросы по JIRA — не стесняйтесь задавать!

Владимир Ситников/Netcracker
Часто под словом «тестирование производительности» подразумевают только тестирование серверной части. Гораздо реже тестируют непосредственно работу браузера. В простом случае подключаем Яндекс.Метрику и/или Google Analytics, и вперёд. Но есть нюанс: в корпоративной среде отправка данных в ЯМ/GA может быть недоступна, да и простого подключению ЯМ/GA недостаточно, чтобы собирать нужное количество информации о производительности приложения. 
В докладе мы рассмотрим то, как измерять длительность операций в браузере. Узнаем, почему времена, получаемые от Selenium, показывают погоду, узнаем, какая польза от Selenium-тестов может быть при замерах производительности. Посмотрим на boomerang.js и узнаем, на какие моменты обращать внимание при интеграции подобных библиотек в проект. Доклад не затрагивает вопросы оптимизации браузера/сервера. 
 Владимир Ситников
Десять лет работает над производительностью и масштабируемостью NetCracker OS — ПО, используемого операторами связи для автоматизации процессов управления сетью и сетевым оборудованием. Увлекается вопросами производительности Java и Oracle Database. В его обязанности входит планирование нагрузочных замеров, анализ и объяснение полученных результатов. Владимир является commiter’ом в Apache JMeter.

Андрей Сатарин/Яндекс
Системы, которые мы разрабатываем, становятся сложнее с каждым днем. И кажется, нет спасения от вездесущей сложности, которая проникает во всe. Один из аспектов этой сложности — конфигурация. С одной стороны, конфигурация сильно влияет на стабильность и доступность системы, с другой — проверке её корректности уделяется очень мало внимания. 
В докладе расскажем, как мы тестируем конфигурацию и насколько это было полезно в нашем проекте. Этот доклад будет полезен всем, кто хочет узнать простой способ увеличения стабильности и доступности системы в продакшне. 
 Андрей Сатарин
Занимается тестированием распределенных систем в Яндексе. В своей карьере успел поработать в совершенно разных проектах: тестировал игру в Mail.ru, систему облачного детектирования в Лаборатории Касперского, систему расчета валютных цен в Deutsche Bank. Интересуется тестированием backend и распределенных систем.

Alan Page/Unity
Если вы — тестировщик, пишущий код, то скорее всего, ваша работа включает в себя автоматизацию тестирования — в особенности, написание тестов, которые автоматизируют рабочий процесс пользователя. Будучи ветераном автоматизированного тестирования (уже двадцать лет, и эта цифра продолжает расти!), Алан Пейдж видел множество команд, попытки которых хоть как-то заняться автоматизацией раз за разом оказывались абсолютно неуспешными. Тем не менее, он умудрился поучаствовать в тех немногих командах, у которых всё получилось, и на этом докладе поделится мудростью и паттернами, которые действительно работают (и кучей паттернов, которые никуда не годятся — тоже поделится). 
Алан покажет успешные стратегии автоматизации, как бороться с flaky-тестами, проведет сквозь опасности автоматизации UI и даст ряд других советов, основываясь на многолетнем опыте в автоматизации тестирования во множестве больших продуктов. Вы можете быть продвинутым тестировщиком, или напротив, только начинать карьеру — в этом докладе найдутся советы для всех, причем такие советы, которые можно применить на практике почти мгновенно. 
 Alan Page
Алан Пейдж проработал тестировщиком программного обеспечения примерно 25 лет. Он был основным автором книги «How We Test Software at Microsoft» и поучаствовал в создании «Beautiful Testing and Experiences of Test Automation: Case Studies of Software Test Automation». Кроме того, он пишет статьи на различные инженерные темы в своем блоге, его посты можно найти повсюду в интернете. Его последняя «книга» является коллекцией эссе на тему автоматического тестирования под общим именем «The A Word». Алан присоединился к Microsoft и стал частью команды Windows 95, и с тех пор работал над множеством релизов Windows, над ранними версиями Internet Explorer и Office Lync. В том числе, Алан два года проработал в Microsoft директором по тестированию. В январе 2017 года Алан ушел из Microsoft на должность директора по качеству в Unity.

Николай Алименков/XP Injection
Тестировщики часто говорят о противостоянии и конфликтах с разработчиками. Но ведь есть команды, где все живут в мире и согласии. Видимо, что-то тут не так? Хотелось бы поговорить о том, как тестировщиков видят сами разработчики. В докладе будет приведена забавная классификация. Кроме известного всем тестировщика-обезьянки будут представлены тестировщик-муха, тестировщик-нацист, тестировщик-панда и многие другие герои. 
Вы сможете лишний раз задуматься над тем, как вас видят со стороны и, возможно, изменить ситуацию к лучшему. Доклад будет также полезен менеджерам проектов и лидерам команд. Вы сможете быстрее распознавать те или иные шаблоны поведения тестировщиков и принимать меры по повышению уровня командной работы. Приходите, будет интересно! 
 Николай Алименков
Практикующий Java-техлид и Delivery Manager. Эксперт в разработке на Java, Agile-практиках и управлении проектами. Разрабатывает на Java более 12 лет, специализируется на разработке сложных распределённых масштабируемых систем. Активный участник и докладчик многих международных конференций. Основатель и тренер тренингового центра XP Injection. Организатор и идеолог конференций Selenium Camp, JEEConf, XP Days Ukraine и IT Brunch. Основатель «Клуба анонимных разработчиков».

Артем Ерошенко/Независимый консультант
Доклад про автоматизацию веб-интерфейсов от автора инструментов Allure и HtmlElements.
Давно занимаясь проектами, связанными с автоматизацией веб-тестирования, Артем сформировал свод правил, которые обеспечивают комфортную работу ему и его команде на протяжении жизни всего проекта от первого теста до нескольких тысяч. Этот свод правил условно разделен на три группы: «Простота разработки», «Доверие к результатам» и «Контроль качества».
В докладе речь пойдет об инструментах, которые позволяют Артему с командой создавать и править тесты максимально просто и наглядно. Также будут рассмотрены подходы, которые помогают добиться доверия к результатам прохождения тестов у всей команды. А еще Артем расскажет про то, как осуществляется контроль за качеством тестов в их проектах.
 Артем Ерошенко
Более 8 лет занимается автоматизацией тестирования веб-приложений. За это время работал в разных командах и в разных ролях: автоматизатор тестирования, менеджер команды разработки инструментов тестирования, руководитель группы автоматизации тестирования. Артем имеет большой опыт работы с популярными инструментами (Selenium, HtmlElements, Allure, Jenkins). Программирует в основном на Java, Groovy.

Николай Алименков/XP Injection
В Agile-подходах поменялось отношение к качеству и роли тестировщика. Теперь за тестирование и качество продукта отвечает вся команда. Но мало кто представляет, как эти подходы работают на практике, как осуществляется взаимодействие тестировщиков с разработчиками, какие стадии проходит задача на пути от требований до приема заказчиком, кто чем занимается на разных этапах.
Команд, где практикуют тесную работу разработчика с тестировщиком, не так много. В некоторых проектах данного подхода избегают ради экономии времени разработчика, на других — ради сохранения независимости мнения тестировщика. Разработчики тоже далеко не всегда понимают, чем конкретно может помочь им на практике тестировщик. Тем не менее, данная связка способна существенно улучшить качество продукта и избежать многих проблем.
В докладе мы поговорим о мотивации парной работы разработчика с тестировщиком, конкретных практиках и подходах на разных этапах работы над задачами, а также преимуществах, которые могут извлечь обе стороны из такой работы.
 Николай Алименков
Практикующий Java-техлид и Delivery Manager. Эксперт в разработке на Java, Agile-практиках и управлении проектами. Разрабатывает на Java более 12 лет, специализируется на разработке сложных распределённых масштабируемых систем. Активный участник и докладчик многих международных конференций. Основатель и тренер тренингового центра XP Injection. Организатор и идеолог конференций Selenium Camp, JEEConf, XP Days Ukraine и IT Brunch. Основатель «Клуба анонимных разработчиков».

Олег Ненашев/CloudBees
Появление Pipeline изменило подходы к автоматизации задач в Jenkins, особенно в случае параллельных сборок и тестов. В нем можно построить свой тестовый фреймворк и предоставить его автоматизаторам как набор библиотек. 
На примере Java-проектов покажем, как можно строить Pipeline-библиотеки для задач QA и переносить проекты на новую платформу. Мы интегрируем Docker, Maven, JUnit, FindBugs, Сoverity, а потом реализуем динамическую параллелизацию тестов. Также поговорим о подводных камнях и о том, как можно эффективно разрабатывать, тестировать и поддерживать подобные фреймворки. 
 Олег Ненашев
Разработчик в CloudBees, состоит в core team проекта Jenkins. C 2008 года занимается автоматизацией, инфраструктурой и фреймворкостроением для крупных программно-аппаратных проектов с помощью Jenkins и десятков других тулов. Пишет код, поддерживает ядро и плагины Jenkins, организует митапы в СПб и других городах.

Всеволод Брекелов/Grid Dynamics
Доклад для тестировщиков и разработчиков, которым интересно узнать: какие грабли могут возникнуть при работе с Google Cloud Standard Environment, как их избежать (протестировать), какие Google-инструменты можно взять и использовать в своих проектах. Также вы узнаете чуть больше о GAE, Memcache, Task Queues, Objectify, Protobuf, Bazel. 
 Всеволод Брекелов
Более 5 лет в тестировании программного обеспечения/автоматизации тестирования. Последний год работает Full Stack Developer/Tech Lead. Опыт построения автоматизации тестирования с нуля для mobile, desktop, веб-проектов (в основном для финансовых компаний). Любит участвовать в хакатонах и работать с умными коллегами. Провёл много собеседований (более 200, и уже перестал считать) для инженеров по автоматизации тестирования, разработчиков, аналитиков. Последние несколько лет работает в компании Grid Dynamics. Сейчас проживает в Калифорнии, работая по контракту в Google.

Андрей Солнцев/Codeborne
Flaky tests — головная боль автотестеров. Ещё вчера тест был зелёный, а сегодня он вдруг покраснел — ни с того ни с сего. Никто ничего не менял. Просто луна не в той фазе. Просто матрица шутит над тобой. 
Мы разберём кучу реальных примеров flaky-тестов из моей домашней коллекции. И разберёмся, как писать тесты, чтобы они были стабильными и независимыми от кармы разработчика. Доклад будет интересен и тестировщикам, и разработчикам — всем, кто балуется автотестами или просто любит разгадывать неразрешимые загадки. 
 Андрей Солнцев
Андрей — разработчик в эстонской компании Codeborne. Автор фреймворка Selenide, организатор таллиннского Devclub, частый докладчик на конференциях. Ярый приверженец экстремального программирования, автоматических тестов, парного программирования и чистого кода.

Alan Page/Unity
Большинство из нас слышали от уважаемых людей (или читали в интернете) «техническом тестировании». Конечно, технические знания — критически важная вещь для успеха. Но, несмотря на то, что большинство докладов о техническом тестировании заключается в обсуждении проблем автоматизации тестирования, как правило, нашим техническим ноу-хау существуют куда более ценные применения. 
В этом завершающем кейноуте Алан Пейдж поделится своим подходом к техническому тестированию, причинами, почему в своей команде они уменьшают количество автоматизации и как именно тестировщики могут более эффективно распорядиться своим техническим бэкграундом, чтобы внести больший вклад в работу команды. Кроме всего прочего, Алан покажет примеры наиболее любимых и практичных инструментов тестирования и расскажет о пользе наработки мощного и разностороннего набора инструментов. 
 Alan Page 
Алан Пейдж проработал тестировщиком программного обеспечения примерно 25 лет. Он был основным автором книги «How We Test Software at Microsoft» и поучаствовал в создании «Beautiful Testing and Experiences of Test Automation: Case Studies of Software Test Automation». Кроме того, он пишет статьи на различные инженерные темы в своем блоге, его посты можно найти повсюду в интернете. Его последняя «книга» является коллекцией эссе на тему автоматического тестирования под общим именем «The A Word». Алан присоединился к Microsoft и стал частью команды Windows 95, и с тех пор работал над множеством релизов Windows, над ранними версиями Internet Explorer и Office Lync. В том числе, Алан два года проработал в Microsoft директором по тестированию. В январе 2017 года Алан ушел из Microsoft на должность директора по качеству в Unity.


Теперь доступна вся информация, позволяющая определиться «смотреть ли открытую трансляцию». Надеюсь, программа получилась хорошей, и вы захотели к нам подключиться. Адрес трансляции вы уже знаете. Ждём 8-9 декабря на конференции и у мониторов!
кибер-ниндзя
=====
5 аргументов в пользу коробочной АТС
2017-12-05, 12:41
Компания «АйПиТелефон»
=====
Дуглас Крокфорд, Азат Мардан, Виталий Фридман и все-все-все: анонс бесплатной YouTube-трансляция HolyJS 2017 Moscow
2017-12-08, 10:22

Любой JavaScript-разработчик однажды задается вопросом: на какую конференцию по JavaScript сходить? Интересные доклады послушать, людей посмотреть, себя показать. В 2016 году мы пошли по тому же пути и обнаружили, что в России ничего подходящего на эту тему нет. Всевозможные митапы по фронту — пожалуйста. Но ведь JS давно уже не только фронт. Короче, мы решили исправить эту несправедливость и создали главную и единственную в России JavaScript-конференцию, HolyJS. На Холи бывает не только фронт, но и бэкенд, десктоп, и даже работа с железом. Теперь нам всем есть куда ходить, но это не конец истории.
В JavaScript-мире есть важнейшая особенность, которая отличает его от миров типа Java и .NET: технологии и фреймворки развиваются стремительно. Не попавшие на конференцию люди оказываются в невыгодном положении — мало кому хочется смотреть записи, устаревшие на 4 месяца. Обычно есть несколько фундаментальных докладов, которыми можно гордиться и пересматривать еще пару лет, но большая часть материала интересна и полезна в моменте: посмотрел, послушал, пошел использовать на работе. Всё это останется за пэйволлом? А как же комьюнити и чувство локтя? Нет ли в этом чего-то неправильного?
Теперь трансляцию первого зала можно смотреть бесплатно, с нашего сайта и на YouTube. Это позволяет всем желающим присоединиться к конференции и провести на ней все два дня. Прийти вживую было бы круче, но на этой полномасштабной трансляции можно по-настоящему погрузиться в тему и ощутить атмосферу происходящего.
С другой стороны, покупатели платной трансляции тоже не обижены: там есть возможность смотреть вообще все залы (то есть в три раза больше докладов) и переключаться между ними, плюс они покупают сервис: например, приоритетное решение любых проблем и гарантированные записи всех докладов в высоком качестве. Всё вместе выглядит довольно сбалансированным и честным решением: мы и комьюнити помогли, и себя не забыли.
Всё вышесказанное было об упаковке. Что же ждет нас внутри? Куча реально огненных докладов, которые ведут не абы кто, а люди первой величины. Например, будет Дуглас Крокфорд и Азат Мардан, если вам о чем-то говорят эти имена. Будет Виталий Фридман и Леа Веру. Мы понимаем, что Дуглас Крокфорд приезжает в Россию редко — поэтому у него будет целых два доклада. И это только первый зал!
Приятного просмотра, и да пребудет с вами сила Пресвятого Джаваскрипта!
Раньше мы выкладывали на Хабр ссылки на всевозможные плейлисты на YouTube и предупреждали: «Вот эта ссылка — на первый день, вот эта — на второй, вот эта — еще на что-нибудь». Вам нужно было сохранить ссылку на этот хабрапост и потом следить за его обновлениями. 
Есть путь гораздо проще. Бесплатную трансляцию можно смотреть прямо с нашего сайта, вот по этой ссылке:
Эта ссылка — конечный источник истины. Именно там отображается самая свежая, актуальная программа. Когда-нибудь в далеком будущем там появятся удобные органы управления онлайн-просмотром, но сейчас для максимального удобства там просто встроен плеер YouTube: можно смотреть с сайта (развернув на весь экран), можно прыгнуть на YouTubе. Еще, можно подписаться на наш канал, чтобы увидеть записи, когда они будут выложены в общий доступ.
Развитие конференции очень похоже на развитие программной платформы. Наши языки и платформы постоянно конкурируют между собой: например, Java гонится за .NET по синтаксису основного языка, а .NET пытается догнать Java по кроссплатформенности, и потом все они пытаются угнаться по удобству и скорости разработки за JavaScript. Как только появляется новая идея, реализации этой идеи начинают появляться во всех популярных технологиях.
Создание конференций очень похоже на развитие программной платформы. У JUG.ru Group есть своя платформа, список базовых фичей. Как только одна из конференций что-то улучшает, обновление получают и все остальные. За этот год, в том числе этой осенью, было множество конференций, и много всего улучшилось.
Онлайн-трансляция — это именно то, что все мы хотим и ожидаем увидеть от хорошей конференции. Чтобы улучшить трансляцию, зачастую достаточно просто добавить больше качественной техники и правильно её настроить. Поэтому трансляции получили за эту осень максимальное количество полезных обновлений. 
Конференция проводится в течение двух дней. Первый день начинается в 10.00, второй — в 10.30. Каждый день начинается открывающим кейноутом и оканчивается завершающим кейноутом в 18.15. Актуальную программу первого зала можно увидеть на странице трансляции.
 Открывающий кейноут. Douglas Crockford — The Post JavaScript Apocalypse
Когда весной легендарный Дуглас Крокфорд выступал на петербургской HolyJS с темой «Numbers», он мимоходом упоминал, что должны появиться новые языки программирования: «не можем же мы оставить нашим детям JavaScript, это было бы ужасно». А в этот раз, открывая конференцию выступлением о «пост-джаваскриптовом апокалипсисе», он раскроет тему подробнее. Каким должен быть язык, который рано или поздно потеснит JavaScript? И что нам делать сейчас, пока его ещё нет?
Minko Gechev — Faster Angular applications
С одной стороны, мы все понимаем важность того, чтобы веб-приложения открывались и работали как можно быстрее. С другой стороны, мы делаем громоздкие single-page applications, где зачастую требуется разом загрузить много ресурсов, а затем производим столько вычислений в главном треде, что «тормоза» неудивительны. Как добиться рендеринга с частотой 60 FPS и быстрой загрузки Angular-приложения? Минко собирается рассказать и о том, и о другом.
Алексей Богачук — Безопасность: уязвимости вашего приложения
Актуальны ли ещё угрозы XSS? Прошло около 20 лет с тех пор, как Cross Site Scripting (XSS) появился как вид атаки. С тех пор мы получили богатый опыт и знания, защита наших сайтов стала намного сложнее, а многочисленные фреймворки были призваны оберегать нас от ошибок. Но последние данные показывают совсем другую картину: в первых кварталах 2017 года количество сообщений об XSS-атаках и количество найденных уязвимостей выросло в несколько раз.
Алексей расскажет о том, почему ваши приложения в опасности, почему фреймворки не спасают, как находить уязвимости и какие инструменты для этого использовать.
Виталий Слободин — Headless browsers: что, как и почему
При взгляде на название доклада многие могут подумать: «К чему мне слушать его, я и так в курсе, что такое headless browsers». И действительно, такие браузеры давно вошли в жизнь разработчиков — с их помощью проводят тестирование кода, проверяют качество и соответствие верстки, и не только. Но проблема в том, что разработчики мало знают про то, как устроен и работает их инструмент. Так что вопрос «как» из названия не менее важен — и многие смогут найти для себя ответ.
Николай Матвиенко — Поиск и устранение неисправностей Node.js-приложений под капотом
«Неочевидные возможности» Node.js, о которых можно будет узнать в кейноуте Азата Мардана — это, конечно, здорово. Но что делать, если с вашим Node.js-приложением возникла проблема, и вам не до прикручивания чего-то нового, а надо быстро починить имеющееся? На докладе Николая вы узнаете о возможностях диагностики production-приложений с использованием отладки и трассировки приложения на разных уровнях; о том, как отслеживать проблемы производительности и утечки памяти внутри Node.js-приложения.
 Завершающий кейноут. Виталий Фридман — New Adventures in Responsive Web Design
О Responsive Web Design говорят далеко не первый год, но со временем вопросов становится не меньше, а только больше. Браузеры стали поддерживать HTTP/2, Service Workers, Responsive Images, Flexbox, SVG, Font Loading API — как теперь правильнее использовать это всё? А если начал использовать, что делать с легаси-браузерами?
Такие вопросы как раз подходят Виталию Фридману, сооснователю известного сайта Smashing Magazine. Во-первых, этот сайт не первый год освещает подобные темы, а во-вторых, самому сайту тоже приходится решать для себя подобные вопросы. В общем, недостатка экспертизы тут явно не будет.
 Открывающий кейноут. Azat Mardan — You don't know Node
Как известно, если кто-то использует JavaScript, это ещё не значит, что он знает JavaScript. Об этом свидетельствует хотя бы популярность книги «You Don’t Know JS» (она обещает по-настоящему научить JS даже тех, кто считал, что и так всё знает). Азат Мардан заявляет, что с Node.js ситуация похожая: многие разработчики, имеющие с ним дело, при этом не вполне знают его возможности. И кейноут «You don’t know Node» призван это исправить, показав пять неочевидных фич.
Антон Лобов — TypeScript, или Зачем так сложно
TypeScript всего за пару лет сумел из очередного «JavaScript с классами» превратиться в мощный инструмент для типизации сложного кода на JavaScript. С точки зрения разработчика всё стало очевиднее и удобнее. Но не всё так очевидно, как хотелось бы.
Антон поможет разобраться, какие подводные камни таит в себе типизация, предлагаемая в TypeScript, зачем она такая сложная и как она напрямую влияет на поддержку языка со стороны IDE и других средств разработки (иногда очень неожиданным образом).
Douglas Crockford — Managing Asynchronicity with RQ
И снова Дуглас Крокфорд. В этот раз уже не с общими рассуждениями о «языке будущего», а с конкретным ответом на конкретные вызовы сегодняшнего дня. RQ — маленькая JS-библиотека от Крокфорда, призванная помочь работать с асинхронностью в серверных приложениях. По словам самого Крокфорда, она «призвана облегчить использование, и даже более того, с помощью минимализма».
Павел Черторогов — GraphQL: заключаем выгодный контракт между сервером и клиентом
GraphQL приходит на смену REST API. GraphQL Model — выработанные за 2 года докладчиком принципы построения, типов и связей. Павел расскажет, как выжать все соки из типизированной схемы и статического анализа кода, затронет «божественный рефакторинг». Будет много информации по бэкенду.
Martin Splitt — Better, faster, stronger: getting more from the web platform
На петербургской HolyJS Мартин разбирал производительность рендеринга «с самых низов», начиная с отдельного пиксела. Эта страсть «зайти туда, докуда многие не доходят» для него типична — и на московской он будет разобраться с фичами, которые считает либо недооцененными, либо слишком новыми для массового внимания. Ими станут: Web Workers (для параллельной работы без блокировки UI-потока), Streams API (для работы с данными по мере их доступности, без ожидания окончания передачи) и WebAssembly.
 Завершающий кейноут. Lea Verou — /Reg(exp){2}lained/: Demystifying Regular Expressions
Леа уже впечатлила своим выступлением многих зрителей весенней петербургской HolyJS, а теперь закроет московскую конференцию другой темой. «Регулярные выражения» могут казаться и чем-то тысячу раз разжёванным, и в то же время всё равно не до конца понятным (а потому пугающим). Но Леа умеет объяснять и демонстрировать всё на наглядных примерах так, что не оказывается ни страшно, ни скучно. А если вы считаете, что уже знаете о «регулярках» достаточно и слушать о них снова вам ни к чему, тут как раз будет хорошая возможность проверить, действительно ли вы можете быстро подобрать в уме правильное выражение для некоторых неочевидных случаев. 
Теперь доступна вся информация, позволяющая определиться «смотреть ли открытую трансляцию». Надеюсь, программа получилась хорошей, и вы захотели к нам подключиться. Адрес трансляции вы уже знаете. Ждём 10-11 декабря на конференции и у мониторов!


кибер-ниндзя
=====
Объяснение SNARKs. От вычислений к многочленам, протокол Пиноккио и спаривание эллиптических кривых (перевод)
2017-12-14, 20:04
Архитектор программного обеспечения
=====
Как рассчитать ROI при внедрении Office 365 с помощью ROI-калькулятора
2017-12-05, 15:26
Пользователь
=====
Эволюция «img»: Gif без формата GIF
2017-12-05, 13:51
Пользователь
=====
Настройка валидации DTO в Spring Framework
2017-12-05, 15:40
backend.developer { java, kotlin }
=====
Agile Kitchen в офисе Туту.ру
2017-12-05, 13:57

В пятницу, 15 декабря в офисе Туту.ру пройдет очередной Agile Kitchen. Темой встречи станет практика и теория масштабирования Agile. Своими кейсами и опытом поделятся эксперты из Сбербанка, ScrumTrek, EPAM и других компаний. Вас ждут интереснейшие доклады и ставший уже привычным игровой трек.
09:30 — 10:00 Регистрация
10:00 — 10:10 Вступительное слово от организаторов / Иван Селеверстов, Scrumtrek
10:10 — 11:00 Результаты исследования Agile в России 2017 / Сергей Рогачев, Scrumtrek
11:00 — 11:40 Канбан в масштабе организации / Сергей Карач
11:40 — 11:55 Кофе
11:55 — 12:35 Как выбраться из ловушки масштабирования / Михаил Вязанкин
12:35 — 13:15 Штаб и Scaled Agile / Владимир Каленов, Сбербанк
13:15 — 14:00 Обед
14:00 — 14:40 Масштабирование Agile — как скрестить ужа с ежом? / Алексей Ионов, Ионов и партнеры
14:40 — 15:20 Spotify и SAFe как способ трансформации крупных организаций / Иван Спресов, EPAM
15:20 — 15:35 Кофе
15:35 — 16:35 Выбери свой подход масштабирования/ Сергей Рогачев, ScrumTrek
16:35 — 17:35 The LeSS Dynamics Game/ Андрей Коломенский, OnAgile 
17:35 — 17:50 Закрытие конференции 
Участие бесплатное, необходима регистрация (обязательно)
Не забудьте в день мероприятия взять с собой паспорт или водительское удостоверение.
Начало мероприятия в 10:00
Адрес: 1-й Нагатинский пр-д, 10, 12 этаж, офис Туту.ру
Страница встречи в Facebook.
И не забудьте зарегистрироваться. До встречи!
Главный редактор «Tutu. Сюжетов»
=====
ASO Monthly #18 Октябрь 2017
2017-12-05, 13:59
Маркетинг, ASO
=====
Принципы SOLID в действии: от Slack до Twilio
2017-12-05, 15:14
автор, переводчик, редактор
=====
Производительность выгрузки большого количества данных из Mongo в ASP.NET Core Web Api
2017-12-05, 14:17
 Возникла необходимость выгрузки большого количества данных на клиент из базы MongoDB. Данные представляют собой json, с информацией о машине, полученный от GPS трекера. Эти данные поступают с интервалом в 0.5 секунды. За сутки для одной машины получается примерно 172 000 записей. 
Серверный код написан на ASP.NET CORE 2.0 с использованием стандартного драйвера MongoDB.Driver 2.4.4. В процессе тестирования сервиса выяснилось значительное потребление памяти процессом Web Api приложения — порядка 700 Мб, при выполнении одного запроса. При выполнении нескольких запросов параллельно объем памяти процесса может быть больше 1 Гб. Поскольку предполагается использование сервиса в контейнере на самом дешевом дроплете с оперативной памятью в 0.7 Гб, то большое потребление оперативной памяти привело к необходимости оптимизировать процесс выгрузки данных. 
Таким образом, базовая реализация метода предполагает выгрузку всех данных и отправку их клиенту. Эта реализация представлена в листинге ниже.
В качестве альтернативы применялся метод использования запросов с заданием номера начальной строки и количества выгружаемых строк, который показан ниже. В этом случае выгрузка осуществляется в поток Response для сокращения потребления оперативной памяти.
Также применялся вариант установки параметра BatchSize в курсоре, данные также записывались в поток Response.
Одна запись в базе данных имеет следующую структуру:
Тестирование производительности осуществлялось при запросе с использованием HttpClient.
Интересными считаю не абсолютные значения, а их порядок.
Результаты тестирования производительности для трех вариантов реализации сведены в таблице ниже.
Данные из таблицы также представлены в виде диаграмм:
Подведя итоги, можно сказать, что использование такого рода мер снижения потребления оперативной памяти приводит к существенному ухудшению производительности — более чем в 2 раза. Рекомендую не выгружать поля, которые не используются клиентом в текущий момент.
Делитесь своими методами решения подобной задачи к комментариях.
Протестирован вариант реализации с yeild return
Дополненные результаты сведены в таблицу:
Так же было замерено время на перемещение курсора await cursor.MoveNextAsync() в варианте 3 и сериализацию batch объектов 
с записью в поток вывода. Перемещение курсора занимает 1/3 времени, сериализация и вывод 2/3. Поэтому выгодно использовать StringBuilder для Batch около 2000, прирост памяти при этом незначительный, а время получения данных снижается более чем на треть до 6 — 7 секунд, уменьшается количество вызовов await Response.WriteAsync(JsonConvert.SerializeObject(doc)). Также можно сериализовать объект вручную.
User
=====
На пути к захвату мира. Vivaldi для ARM
2017-12-05, 14:33
Зрю в корень, жгу глаголом
=====
Гаджет моего тела (Часть #1)
2017-12-06, 09:28
Источник
«Красота требует жертв. Но все чаще она требует денег. Очень много денег».
(из истории болезни)
Гаджеты, они сегодня как женщины. Они привлекательны, они элегантны, они желанны почти для всех, их появления ожидают с нетерпением, переходящим в драки, и, главное, они везде. Гаджетофобы тоже встречаются – куда же без них — но круг их невелик. Что же касается самих женщин, то, похоже, конвергенция их с гаджетами незаметно произошла уже давно и безвозвратно. И теперь абсолютно везде вы можете встретить тех, кто занят исключительно своим смартфоном. Производители последних желали бы повторить успех на новом витке развития гаджетов. Например, в сфере электронных часов и фитнес-браслетов, но пока, однако, этого не случилось, по-видимому, из-за ограниченной потребности в таких устройствах. Ну что же, еще не все потеряно. Ниже посмотрим, что же сегодня с развитием Интернета вещей (IoT) нам предлагает рынок современных гаджетов.
Развитие так называемого «Интернета вещей», о котором нам уже несколько лет вещают из тех же гаджетов, обещает новую жизнь целым классам носимой электроники, начиная хотя бы с одежды. Собственно, термин «носимая электроника» (wearable electronics) возник около десяти лет назад, хотя устройства данного класса существовали достаточно давно. Первыми из них устройств стали наручные электронные часы. Следующим стало появление мобильного телефона, а вслед за ним — мультимедийного плеера. Сегодня к данному классу техники относятся все электронные устройства, носимые пользователем на себе или встроенными в одежду, другие аксессуары: перчатки, обувь, головные уборы, украшения. 
Источник
Наиболее простой разновидностью wearable electronics является «умная» ткань или одежда (smart fabrics). Взять хотя бы из самого простого разгрузочную жилетку для гаджетов от американской компании ScotteVest, где предусмотрено не только множество карманов, но и запатентованные внутренние пространства для проводов и кабелей. Похоже, эти патенты уже не принесут владельцам прибыли. Более изощренные решения со встроенными в одежду датчиками для самых различных целей от медицинских до спортивных и от мониторинговых до охранных тоже есть, причем уже более 10 лет, но они, как правило, имеют узкую специализацию и, как следствие, заметное увеличение массогабаритных показателей одежды. 
Источник #1, Источник #2
Конструирование одежды, интегрированной с электроникой для управления гаджетами или взаимодействием с ними – еще одно направление моды. При повышении уровня интеграции и миниатюризации электронных узлов можно получить полностью интегрированную в одежду электронную систему, включая электронные модули, межмодульные соединения, датчики, интерфейсы ввода данных и всевозможные индикаторы. В ряде случаев в одежду могут быть встроены источники питания. Это второе поколение smart fabrics, имеющее более высокий уровень интеграции и даже свой термин SFIT (Smart Fabric and Interactive Textile). Ее широкому распространению мешает лишь пока еще узкая ниша применения и отсутствие общепринятых стандартов. С другой стороны, современная электроника слишком быстро совершенствуется, чтобы за ней успевали какие-то стандарты как моды, так и техники. 
Источник #1, Источник #2
Одной из сфер применения smart fabrics является обеспечение личной безопасности человека. Это и просто подача сигнала SOS, и даже туфли c вмонтированным электрошокером, приводимым в действие тревожной кнопкой на пульте дистанционного управления, спрятанном в ожерелье. Есть еще бейсбольная кепка со встроенным видеорегистратором или очки с видеокамерой.
Сложно сказать, можно ли отнести к носимой электронике «умные» автомобили, но в последнее время они стремительно превращаются в мобильные гаджеты, в которые оказываются встроенными теперь их пользователи и которые требуют соответствующей защиты. Каждое следующее поколение автомобилей демонстрирует все большую интегрированность технологий, в том числе для удаленной диагностики, телематики, самоуправляемости и разнообразного информационного обмена (инфотейнмента). 
Источник
Большое количество сторонних приложений, сложность самих систем и постоянно растущий объем обновлений сегодня затрудняют качественную проверку автомобильных систем на наличие киберугроз, а также программных и архитектурных ошибок. Теперь они готовы защищать своих владельцев, и, в частности, в рамках форума New Mobility World / IAA 2017 во Франкфурте «Лаборатория Касперского» и компания AVL представили прототип модуля безопасного соединения (Secure Communication Unit – SCU) для таких автомобилей. При всем своем удобстве современные автомобили остаются потенциально уязвимы для злоумышленников, и безопасный и защищенный SCU-модуль станет единой точкой входа/выхода всех подключений и коммуникаций автомобильных систем.
Источник
Стоит напомнить, что сегодня «умные» автомобили — это отнюдь не только смесь транспорта, робота и искусственного интеллекта, как это отражается в массовом сознании будущих потребителей. Фактически же «умными» автомобили подразделяются на: автономные (самоуправляемые) и полуавтономные; обладающие продвинутой бортовой системой навигации и инфотейнмента; экологически чистые электромобили и автомобили с альтернативными системами питания; эксклюзивные или уникальные представители мира автомобилей, созданные с определенной целью. 
Источник
Все чаще под «умными» автомобилями подразумеваются именно автомобили, которым не нужен водитель, разработки Google, обладающие собственным набором датчиков для перемещения по дорогам, не оснащенные рулем и педалями и обещающие произвести революцию в мире дорожных путешествий. В конце концов, автомобиль, с которым можно поговорить (с помощью приложения Siri, например), тоже можно назвать «умным». Впрочем, управлять автомобилем с помощью wearable electronics, очевидно, тоже будет можно. 
Источник #1, Источник #2, Источник #3
Чтобы электронные схемы и проводники можно было встраивать в одежду или различные приборы, надеваемые на тело, они должны быть не только гибкими, но и очень эластичными. Технологии «электронной ткани» и «гибкой электроники» (flexible electronics) взаимосвязаны не только друг с другом, но и с теми же «умными» автомобилями. Они подразумевают создание новых материалов и методов для производства электронных схем на гибких подложках. 
Источник
Для встраивания в структуру ткани (одежды или сидения) требуется гибкая клавиатура, гибкий дисплей, гибкие датчики давления и температуры. Все это сегодня уже имеет место быть.
Источник
Тенденция встраивания различных полезных устройств добралась даже до домашних тапочек, из которых, к примеру, можно сделать пылесос. Для этого в их подошвах размешаются сопла пылезаборников, контейнеры для пыли, источник питания и электродвигатель с миниатюрной воздушной турбинкой. Возможно, чтобы поддерживать чистоту в казарме, аналогичное устройство можно встроить и в сапоги.
Электронные носимые системы навигации обеспечивают определение координат владельца на местности и помогают выбрать правильный маршрут при движении, а также обеспечивают передачу координат для оказания экстренной помощи в опасных ситуациях. Часто они используются и в военных целях. Несколько лет назад компанией Honeywell была представлена серия магниторезистивных датчиков, на базе которых реализуются миниатюрные навигационные модули с GPS-приемником, размещенные в поясе. Специальный алгоритм учитывает любые движения человека: ходьбу, бег, смещение боковое, ползание, движение задним ходом, даже топтание на месте. Автоматический компас определяет положение тела оператора — стоит он или лежит. Низкое энергопотребление позволяет носить устройство постоянно включенным. Барометрический высотомер определяет номер этажа при движении объекта внутри здания. Программное обеспечение может модифицироваться дистанционно. Для модуля не требуется подключение дополнительных датчиков движения, которые крепятся, например, на ноги. Без всего этого современный солдат может чувствовать себя совсем незащищенным.
Источник #1, Источник #2
Первая интегрированная боевая солдатская система Land Warrior появилась более пятнадцати лет назад. Она включала в себя встроенный в униформу компьютер, а также рации, системы управляемого оружия. Окологлазный микродисплей (почти как у Терминатора) встраивается в шлем, а система навигации — в пояс и ботинки. Все эти устройства связаны между собой каналами связи. С их помощью солдат может передавать голосовые и цифровые данные, команды, изображения, как другим членам своего подразделения, так и командирам подразделений.
Источник
Появляются новые типы нательной «умной одежды» для военнослужащих (smart shirt), которая содержит систему датчиков для мониторинга состояния и положения: локальной температуры тела, ЭКГ, пульса, наличия ранений или ожогов, целостности одежды. Нательная рубашка пронизана оптоволоконными нитями, которые позволяют, в частности, косвенно контролировать целостность кожного покрова, поскольку при проникающих ранениях нарушается локальная проводимость сети. Определяется положение зон поражения на теле пулей или осколками и тип ранения. В ту же одежду встраиваются микрошприцы с электронным управлением от автономного носимого компьютера или с терминала командира подразделения для введения антишоковых препаратов или противоядий. Активация шприцев может осуществляться как по инициативе самого бойца, так и по команде из блока носимой электроники или же по радиоканалу из командного терминала на основании показаний датчиков при потере бойцом сознания (например, после ранения или в результате контузии). «Умная одежда» также может быть совмещена с бронежилетом. При этом защитная одежда содержит армированные силовые элементы и интегрированные датчики пульса, температуры, а также механических повреждений текстильной «брони». 
Источник
В какой-то мере к электронной одежде можно отнести и экзоскелеты для военных и людей с ограниченными возможностями, которые удачно дополняют и заменяют недостаточные возможности человека. 
Источник
Впрочем, носимая электроника – лишь этап перехода к электронике, встраиваемой в человеческое тело, которое станет электронным. Сможем ли мы действительно управлять приборами силой мысли? Не превратимся ли в придатки машин? Сегодня об этом спорят инженеры и философы, дизайнеры и врачи, специалисты по безопасности и педагоги.
И мы тоже скоро вернемся к этой теме.
Автор публикации:
Александр ГОЛЫШКО, системный аналитик ГК «Техносерв»
User
=====
Производительность как восприятие: управление терпением
2017-12-22, 12:53
Crusader
=====
CRM система или Help Desk? В чем принципиальная разница, откуда путаница, и что есть на рынке Help Desk-ов?
2017-12-06, 09:42
SaaS, управление проектами, разработка ПО, Okdesk
=====
RAIDIX и Intel Lustre: как сделать люстру на много лампочек
2017-12-05, 15:11
User
=====
DNS-туннель, PsExec, кейлоггер: разбираем схему и технические инструменты атаки
2017-12-05, 15:37
 Имя компонента
 
 Назначение компонента
 
 Malware_dll.dll
 
 Кейлоггер (32-битная версия)
 
 Malware_dll64.dll
 
 Кейлоггер (64-битная версия)
 
 Bach.dll
 
 Переименованная оригинальная библиотека System_dll.dll, которая является службой SystemService
 
 System_dll.dll
 
 Библиотека, которая при запуске службы System_Service подкачивается в svchost.exe. System_dll.dll поддерживает те же вызовы, что и system_dll2.dll, путём перенаправления всех этих функций в system_dll2.dll. Подтягивает _________.dll. Является 32-разрядной версией
 
 System_dll.dll_
 
 64-разрядная версия System_dll.dll
 
 _________.dll
 
 BackDoor, 32-разрядная версия
 
 _________.dll_ver2
 
 64-разрядная версия _________.dll
 
 S64
 
 Аналог System_dll.dll для х64 архитектуры
 
 P64
 
 Аналог _________.dll для х64 архитектуры
 
 It_helpdesk.exe
 
 Переименованный PsExesvc.exe (компонент PSExec, который создается и запускается на удаленной машине с целью выполнения заданных действий
 
 Users.exe
 
 BackDoor. Функционал аналогичен _________.dll, но маскируется под jusched.exe – «Java Update Scheduler»
 
Пользователь
=====
Облачные сервисы IBM Connections и Verse: обзор возможностей
2017-12-05, 15:39
User
=====
Что нужно знать о чат-ботах?
2017-12-05, 15:44
Пользователь
=====
Dart: как правильно начать знакомство с языком?
2017-12-06, 13:26

Всем привет! На прошлой неделе мы провели вторую ежегодную встречу русскоязычного сообщества Dart (ака Dartup) и получили очень информативную и полезную обратную связь от людей, пришедших на мероприятие. В этом году нас было около 130 человек. Если вы не были и хотите посмотреть, вот ссылка на записи. Спасибо за митап всем участникам, организаторам и партнерам.
Многие гости в кулуарах и в телеграм-канале, посвященном встрече, спрашивали нас, почему, обладая большой экспертизой в языке, мы до сих пор не задумались о Dart академии, вебинарах или хотя бы кратком руководстве для новичков. Признаюсь, чтобы начать такой процесс, нам нужен был импульс от вас. Мы его получили. Поэтому начинаем серию статей для тех, кто интересуется языком и хочет попробовать в нем разобраться. И если вам понравится и пригодится эта информация, то мы как компания сможем больше уделять этому времени.
Давайте начнем с базового плана:
Как это ни страно, на https://www.dartlang.org. Также там есть мини-курс по синтаксису и возможностям языка. А так как Dart — это целая экосистема, то, как и в Java, надо изучить и стандартные библиотеки, для этого есть библиотечный тур. И если после прочтения вы всерьез заинтересовались возможностями языка — обязательно уделите внимание гайду Effective Dart — советы, изложенные в нем, помогут сделать ваш код высокоэффективным. Для тех, кто любит сухой язык API вместо текста, вот прекрасная ссылка.
Dart, как и JavaScript, стандартизирован ECMA-408, в этом документе достаточно подробно расписаны все нюансы языка.
Дарт — язык универсальный. С его помощью можно создавать утилиты командной строки, серверные приложения, заниматься Web-разработкой и даже делать приложения для мобильных платформ. Ниже мы расскажем об этом подробнее.
CLI, весь тулинг дарта написан на самом языке. То есть pub, analyzer, dartdoc, dartfmt, dart2js — всё создано с использованием самого Dart, вы можете посмотреть исходники по ссылкам. Утилиты достаточно просто писать с учетом того, что есть мануал. 
У нас на бэкенде в продакшне работает приложение, написанное на Дарте, оно использует БД и активно работает с сокетами. Также мы используем свои cli-утилиты для облегчения сборки или кодогенерации. Более подробно про разработку на DartVM можно прочитать тут.
Одной очень крутой особенностью DartVM является запуск отладчика и профилировщика, который входит в сам DartVM и называется Observatory. Как утверждает Google, отладчик практически не влияет на производительность исполняемого кода, при этом через данный инструмент нельзя подменить исполняемые исходники, но можно менять состояние.
Тут всё довольно просто — вы можете использовать различные биндинги через js-interop к библиотекам js-мира. Да, нужно писать враперры, но Dart и JS — это разные миры, так что приходится платить за типизацию и поддержку кода. 
Можно писать на чистом дарте без фреймворков или использовать космолёт AngularDart. В Wrike мы используем AngularDart на проде для основного продукта (в данный момент на чистом Дарте написано приложение, в минифицированном виде занимающее у клиента 15Мб), так что можем сказать, что он проверен временем и готов для боевых условий. И пусть вас не смущает версионность, пути между TS и Dart версией разошлись с версии 2.0 (еще раз: версии Angular для TypeScript и Dart — не совпадают!), сейчас это разные миры с похожим API. 
Так же есть нативный redux и over_react.
Dart предлагает единый подход разработки сразу на обе мобильные платформы — Flutter. Пока это альфа, но работает очень быстро и стабильно. В компании мы только начинаем делать на ней прототипы, поэтому пока большой экспертизой похвастаться не можем. Но, по предварительным оценкам, флаттер может пригодиться нам, чтобы унифицировать бизнес-логику и упростить разработку. 
Минимальный размер приложения для Android в релиз моде весит около 5Мб + всё, что вы напишите сами. Вместе с флаттером Google продвигает инструменты для мобильной разработки, что делает её очень простой. Особенно хочется отметить фичу Hot Reload, которая делает разработку под мобильные устройства фантастической.


Если вы не хотите ничего устанавливать и желаете просто "пощупать" язык, вам нужно зайти на DartPad, где можно пошарить код или загрузить примеры. Есть ограничение на подключение внешних пакетов, но стандартной библиотеки на первых порах хватит всем. При этом вы получаете полноценный редактор с подсветкой, ассистентом и выводом ошибок компилятора.
Для этого можно воспользоваться различными редакторами:
→ Подробная инструкция.
Каждый из них будет использовать dartanalyzer server, так что вы получите практически во всех редакторах одну и ту же поддержку функциональности.
Вам нужно поставить любой ваш любимый редактор, который поддерживает Dart.
Далее создать проект:
Любой путь использует пакет stagehand, тут каждый выбирает свой вариант. Для примера я буду использовать шаблон 'Bare-bones Web App', в котором не используется никакой фреймворк, всё написано на чистом Dart с использованием стандартной библиотеки 'dart:html' для манипуляции с дом-деревом.


После исполнения команды у нас будет создано два файла
.packages — это файл с ссылками на пакеты.
.pubscpec.lock — это файл с конкретными версиями пакетов, которые разрезолвил pub.
В мире Dart pub выполняет роль пакетного менеджера, все доступные пакеты вы можете посмотреть тут. Также pub умеет публиковать пакеты, плюс есть еще две возможности, которые скоро уедут из pub — это сборка проекта и запуск http-сервера для запуска проекта в браузере, если мы пишем под веб.
Теперь запустим наш проект:
Консоль:

Консоль:

Тогда при вызове комманды pub serve будет автоматически выбран нужный компилятор.
Если вам потребуется собрать бандл для продакшна, то достаточно вызвать
и вы получите всё что нужно для запуска приложения в папке build.
Пока на этом остановимся. Задавайте в комментариях вопросы и пишите предложения по поводу тем для будущих статей в этой серии. Буду рад ответить.
Другие материалы русскоязычного сообщества Dart
Пользователь
=====
Отчет с митапа Осенний Postgres в Райффайзенбанке
2017-12-06, 14:07
Пользователь
=====
Как мы решили внедрять Помидор в компании
2017-12-05, 17:15
User
=====
Кейс: продвижение мобильного браузера Smart Search через влогеров
2017-12-05, 17:43
Пользователь
=====
Не было печали, апдейтов накачали (Arch)
2017-12-05, 19:49
По мотивам этого поста.
Про Archlinux ходит множество слухов, в том числе не совсем правдивых. В частности, устоявшееся общественное мнение говорит, что Арч часто ломается при обновлениях, так как bleeding edge. На практике это это одна из самых живучих и ремонтопригодных систем, которая может жить годами без переустановок, при этом обновляясь чуть не каждый день.
Иногда, около одного-двух раз в год, проблемы всё-таки возникают. Какой-нибудь злостный баг умудряется просочиться в репозитории, после обновления система ломается, и вы как пользователь мало что можете с этим сделать — надо откатываться.
С очередным обновлением всё ломается к хренам собачьим. Уровень возможных проблем варьируется от "шрифты стали некрасивые" до "перестала работать сеть", а то и "ядро не видит дисковые разделы". Многие пользователи Арча умеют справляться с этой проблемой так или иначе, а в этой статье я расскажу как это делаю я.
Для проведения ремонтных работ нам нужна консоль и работающая сеть. Если есть — хорошо. Если нет — есть простой и быстрый способ гарантированно их получить. Грузимся с загрузочной флешки, монтируем наши дисковые разделы, чрутимся в систему:
Всё, мы в консоли нашей системы, и у нас есть сеть.
А наличие загрузочной флешки в арсенале арчевода — практически обязательно.
Система отремонтирована!
Существует специальный сервер обновлений под названием Arch Linux Archive (ранее он назывался Arch Linux Rollback Machine), в котором хранятся "слепки" всех репозиториев на каждую отдельную дату.
Желаемую дату можно выбрать, специальным образом задав имя сервера в файле mirrorlist. Проще всего старый файл забэкапить, создать новый и добавить туда единственную строчку (в примере задано 1 декабря 2017г)
После чего команда pacman -Syyuu принудительно обновит базу данных пакетов на ту, которая соответствует выбранной дате, и принудительно переустанавливает все пакеты в системе, версии которых не соответствуют этой дате.
Наша задача — определить ближайшую дату, откат на которую даёт рабочую систему, выяснить какие пакеты обновились на следующий день, и выяснить, какой из этих пакетов вызвал поломку.
Приступим!
Если система обновлялась не очень давно, можно пошагово уменьшать дату в mirrorlist, каждый раз выполняя pacman -Syyuu и проверяя, не исчезла ли проблема. Плюсом такого подхода является то, что можно с высокой вероятностью сразу вычислить конкретный пакет и добавить его в /etc/pacman.conf в строчку IgnorePkg — до лучших времен.
Если с момента последнего обновления прошёл, допустим, месяц — то итеративно делим временной промежуток пополам. Сперва откатываемся на полмесяца назад. Если проблема исчезла — то обновляемся на четверть месяца вперед, если нет — то четверть месяца назад. И так далее. Этот способ позволяет определить точный момент сбоя, который произошёл в течение последнего года, всего за 9 смен даты.
Итак, допустим — мы не обновлялись уже неделю, pacman сообщает что есть 200 необновленных пакетов. После обновления система ломается.
Откатываемся назад на сутки, несколько пакетов "обновляются" до более ранней версии. Проверяем — система все ещё сломана.
Откатываемся ещё на сутки, ещё несколько пакетов снижают версию. Система сломана.
Ещё на сутки, очередные три пакета "обновляются" наоборот и ура, проблема исчезла!
Теперь мы точно знаем, что виноват один из этих трех пакетов. Допустим, это linux, linux-headers и gnome-shell. Так как linux-headers тянутся вслед за linux, их в расчёт не берем, это зависимость. Так что у нас всего два варианта.
Далее мы поодиночке добавляем кандидатов в /etc/pacman.conf.
Начнём с пакета linux:
После чего обновляем систему через pacman -Syu и смотрим, не исчезла ли проблема. Если она исчезла — то как раз потому, что пакет-виновник записан в pacman.conf как запрещенный к обновлению. Если не исчезла — снова откатывается на рабочую систему, вписываем в IgnorePkg следующего кандидата и повторяем цикл.
А чей, собственно говоря, баг? Данная статья посвящена проблемам на уровне всего дистрибутива. Нет никакого смысла откатываться на старые версии пакетов, если проблема не вызвана их обновлениями. Поэтому первое, что делаем — это гуглим ошибку всеми возможными способами, настроив выдачу только на свежие записи. Если проблема общая — то с огромной вероятностью вы наткнетесь на свежий тред на каком-нибудь официальном форуме дистрибутива, где узнаете все подробности — точную дату проблемного обновления, имя проблемного пакета, и даже технические причины (что, собственно, сломалось под капотом).
Возьмем пример выше. Допустим, мы выяснили, что наша система не грузится в графический режим после обновления пакета gnome-shell до версии 3.26.2
Что делать дальше?
Сперва делаем, чтобы система работала — откатываемся на дату на день раньше вредного обновления и добавляем gnome-shell в IgnorePkg (как вариант — добавить всю группу gnome в IgnoreGroup, чтобы гном не обновлялся частично).
Далее пытаемся найти в гугле ответ на вопрос — "это моя личная проблема или у других тоже так?" Если такой проблемы в интернете не обнаружилось — это может быть признаком того, что баг слишком свежий. Ждем для уверенности несколько дней, иногда повторяя поисковые запросы в разных сочетаниях. Если проблема в том, что проблемный пакет действительно пролез в репозитории — она обязательно вылезет в интернете! У арча огромная пользовательская база, и люди будут писать багрепорты и задавать вопросы на форумах.
Если в течение нескольких дней вы не видите никаких упоминаний о проблеме — у меня для вас плохие новости. Систему вы с вероятностью, близкой к 100%, сломали сами. Вспоминайте свои действия, анализируйте логи, разбирайтесь что произошло. Вообще, такой случай (когда какое-то обновление приводит к поломке, но это не баг) характерно для ситуаций, которые всегда освещаются в новостях в заголовком "при обновлении требуется ручное вмешательство". Такое случается, когда очередное обновление привносит настолько серьезные изменения в систему, что средствами самого обновления корректно это осуществить не удаётся.
Пользователь
=====
МЕТРО ИБ
2017-12-05, 19:57
Пользователь
=====
Шон Барретт: как я попал в видеоигровую индустрию
2017-12-07, 10:38
Переводчик-фрилансер
=====
8 учебных проектов
2017-12-05, 23:28
Пользователь
=====
Сверточная сеть на python. Часть 1. Определение основных параметров модели
2017-12-07, 21:12


























Пользователь
=====
Начало новой эры криптовалют? Биржи CME Group выпускают фьючерс на биткоин
2017-12-05, 23:54
Корпоративный аккаунт
=====
Взлом сайта с помощью другого сайта. Доверяй, но проверяй
2017-12-06, 01:36
Web-разработчик
=====
Как мы запустили роутер на Indiegogo или какие возможности есть на рынке связи
2017-12-06, 02:24
Pragmatic Dreamer
=====
Многопоточное программирование в Android с использованием RxJava 2
2017-12-07, 03:34
Android devepoler
=====
Взламываем собеседования: по алгоритмам, по архитектуре, поведенческие и прочее
2017-12-07, 23:30
Пользователь
=====
Service Desk — быстрый старт. 4 часть. Учет активов
2017-12-06, 08:00
Программист
=====
Pygest #19. Релизы, статьи, интересные проекты, пакеты и библиотеки из мира Python [20 ноября 2017 — 5 декабря 2017]
2017-12-06, 12:30
 Всем привет! Это уже девятнадцатый выпуск дайджеста на Хабрахабр о новостях из мира Python. 

Присылайте свои интересные события из мира Python. Вместе мы сделаем Python еще лучше:) 

Итак, поехали!
Пользователь
=====
Реальные истории из жизни одного хостинга, или воспоминания динозавров FirstVDS
2017-12-06, 10:53
Маркетолог
=====
Selenium Manager: история одного интерфейса
2017-12-06, 21:16
Lead Automation QA Engineer at Badoo
=====
7 лет работы, 2 млн отрезов и 150 километров чеков: еще немного о надежности железа в современных магазинах
2017-12-06, 12:28
Бизнес-решения для ритейла
=====
Готовимся к публикации игры в Xiaomi Mi Game Center (Unity, C#)
2017-12-07, 11:11
Unity Developer
=====
Свежие утилиты, плагины и инструменты для продуктивности дизайнера. Выпуск первый
2017-12-07, 09:19
Готовые дизайн системы в Figma → setproduct.com
=====
Как перенести электронную почту с cPanel на Zimbra Collaboration 8.0
2017-12-06, 11:05
Глава представительства Zextras в России и СНГ
=====
Готовим свой UI-интерфейс к Zabbix API средствами React component
2017-12-06, 11:20
Пользователь
=====
Английские фразы, которые не стоит переводить буквально
2017-12-06, 13:25
Разработчик
=====
Материалы открытого курса OpenDataScience и Mail.Ru Group по машинному обучению и новый запуск
2017-12-20, 15:00
Недавно OpenDataScience и Mail.Ru Group провели открытый курс машинного обучения. В прошлом анонсе много сказано о курсе. В этой статье мы поделимся материалами курса, а также объявим новый запуск. 

UPD: теперь курс — на английском языке под брендом mlcourse.ai со статьями на Medium, а материалами — на Kaggle (Dataset) и на GitHub.
Кому не терпится: новый запуск курса — 1 февраля, регистрация не нужна, но чтоб мы вас запомнили и отдельно пригласили, заполните форму. Курс состоит из серии статей на Хабре (Первичный анализ данных с Pandas — первая из них), дополняющих их лекций на YouTube-канале, воспроизводимых материалов (Jupyter notebooks в github-репозитории курса), домашних заданий, соревнований Kaggle Inclass, тьюториалов и индивидуальных проектов по анализу данных. Главные новости будут в группе ВКонтакте, а жизнь во время курса будет теплиться в Slack OpenDataScience (вступить) в канале #mlcourse_ai.
Часто вам будут говорить, что от вас ничего не требуется, через пару месяцев вы станете экспертом анализа данных. Я все еще помню фразу Andrew Ng из его базового курса "Machine Learning": «вы не обязаны знать, что такое производная, и сейчас вы разберетесь, как работают алгоритмы оптимизации в машинном обучении». Или «вы уже почти что эксперт анализа данных» и т.д. При всем безмерном уважении к профессору — это жесткий маркетинг и желтуха. Вы не разберетесь в оптимизации без знания производных, основ матана и линейной алгебры! Скорее всего вы не станете даже Middle Data Scientist, пройдя пару курсов (включая наш). Легко не будет, и больше половины из вас отвалится примерно на 3-4 неделе. Если вы wannabe, но не готовы с головой погрузиться в математику и программирование, видеть красоту машинного обучения в формулах и добиваться результатов, печатая десятки и сотни строк кода — вам не сюда. Но надеемся, вам все же сюда. 
В связи с вышесказанным мы указываем порог вхождения — знание высшей математики на базовом (но не плохом) уровне и владение основами Python. Как подготовиться, если этого у вас пока нет, подробно описано в группе ВКонтакте и тут под спойлером, чуть ниже. В принципе можно пройти курс и без математики, но тогда см. следующую картинку. Конечно, насколько дата саентисту нужно знать математику — это холивар, но мы тут на стороне Андрея Карпатого, Yes you should understand backprop. Ну и вообще без математики в Data Science — это почти как сортировать пузырьком: задачу, может, и решишь, но можно лучше, быстрее и умнее. Ну и без математики, конечно, не добраться до state-of-the-art, а за ним следить очень увлекательно. 
Математика
Python
Курсов по машинному обучению полно, есть действительно классные (как специализация «Машинное обучение и анализ данных»), но многие сваливаются в одну из крайностей: либо слишком много теории (PhD guy), либо, наоборот, практика без понимания основ (data monkey).
Мы ищем оптимальное соотношение: у нас много теории в статьях на Хабре (показательна 4-я статья про линейные модели), мы пытаемся ее преподнести максимально понятно, на лекциях излагаем еще более популярно. Но и практики море — домашние задания, 4 соревнования Kaggle, проекты… и это еще не все. 
Чего не хватает в большинстве курсов — так это живого общения. Новичкам порой нужен всего один короткий совет, чтобы сдвинуться с места и сэкономить часы, а то и десятки часов. Форумы Coursera обычно к какому-то моменту вымирают. Уникальность нашего курса — активное общение и атмосфера взаимоподдержки. В Slack OpenDataScience при прохождении курса помогут с любым вопросом, чат живет и процветает, возникает свой юмор, кто-то кого-то троллит… Ну а главное, что авторы домашних заданий и статей — там же в чате — всегда готовы помочь. 

Из паблика ВКонтакте «Мемы про машинное обучение для взрослых мужиков».
Соревнования Kaggle — отличный способ быстро прокачаться в практике анализ данных. Обычно в них начинают участвовать после прохождения базового курса машинного обучения (как правило, курса Andrew Ng, автор, безусловно, харизматичен и прекрасно рассказывает, но курс уже сильно устарел). У нас в течение курса будет предложено поучаствовать аж в 4 соревнованиях, 2 из них — часть домашнего задания, надо просто добиться определенного результата от модели, а 2 других — уже полноценные соревнования, где надо покреативить (придумать признаки, выбрать модели) и обогнать своих товарищей.
Ну тоже немаловажный фактор, чего уж там. Сейчас на волне распространения машинного обучения вы встретите немало курсов, предлагающих обучить вас за весьма кругленькую компенсацию. А тут все бесплатно и, без ложной скромности, на очень достойном уровне. 
Здесь мы вкратце опишем 10 тем курса, чему они посвящены, почему без них не может обойтись курс базового машинного обучения, и что нового мы внесли. 
Тема 1. Первичный анализ данных с Pandas. Статья на Хабре
 Хочется сразу начать с машинного обучения, увидеть математику в действии. Но 70-80 % времени работы над реальным проектом — это возня с данными, и тут Pandas очень хорош, я его использую в работе практически каждый день. В статье описываются основные методы Pandas для первичного анализа данных. Затем мы анализируем набор данных по оттоку клиентов телеком-оператора и пытаемся «прогнозировать» отток без всякого обучения, просто опираясь на здравый смысл. Недооценивать такой подход ни в коем случае нельзя. 
Тема 2. Визуальный анализ данных c Python. Статья на Хабре
 Роль визуального анализ данных сложно переоценить — так создаются новые признаки, ищутся закономерности и инсайты в данных. К.В. Воронцов приводит пример, как именно благодаря визуализации догадались, что при бустинге классы продолжают «раздвигаться» по мере добавления деревьев, и потом уже этот факт был доказан теоретически. В лекции мы рассмотрим основные типы картинок, которые обычно строят для анализа признаков. Также обсудим то, как вообще подглядеть в многомерное пространство — c помощью алгоритма t-SNE, который порой помогает рисовать вот такие елочные игрушки. 
Тема 3. Классификация, деревья решений и метод ближайших соседей.
Статья на Хабре
 Тут мы начнем говорить про машинное обучение и про два простых подхода к решению задачи классификации. Опять же, в реальном проекте надо начинать с самых простых подходов, и именно деревья решений и метод ближайших соседей (а также линейные модели, следующая тема) стоит попробовать в первую очередь после эвристик. Затронем важный вопрос оценки качества моделей и кросс-валидацию. Подробно обсудим плюсы и минусы деревьев и метода ближайших соседей. Статья длинная, но в особенности деревья решений заслуживают внимания — именно на их основе выстроены случайный лес и бустинг — алгоритмы, которые вы наверное будете больше всего использовать на практике. 
Тема 4. Линейные модели классификации и регрессии.
Статья на Хабре
 Эта статья уже будет размером с небольшую брошюру и недаром: линейные модели — самый широко используемый на практике подход к прогнозированию. Эта статья — как наш курс в миниатюре: много теории, много практики. Мы обсудим, каковы теоретические предпосылки метода наименьших квадратов и логистической регрессии, а также в чем плюсы практического применения линейных моделей. Отметим при этом, что излишнего теоретизирования не будет, подход к линейным моделям в машинном обучении отличается от статистического и эконометрического. На практике мы применим логистическую регрессию уже ко вполне реальной задаче идентификации пользователя по последовательности посещенных сайтов. После четвертого домашнего задания отсеется много народу, но если вы его все-таки сделаете, то будете иметь уже очень неплохое представление о том, какие алгоритмы используются в production-системах. 
Тема 5. Композиции: бэггинг, случайный лес. Статья на Хабре
 Тут опять и теория интересная, и практика. Мы обсудим то, почему для моделей машинного обучения работает «мудрость толпы», и много моделей работают лучше, чем одна, даже лучшая. А на практике покуртим случайный лес (композицию многих деревьев решений) — то, что стоит попробовать, если вы не знаете, какой алгоритм выбрать. Подробно обсудим многочисленные плюсы случайного леса и области его применения. И, как всегда, не без недостатков: все же есть ситуации, когда линейные модели будут работать лучше и быстрее.
Тема 6. Построение и отбор признаков. Приложения в задачах обработки текста, изображений и геоданных. Статья на Хабре, лекция про регрессию и регуляризацию. 
 Тут план статей и лекций немного расходится (всего один раз), уж слишком велика четвертая тема линейных моделей. В статье описаны главные подходы к извлечению, преобразованию и построению признаков для моделей машинного обучения. Вообще это занятие, построение признаков, — наиболее творческая часть работы Data Scientist-а. И конечно, важно знать, как работать с различными данными (текстами, картинками, геоданными), а не просто с готовым датафреймом Pandas.
На лекции опять обсудим линейные модели, а также основную технику настройки сложности ML-моделей — регуляризацию. В книге "Deep Learning" даже ссылаются на одного известного товарища (лень лезть за пруф-линком), который утверждает, что вообще «все машинное обучение — суть регуляризация». Это, конечно, преувеличение, но на практике, чтобы модели хорошо работали, их надо настраивать, то есть именно правильно использовать регуляризацию. 
Тема 7. Обучение без учителя: PCA, кластеризация. Статья на Хабре
 Тут мы переходим к обширной теме обучения без учителя — это когда есть данные, а вот целевого признака, который хотелось бы прогнозировать — вот его нет. Таких неразмеченных данных пруд пруди, и надо уметь и из них извлекать пользу. Мы обсудим только 2 типа задач — кластеризацию и снижение размерности. В домашнем задании вы будете анализировать данные с акселерометров и гироскопов мобильных телефонов и пытаться по ним кластеризовать носителей телефонов, выделять типы активностей.
Тема 8. Обучение на гигабайтах c Vowpal Wabbit. Статья на Хабре
 Теория тут — это разбор стохастического градиентного спуска, именно этот метод оптимизации позволил успешно обучать и нейронные сети, и линейные модели на больших обучающих выборках. Тут мы также обсудим, что делать, когда признаков становится уж слишком много (трюк с хэшированием значений признаков) и перейдем к Vowpal Wabbit — утилитке, с помощью которой можно за считанные минуты обучить модель на гигабайтах данных, да порой еще и приемлемого качества. Рассмотрим много приложений в различных задачах — классификации коротких текстов, а также категоризации вопросов на StackOverflow. Пока перевод именно этой статьи (в виде Kaggle Kernel) служит примером того, как мы будем подавать материал на английском на Medium.
Тема 9. Анализ временных рядов с помощью Python. Статья на Хабре
 Тут обсудим различные методы работы с временными рядами: какие этапы подготовки данных необходимы для моделей, как получать краткосрочные и долгосрочные прогнозы. Пройдемся по различным типам моделей, начиная от простых скользящих средних и заканчивая градиентным бустингом. Также посмотрим на способы поиска аномалий во временных рядах и поговорим о достоинствах и недостатках этих способов.
Тема 10. Градиентный бустинг. Статья на Хабре
 Ну и куда без градиентного бустинга… это и Матрикснет (поисковая машина Яндекса), и Catboost — новое поколение бустинга в Яндексе, и поисковик Mail.Ru. Бустинг решает все три основные задачи обучения с учителем — классификации, регрессии и ранжирования. И вообще его хочется назвать лучшим алгоритмом, и это близко к правде, но лучших алгоритмов не бывает. Но если у вас не слишком много данных (влезает в оперативную память), не слишком много признаков (до нескольких тысяч), и признаки разнородные (категориальные, количественные, бинарные, и т.д.), то, как показывает опыт соревнований Kaggle, почти наверное лучше всего в вашей задаче себя проявит градиентный бустинг. Поэтому недаром появилось столько крутых реализаций — Xgboost, LightGBM, Catboost, H2O...
Опять же, мы не ограничимся мануалом «как тюнить иксжбуст», а подробно разберемся в теории бустинга, а затем рассмотрим его на практике, в лекции дойдем и до Catboost. Тут заданием будет побить бейзлайн в соревновании — это даст неплохое представление о методах, работающих во многих практических задачах. 
Курс стартует 5 февраля 2018 года. В течение курса будут:
Как подключиться к курсу?
Формальной регистрации не нужно. Просто делайте домашки, участвуйте в соревнованиях, и мы учтем вас в рейтинге. Тем не менее, заполните этот опрос, оставленный e-mail будет вашим ID во время курса, заодно напомним о старте ближе к делу. 
Площадки для обсуждения
Удачи! Напоследок хочу сказать, что все получится, главное — не бросайте! Вот это «не бросайте» вы сейчас пробежали взглядом и скорее всего даже не заметили. Но задумайтесь: именно это главное.
Machine Learning Scientist, NLP practitioner
=====
Расширяем функционал Ansible с помощью плагинов: часть 1
2017-12-07, 07:00
У себя в D2C мы активно используем Ansible. С его помощью мы создаем виртуальные машины у облачных провайдеров, устанавливаем программное обеспечение, а также управляем Docker-контейнерами с приложениями клиентов. В прошлой статье я рассказывал о том, как заставить Ansible работать быстрее, теперь расскажу о том, как расширить его функциональность.
Ansible – необычайно гибкий инструмент. Он написан на Python и в основном состоит из заменяемых «кубиков» – плагинов и модулей. Плагины влияют на ход работы Ansible на машине управления, модули – исполняются на удаленных хостах и возвращают на машину управления результат. Поэтому если функционала Ansible «из коробки» вам не хватает, достаточно написать свой плагин или модуль, а потом добавить его в систему. Дополнительным удобством является то, что плагины и модули не нужно никак специально устанавливать на машине управления и можно распространять прямо со своими плейбуками.
Рассмотрим пример:
В данном случае copy – это модуль. Он будет выполнен на целевой машине; shuffle – Jinja2 фильтр, загруженный плагином. Плагины в Ansible выполняют не только видимую, но и скрытую от глаз работу.
Важно: все плагины в Ansible выполняются в контексте локального хоста (т.е. машины управления). Одной из частых ошибок является попытка прочитать переменные окружения на целевом хосте с помощью lookup-плагина env:
В этом случае не важно на каком сервере выполняется задача, переменная USER будет равна значению, которое выставлено у процесса ansible на машине управления. Аналогично со всеми другими плагинами.
Перечислю виды плагинов в алфавитном порядке, для Ansible 2.3.x:
Action-плагины используются в качестве «обвязки» (wrappers) для модулей. Они выполняются непосредственно перед отправкой модулей на исполнение на целевые хосты. Обычно их используют для предварительной подготовки данных или для пост-обработки результатов выполнения модуля.
В общем виде выполнение задачи для mymodule выглядит так:
Если action-плагина mymodule не существует, используется базовый класс плагина.
Cache-плагины используются для организации хранилища (бэкендов) фактов. По умолчанию используется memory бэкенд, поэтому факты сохраняются только во время выполнения плейбука. Альтернативные бэкенды, доступные «из коробки»: jsonfile, memcached, pickle, redis, yaml.
Кэш фактов используется, если необходимо работать со множеством удалённых хостов и сбор фактов занимает продолжительное время. В такой ситуации можно по расписанию актуализировать кэш, а в самих плейбуках сбор фактов не выполнять или выполнять в редких случаях принудительной командой.
Callback-плагины предоставляют возможность реагировать на события, которые генерирует Ansible во время выполнения плейбука. Например, вывод журнала работы Ansible на экран делается callback-плагином default, который реагирует на множество событий и выводит на экран, что происходит. Можно включить callback-плагин slack и получать информацию о ходе выполнения плейбука в канал в Slack.
Connection-плагины предоставляют различные способы подключения к целевым хостам – например, ssh – для Unix, winrm – для Windows, docker – для запуска модулей внутри контейнеров. Самые распространённые – ssh (по умолчанию) и local, который используется для запуска команд локально на машине управления.
Filter-плагины добавляют новые Jinja2 фильтры. Так как для работы с переменными в Ansible используется «движок» шаблонизации Jinja2, то в плейбуках доступны почти все его возможности, в том числе встроенные и дополнительные фильтры. Если требуются нестандартные фильтры, их можно добавить своими плагинами.
Lookup-плагины используются для поиска или загрузки данных из внешних источников, а также для создания циклов.
Например, для загрузки значения из etcd можно использовать {{ lookup('etcd', 'foo') }}.
Чтобы сделать цикл по строчкам вывода команды, можно использовать плагин lines:
В этой задаче выполнится команда cat /etc/passwd (на локальном компьютере) и для каждой из строк вывода выплонится debug.
Для создания цикла можно использовать любой lookup-плагин в конструкции with_<plugin-name>:. Когда вы делаете самый примитивный цикл with_items, вызывается lookup-плагин items.
Список доступных плагинов удобнее всего смотреть в репозитории (обращайте внимание на версию – данная ссылка для ветки 2.3.х).
Shell-плагины позволяют учитывать нюансы разного поведения оболочек на целевых устройствах. Например, bash или csh. Для Windows используется плагин powershell.
Strategy-плагины определяют ход выполнения задач на целевых хостах. «Из коробки» доступны три плагина:
Terminal-плагины позволяют учитывать разновидности интерактивных сред. Данные плагины используются для сетевых устройств типа коммутаторов и роутеров, так как работа с оболочкой на этих устройствах значительно отличается от работы полноценного shell'а на компьютере.
Test-плагины добавляют Jinja2 тесты, которые используются в условных конструкциях. Аналогично фильтрам есть встроенные и дополнительные тесты.
Vars-плагины используются для манипуляции с переменными хостов (host vars, group vars) – встречаются крайне редко.
Приведу несколько примеров плагинов в порядке возрастания сложности.
Например, мы часто работаем со спискам серверов EC2 и необходимо выбирать из списка инстансов те, которые работают. Можно использовать выражение:
{{ ec2.instances | selectattr('state','equalto','running') | list }}
Или написать свой test-плагин (положить в ./test_plugins/ec2.py):
И уже использовать:
{{ ec2.instances | select('ec2_running') | list }}
Очевидно, что я немного упростил пример, и в случае, если нам нужно проверять несколько статусов одновременно, то пришлось бы делать цепочку из множества selectattr. В своём же тесте мы можем описать любую логику и при этом код плейбука держать лаконичным и хорошо читаемым.
Аналогично можно использовать свои тесты внутри when:
Задача будет выполнена, если my_instance в состоянии running.
Можно создавать тесты с параметром. Пример — стандартный тест divisibleby, который проверяет делится ли чисто на какое-то другое.
Фильтры используются для модификации переменных. Например, в Ansible очень долго не было механизмов работы с датой. Если вам нужно в плейбуках принимать решения на основе значений времени, вы можете использовать такой фильтр (положить в ./filter_plugins/add_date.py):
Теперь в плейбуках можно «заглядывать» в будущее:
Action-плагины удобно использовать, когда нужно немного модифицировать данные поступающие в модули или из него, или когда нужно выполнять какую-то задачу всегда локально на сервере управления. Пример — модуль debug для вывода информации, на самом деле не модуль так как никогда не копируется на удаленный хост, а существует лишь в виде action-плагина.
Чтобы показать, как работают action-плагины, модифицируем поведение модуля setup, который используется для сбора фактов. Его удобно использовать в качестве ad-hoc команды, чтобы посмотреть информацию о серверах:
ansible all -i myinventory -m setup
У этого модуля есть параметр filter, которым можно отфильтровать результат. Но у него есть одна особенность – он применяется только к ключам верхнего уровня. Если нам нужно проверить на серверах только временную зону, мы не можем указать tz. Или если нам нужно увидеть все ipv4 адреса мы не можем сделать фильтр по всем таким полям.
Добавим обертку в виде action-плагина (положить в ./action_plugins/setup.py):
Минимально необходимая реализация плагина – наследование от ActionBase и описание метода run.
В нашем примере мы:
Теперь мы добавили новый функционал в существующий модуль и при этом не трогали его код. С выходом новых версий Ansible модуль setup может научиться делать что-то ещё, но наш плагин по прежнему будет работать поверх этих возможностей.
Callback-плагины используются для того, чтобы следить за событиями, которые происходят внутри Ansible во время выполнения плейбука и как-то реагировать на них. Одним из самых частых использований таких плагинов являются протоколирование, логгирование и оповещение.
Список callback-плагинов, доступных «из коробки», можно посмотреть в репозитории.
Для уведомлений, к примеру, доступны: mail, slack, hipchat.
Для модификации протоколирования, например: minimal, json. Для задания стандартного плагина вывода можно использовать настройку:
Теперь Ansible не будет выводить человеко-читабельный протокол по ходу выполнения, а в самом конце выполнения плейбука выдаст огромный JSON со всей информацией. Его можно использовать для автоматического анализа результата в cron-задачах или на вашем сервере CI/CD.
Например, можно запустить плейбук и посчитать количество хостов, в которых были изменения:
ANSIBLE_STDOUT_CALLBACK=json ansible-playbook myplaybook.yml | jq '.stats | map(select(.changed > 0)) | length'
В качестве примера callback-плагина приведу «оповещалку» о выполнении плейбука, которая выводит уведомление в вашей графической оболочке по результатам выполнения плейбука (положить в ./callback_plugins/notify_me.py):
В плагине предусмотрена некая попытка кросс-плаформенности :)
Мы наследуемся от класса CallbackBase и переопределяем метод v2_playbook_on_stats, который вызывается в момент готовности финального отчета о выполнении плейбука. Стандартный плагин протоколирования по этому методу формирует таблицу PLAY RECAP.
Нам понадобится вспомогательная функция notify, которая в зависимости от платформы пытается отправить пользователю оповещение.
В основном теле нашего метода мы проверяем есть ли хосты с ошибками: если есть – отправляем плохую нотификацию со списком хостов, если нет – отправляем хорошую нотификацию Job's done!.
Обратите внимание на CALLBACK_NEEDS_WHITELIST = True. Этот параметр говорит Ansible, что данный плагин требует принудительного включения. То есть, не смотря на готовность плагина к работе, он будет включен лишь при добавлении его в whitelist. Это сделано, чтобы при работе с плейбуками экран не замусоривался, но можно было легко поставить такое уведомление для «долгоиграющих» плейбуков, которые вы запускаете в фоне и идете заниматься другим делом. Проверить работу можно так:
ANSIBLE_CALLBACK_WHITELIST=notify_me ansible-playbook test.yml
Полный список методов(событий), которые можно переопределять в callback-плагинах, лучше всего посмотреть в исходном коде.
--
Можете смело экспериментировать с примерами из статьи. Ещё несколько видов плагинов рассмотрим в следующей части. Stay tuned!
Ansible ninja
=====
Что такое виртуализация и как работает виртуальный сервер
2017-12-06, 13:45
Пользователь
=====
Что нового в WebStorm 2017.3
2017-12-06, 13:52
User
=====
Короткое плечо совпадения
2017-12-08, 10:01
















Переводчик-фрилансер
=====
Практическое использование multiple bounds generic в Java
2017-12-06, 14:45
Не сильно ошибусь, если предположу, что мало кто активно использует эту возможность языка. Для тех кто не помнит, что это такое можно почитать здесь. Я же переду к практике.
Наткнулся на проблему: надо послать уже существующее событие (GWT) по нажатию кнопки, но перед посылкой поставить атрибут (Command). Казалась бы, при чём здесь шаблоны…
А вот при чём:
Сам метод для создания кнопки тривиален, command это поле класса:
Проблема в строчке:
Такого метода у объекта Event нет. Решение, вроде, очевидное: унаследовать наше события от промежуточного класса CommandEvent, у которого будет этот метод и который унаследован от Event. Наш метод выглядит теперь так:
Эврика? Ха! Тут мы обнаруживаем, что одно из наших событий уже унаследовано от другого дочернего класса (e.g. GwtEvent) и никак не может наследовать наш класс CommandEvent.
Следующий шаг — создаём интерфейс ICommandEvent c методом setCommand() и каждое наше событие реализует его. Наш метод выглядит теперь так:
Ну некрасиво! К тому же в него можно передать любое событие, а обнаружится это только при запуске, что нехорошо.
И тут пора вспомнить о теме этой заметки — multiple bounds generic в Java. С ними наш метод выглядит так:
Ровно то, что и требовалось.
Человек
=====
Восстановление таблиц Хаффмана в Intel ME 11.x
2017-12-06, 14:50
Пользователь
=====
Mac OS High Sierra, есть ли удалённый «passwordless root»? (Да, но маловероятно)
2017-12-06, 15:13
User
=====
Сказ о том, как SQL время экономит
2017-12-25, 08:59
Автор
=====
Эволюция твердотельных накопителей: от первых моделей 70-х до наших дней
2018-02-06, 00:05
Пользователь
=====
GDPR — новые правила обработки персональных данных в Европе для международного IT-рынка
2017-12-06, 17:40
В мае 2018 года Европа переключится на обновлённые правила обработки персональных данных, установленные Общим регламентом по защите данных (Регламент ЕС 2016/679 от 27 апреля 2016 г. или GDPR — General Data Protection Regulation). Данный регламент, имеющий прямое действие во всех 28 странах ЕС, заменит рамочную Директиву о защите персональных данных 95/46/ЕС от 24 октября 1995 года. Важным нюансом GDPR является экстерриториальный принцип действия новых европейских правил обработки персональных данных, поэтому российским компаниям следует внимательно отнестись к ним, если услуги ориентированы на европейский или международный рынок. 
Новый регламент предоставляет резидентам ЕС инструменты для полного контроля над своими персональными данными. С мая 2018 года ужесточается ответственность за нарушение правил обработки персональных данных: по GDPR штрафы достигают 20 миллионов евро (около 1,5 млрд руб.) или 4% годового глобального дохода компании. В настоящей статье мы проанализировали новые правила обработки персональных данных в ЕС и сформулировали рекомендации для российских компаний по методам реагирования на GDPR. 
GDPR имеет экстерриториальное действие и применяется ко всем компаниям, обрабатывающим персональные данные резидентов и граждан ЕС, независимо от местонахождения такой компании. 
Разумеется, филиалы, представительства российских организаций на территории ЕС должны будут соответствовать новым требованиям.
Другую (неочевидную) категорию субъектов рассмотрим на следующем примере:
Организация базируется в России. Она продает онлайн товары и услуги пользователям, в том числе пользователям из ЕС. Услуги предоставляются пользователям на локальных языках в местных валютах на национальных доменах верхнего уровня стран ЕС (напр., «.de», «.nl» или «.co.uk»). При этом эта организация не производит никаких операций или субподрядчиков непосредственно на территории ЕС. 
Должна ли такая организация соблюдать GDPR?
Да. 
Ведь услуги и товары очевидно предлагаются жителям ЕС, потому что: 
Это означает, что организации, обрабатывающие персональные данные европейцев в России при реализации онлайн-продаж (например, РЖД, авиакомпании, гостиницы, хостелы и иные), подпадают под действие GDPR и обязаны соблюдать новые европейские правила обработки персональных данных. 
Важно отметить, что помимо обработки персональных данных в GDPR используется понятие мониторинга поведения субъектов данных, которое загоняет под действие GDPR ещё одну категорию субъектов. GDPR применяется к организациям, созданным за пределами ЕС, если они (в качестве контролера или процессора) контролируют поведение жителей ЕС (в той мере, в которой такое поведение имеет место в ЕС). 
Мониторинг может включать: 
Европейский законодатель также разделяет понятия контроллер данных (data controller) и процессор данных (data processor). Контроллер, действуя в качестве капитана судна, несет бОльшую юридическую ответственность, чем процессор, который действует в качестве моряка на судне. По сути контроллеры решают, что происходит с персональными данными и несут ответственность за обработку, а процессоры являются некими “исполнителями”. 
Например, облачная система, которой пользуются ваши сотрудники для целей выполнения задач и проектов, где также хранятся персональные данные клиентов, будет являться процессором данных, а вы, соответственно, контроллером.
Персональные данные — это любая информация, относящаяся к идентифицированному или идентифицируемому физическому лицу (субъект данных), по которой прямо или косвенно можно его определить. К такой информации относится в том числе имя, данные о местоположении, онлайн идентификатор или один или несколько факторов характерных для физической, физиологической, генетической, умственной, экономической, культурной или социальной идентичности этого физического лица (п. 1 ст. 4). Определение широкое и достаточно четко дает понять, что даже IP адреса также могут быть персональными данными. 
Важно отметить, что существуют определенные типы персональных данных, относящиеся к категории особых или конфиденциальных персональных данных. Это информация, раскрывающая: расовое или этническое происхождение, политические взгляды, религиозные или философские убеждения и членство в профсоюзах. Кроме того, к этой группе относятся генетические, биометрические данные, используемые для идентификации физического лица, данные о состоянии здоровья, сведения, касающиеся сексуальной жизни или сексуальной ориентации (ст. 9). 
Общий подход европейцев к обработке персональных данных сформулирован в виде 6 основных принципов: 
1) Законность, справедливость и прозрачность. Персональные данные должны обрабатываться законно, справедливо и прозрачно. Любую информацию о целях, методах и объёмах обработки персональных данных следует излагать максимально доступно и просто.
2) Ограничение цели. Данные должны собираться и использоваться исключительно в тех целях, которые заявлены компанией (онлайн-сервисом).
3) Минимизация данных. Нельзя собирать личные данные в большем объёме, чем это необходимо для целей обработки.
4) Точность. Личные данные, которые являются неточными, должны быть удалены или исправлены (по требованию пользователя).
5) Ограничение хранения. Личные данные должны храниться в форме, которая позволяет идентифицировать субъекты данных на срок не более, чем это необходимо для целей обработки.
6) Целостность и конфиденциальность. При обработке данных пользователей компании обязаны обеспечить защиту персональных данных от несанкционированной или незаконной обработки, уничтожения и повреждения. 
Уведомление о случаях нарушения GDPR 
Компании обязаны уведомлять регулирующие органы (а в некоторых случаях и субъектов данных) о любых нарушениях, связанных с персональными данными в течение 72 часов после обнаружения такого нарушения. 
Например, недавняя новость о хакерской атаке на Uber — яркий пример нарушения данного правила. Uber сообщил прессе, что хакеры получили доступ к персональным данным 57 миллионов пользователей и водителей спустя целый год. Если бы сейчас действовал GDPR, то избежать высокого штрафа в размере 4% от годового оборота было бы невозможно.
Список национальных регуляторов в области персональных данных по всем странам ЕС приведён тут. Также есть общеевропейский регулятор — Working party 29 или Рабочая группа по статье 29. Однако после вступления GDPR в силу Рабочую группу по статье 29 заменит новый орган — Европейский совет по защите данных (European Data Protection Board — EDPB).
Права субъекта данных (физического лица) 
GDPR значительно расширяет права граждан и резидентов ЕС по контролю за их персональными данными. Европейские пользователи имеют право запрашивать подтверждение факта обработки их данных, место и цель обработки, категории обрабатываемых персональных данных, каким третьим лицам персональные данные раскрываются, период, в течение которого данные будут обрабатываться, а также уточнять источник получения организацией персональных данных и требовать их исправления. Более того, пользователь имеет право требовать прекращения обработки своих данных. 
В GDPR также предусмотрено право на забвение (right to erasure, right to be forgotten), которое дает европейцам возможность удалять свои личные данные по запросу во избежание их распространения или передачи третьим лицам. 
Это не новое право, оно также есть в действующей Директиве. Суд справедливости ЕС (CJEU — Court of Justice of the European Union) в решении по делу Google Spain в 2014 году разъяснил, что субъекты данных имеют право на удаление информации о них из результатов поиска, если она не представляет общественного интереса. Однако, право на забвение распространяется не только на поисковые системы. Любая компания, обрабатывающая данные, должна удалять чьи-либо персональные данные по запросу, если это не противоречит интересам общества или иным фундаментальным правам европейцев. 
Например, если вы новостной сервис, то прежде, чем удалять данные, проверьте и убедитесь, что такое удаление не повлияет на свободу слова и на право к доступу информации, гарантированные европейцам статьей 11 Хартии Европейского союза по правам человека.
Право на переносимость данных
Право на переносимость данных (right to data portability) является новацией в правилах обработки данных ЕС, введенное GDPR. Данное право заключается в том, что компании обязаны предоставлять бесплатно электронную копию персональных данных другой компании по требованию самого субъекта персональных данных. 
Например, стартап под названием “Солнышко” хочет выйти на рынок с сайтом для обмена социальными медиа, но на рынке уже есть свои гиганты с большой долей рынка. Право на переносимость данных упростит потенциальным клиентам процесс передачи своих данных от одного онлайн-сервиса к другому (без повторного введения одних и тех же данных на разных сайтах). 

Другой пример. Субъект данных пользуется сервисом чтения электронных книг “Электронная книжка”. В один прекрасный момент пользователь решает перейти на сервис “Читай онлайн”. В данном случае право на переносимость данных позволяет получить от “Электронной книжки” персональные данные (например, предпочтения в литературе и другие) и передать их другому сервису.
Согласие на обработку 
GDPR устанавливает высокие требования в отношении формы получения согласия на обработку данных. Согласие человека на обработку его персональных данных должно быть выражено в форме утверждения или в форме четких активных действий пользователя. Согласие на обработку персональных данных будет недействительно, если у пользователя не было выбора или не было возможности отозвать свое согласие без ущерба для самого себя. Если пользователь дал согласие на обработку своих персональных данных, контроллер должен иметь возможность продемонстрировать это. 
Не рекомендуем использовать по умолчанию поля о согласии с уже поставленной галочкой или другие методы получения согласия по умолчанию. Согласие также не может быть выражено в виде молчания или бездействия пользователя. Информация о порядке отзыва согласия на обработку персональных данных должна быть размещена таким образом, чтобы пользователь мог легко её найти.
Особая защита детей 
Детские персональные данные заслуживают особой защиты, ведь они менее осведомлены о рисках, последствиях, гарантиях и их правах в отношении обработки персональных данных. Согласие на обработку данных ребенка должно быть авторизовано родителями (или законными представителями ребенка). Возрастной порог для родительской авторизации устанавливается государствами-членами ЕС отдельно (от 13 до 16 лет).
Назначение ответственного за защиту персональных данных 
Это требование относится к компаниям, которые осуществляют регулярные и систематические крупномасштабные наблюдения, мониторинг лиц (выше о нем упоминалось); или которые осуществляют крупномасштабную обработку специальных персональных данных, например, медицинские записи или сведения об уголовной судимости.
В любом случае, любая организация может добровольно назначить сотрудника по защите данных для управления процессами обработки данных пользователей и контроля за соблюдением требований GDPR. В таком случае компания должна опубликовать информацию о таком сотруднике, а также направить её национальному регулятору по защите персональных данных соответствующей страны ЕС.
Если вы входите в зону действия нового европейского регламента о защите данных или планируете расширяться и предоставлять услуги и товары в страны ЕС, то рекомендуется провести комплексную оценку применяемых в компании методов и средств обработки персональных данных и привести их в соответствие с новыми правилами GDPR. Следует также пересмотреть политику конфиденциальности и положения об обработке персональных данных пользовательских соглашений (Terms of use) своих сайтов и онлайн-сервисов, ориентированных на европейских потребителей и пользователей. Для соответствия требованиям GDPR необходимо разработать внутренние политики защиты данных, обучать персонал, проводить проверки деятельности по обработке данных, вести документацию по процессам обработки, внедрять меры по встроенной системе конфиденциальности, а также назначить сотрудника ответственного за обработку персональных данных (естественно, с учётом характера и объёмов обрабатываемых персональных данных).
Несмотря на то, что новые требования к обработке персональных данных серьезны, в них есть положительные стороны для внеевропейских игроков: легче придерживаться единого набора правил защиты и обработки данных, чем учитывать национальные нюансы обработки персональных данных каждой отдельной страны ЕС, как это приходилось делать до введения GDPR. Более того, реформа направлена на стимулирование экономического роста путем сокращения расходов и бюрократии для компаний, работающих в ЕС. Соблюдение одного правила вместо 28 (количество стран-членов ЕС) поможет маленьким и развивающимся компаниям выйти на новые рынки. Согласно закону в ряде случаев обязательства изменяются в зависимости от размера бизнеса, природы обрабатываемых данных и иных факторов.
Также следует заранее продумать механизмы реагирования на запросы европейских регуляторов и субъектов персональных данных (пользователей), которые возможны в рамках GDPR (например, об уточнении данных, их удалении, прекращении обработки или передаче другой компании по праву на переносимость данных). 
GDPR — важнейший законодательный документ, который существенно повышает уровень защиты персональных данных в ЕС и за его пределами. Он требует очень внимательного изучения и соблюдения. Реформа дает ясность и последовательность правил, которые должны применяться в области защиты данных. Она также восстанавливает доверие пользователя-потребителя, что позволяет бизнесу максимально использовать возможности на едином европейском цифровом рынке. Сбор, анализ и перемещение персональных данных по всему миру приобрели огромное экономическое значение. Персональные данные – это, безусловно, “валюта” современной экономики. И если вы осуществляете сбор пользовательских данных в каком-либо виде — за их сохранностью надо внимательно следить, чтобы избежать утечек и возможных манипуляций ими третьими лицами.

Интернет-активист, лидер «РосКомСвободы»
=====
[Перевод статьи] 7 базовых правил защиты от фишинга
2017-12-06, 17:03
О том, что такое фишинг известно давно. Первые фишинговые атаки были зафиксированы вскоре после появления всемирной паутины. Но несмотря на то, что специалисты по ИБ создают все более совершенные способы защиты от фишинга, новые фишинговые сайты продолжают появляться ежедневно.
Согласно данным некоторых исследований, в 2016 году ежедневно создавалось около 5000 фишинг сайтов. В 2017 эта цифра будет еще больше. Секрет жизнестойкости этого вида мошенничества в том, что он опирается не на “дыры” в программном обеспечении, а на уязвимость в человеческой сущности, у которой есть доступ к важным данным. Поэтому нелишним будет в очередной раз напомнить, что такое фишинг, каковы самые распространенные виды фишинговых атак, а также способы противодействия им.
Фишинг — это вид интернет-мошенничества, построенный на принципах социальной инженерии. Главная цель фишинга — получить доступ к критически важным данным (например, паспортным), учетным записям, банковским реквизитам, закрытой служебной информации, чтобы использовать их в дальнейшем для кражи денежных средств. Работает фишинг через перенаправление пользователей на поддельные сетевые ресурсы, являющиеся полной имитацией настоящих.
К этой категории можно отнести большую часть всех фишинговых атак. Злоумышленники рассылают электронные письма от имени реально существующей компании с целью завладеть учетными данными пользователей и получить контроль над их личными или служебными аккаунтами. Вы можете получить фишинговое письмо от имени платежной системы или банка, службы доставки, интернет-магазина, социальной сети, налоговой и т.д.
Фишинговые письма создают с большой скрупулезностью. Они практически ничем не отличаются от тех писем, которые пользователь регулярно получает в рассылках от настоящей компании. Единственное, что может насторожить — просьба перейти по ссылке для выполнения какого-либо действия. Переход этот, однако, ведет на сайт мошенников, являющийся “близнецом” страницы сайта банка, социальной сети или другого легального ресурса.
Побудительным мотивом для перехода по ссылке в подобных письмах может выступать как “пряник” (”Вы можете получить 70% скидку на услуги, если зарегистрируетесь в течение суток”), так и “кнут” (”Ваша учетная запись заблокирована в связи с подозрительной активностью. Чтобы подтвердить, что вы владелец аккаунта, перейдите по ссылке”).
Приведем список самых популярных уловок мошенников:
Тактика запугивания пользователя может быть очень эффективной. Угроза того, что аккаунт был или в ближайшее время будет заблокирован, если пользователь сейчас же не зайдет в учетную запись, заставляет тут же потерять бдительность, перейти по ссылке в письме и ввести свой логин и пароль.
В таком письме пользователя просят срочно войти в учетную запись и обновить настройки безопасности. Действует тот же принцип, что и в предыдущем пункте. Пользователь паникует и забывает о бдительности.
Чаще всего такие письма присылают от имени финансовых организаций. Пользователи склонны верить правдивости писем, поскольку финансовые организации действительно не пересылают конфиденциальную информацию по электронной почте.
Такие письма входят в тренд, как только близится время платить налоги. Темы писем могут быть самыми разными: уведомление о задолженности, просьба выслать недостающий документ, уведомление о праве на получение возврата налога, и т.д.
Не всегда фишинг бьет наудачу — часто атаки являются персонифицированными, целенаправленными. Цель та же — заставить пользователя перейти на фишинговый сайт и оставить свои учетные данные.
Естественно, у будущей жертвы больше доверия вызовет письмо, в котором к ней обращаются по имени, упоминают место работы, должность, занимаемую в компании, еще какие-либо индивидуальные данные. Причем информацию для направленных фишинговых атак люди чаще всего предоставляют сами. Особенно “урожайны” для преступников такие ресурсы как всем известная LinkedIn — создавая резюме в расчете на потенциальных работодателей, каждый старается указать побольше сведений о себе.
Для предупреждения подобных ситуаций организациям следует постоянно напоминать сотрудникам о нежелательности размещения личной и служебной информации в открытом доступе.
Особый интерес для мошенников представляют учетные данные руководства.
Как правило, специалисты по безопасности любой фирмы внедряют четкую систему допусков и уровней ответственности, в зависимости от должности работника. Так, менеджер по продажам имеет доступ к базе данных продукции, а список сотрудников компании для него — запретная зона. HR-специалист, в свою очередь, полностью в курсе, какие вакансии кем заняты, какие только что освободились, кто достоин повышения, но понятия не имеет о номерах и состоянии банковских счетов родной фирмы. Руководитель же обычно сосредотачивает в своих руках доступ ко всем критически важным узлам жизни предприятия или организации.
Получив доступ к учетной записи главы компании, специалисты по фишингу идут дальше и используют ее для коммуникации с другими отделами предприятия, например, одобряют мошеннические банковские переводы в финансовые учреждения по своему выбору.
Несмотря на высокий уровень допуска, менеджеры высшего звена не всегда участвуют в программах обучения персонала основам информационной безопасности. Вот почему, когда фишинговая атака направлена против них, это может привести к особенно тяжелым последствиям для компании.
Сравнительно недавно в фишинге появилось новое направление — охота за логинами и паролями для входа в облачные хранилища данных.
В облачном сервисе Dropbox и на Google Диске пользователи, как личные так и корпоративные, хранят множество конфиденциальной информации. Это презентации, таблицы и документы (служебные), резервные копии данных с локальных компьютеров, личные фотографии и пароли к другим сервисам.
Неудивительно, что получить доступ к учетным записям на этих ресурсах — заманчивая перспектива для злоумышленников. Для достижения этой цели используют стандартный подход. Создается фишинговый сайт, полностью имитирующий страницу входа в аккаунт на том или ином сервисе. Потенциальных жертв на него в большинстве случаев перенаправляет фишинговая ссылка в электронном письме.
Ссылка на подозрительный сайт с целью украсть данные пользователя — не самое страшное, на что способен фишинг. Ведь в этом случае преступники получат доступ лишь к определенной части конфиденциальной информации — логину, паролю, т.е к аккаунту в определенном сервисе. Гораздо хуже, когда фишинг-атака приводит к компрометации всего компьютера жертвы вредоносным программным обеспечением: вирусом-шифровальщиком, шпионом, трояном.
Такие вирусы могут содержаться во вложениях к письмам. Предполагая, что письмо пришло от доверенного источника, пользователи охотно скачивают такие файлы и заражают свои компьютеры, планшеты и лэптопы.
Классический фишинг со ссылками на сомнительные ресурсы постепенно становится менее эффективным. Опытные пользователи веб-сервисов обычно уже осведомлены об опасности, которую может нести ссылка на подозрительный сайт и проявляют осмотрительность, получив странное письмо или уведомление. Заманить жертву в свои сети становится все труднее.
В качестве ответа на снижение результативности традиционных атак злоумышленниками был придуман фарминг — скрытое перенаправление на мошеннические сайты.
Суть фарминга состоит в том, что на первом этапе в компьютер жертвы тем или иным образом внедряется троянская программа. Она зачастую не распознается антивирусами, ничем себя не проявляет и ждет своего часа. Вредонос активируется лишь тогда, когда пользователь самостоятельно, без всякого внешнего воздействия, решает зайти на интересующую преступников страницу в интернете. Чаще всего это сервисы онлайн-банкинга, платежные системы и прочие ресурсы, осуществляющие денежные транзакции. Здесь-то и происходит процесс подмены: вместо проверенного, часто посещаемого сайта хозяин зараженного компьютера попадает на фишинговый, где, ничего не подозревая, указывает нужные хакерам данные. Делается это с помощью изменения кэша DNS на локальном компьютере или сетевом оборудовании. Такой вид мошенничества особенно опасен из-за трудности его обнаружения.
Полностью уничтожить фишинг в обозримом будущем вряд ли получится: человеческая лень, доверчивость и жадность тому виной.
Ежедневно происходят тысячи фишинговых атак, которые могут принимать самые разнообразные формы:
Только наличие своевременной и наиболее полной информации о методах хакеров, а также здоровая подозрительность по отношению к необычным, неожиданным сообщениям и предложениям, позволят существенно снизить ущерб от этого вида интернет-мошенничества.
Потому обязательно ознакомьтесь с правилами защиты от фишинга. И прежде всего никому не передавайте свои пароли, заведите привычку всегда вбивать адреса нужных сайтов вручную или пользоваться закладками в браузере, будьте особенно внимательны к ссылкам в письмах.
Источник — блог компании Protectimus Solutions LLP
Пользователь
=====
Инструкция по применению сисадмина в малом бизнесе
2017-12-06, 16:41
Пользователь
=====
Как в банках: государство застрахует биржевые инвестиции граждан на сумму до 1,4 млн рублей
2017-12-06, 16:59
Пользователь
=====
Релиз CLion 2017.3: существенные улучшения поддержки C++, интеграция с Valgrind Memcheck и Boost.Test и многое другое
2017-12-06, 17:55
Разработчик и ПММ
=====
Отладка злого бага в рантайме Go
2017-12-07, 15:22
Я большой поклонник Prometheus и Grafana. Поработав SRE в Google, я научился ценить хороший мониторинг и за прошедший год предпочитал пользоваться комбинацией этих инструментов. Я использую их для мониторинга своих личных серверов (black-box и white-box мониторинг), внешних и внутренних событий Euskal Encounter, для мониторинга клиентских проектов и много другого. Prometheus позволяет очень просто писать кастомные модули экспорта для мониторинга моих собственных данных, к тому же вполне можно найти подходящий модуль прямо из коробки. Например, для создания симпатичной панели имеющихся метрик Encounter-событий мы используем sql_exporter.

Панель событий для Euskal Encounter (фальшивые данные стейджинга)
Можно легко закинуть node_exporter на любую машину и натравить экземпляр Prometheus, чтобы получить основные системные метрики (использование процессора, памяти, сети, дисков, файловой системы и т. д.). Я подумал, а почему бы таким образом не мониторить свой ноутбук? У меня «игровой» ноут Clevo, который я использую в качестве основной рабочей станции. Обычно он стоит дома, но иногда ездит со мной на большие мероприятия вроде Chaos Communication Congress. И раз у меня уже есть VPN между ноутом и одним из моих серверов, на котором крутится Prometheus, я могу просто выполнить emerge prometheus-node_exporter, поднять службу и указать её экземпляру Prometheus. Он автоматически сконфигурирует предупреждения, и мой телефон начнёт разрываться, когда я открою слишком много вкладок в Chrome и исчерпаю все 32 Гб оперативки. Замечательно.
Примерно через час после настройки мой телефон сообщил, что свежедобавленный объект мониторинга недоступен. Увы, я прекрасно соединялся с ноутбуком по SSH, так что всё дело было в обрушении node_exporter.
node_exporter, как и многие компоненты Prometheus, написан на Go. Это относительно безопасный язык: он позволяет вам при желании выстрелить себе в ногу, а его гарантии безопасности несравнимы с Rust, например, но всё же случайно вызвать segfault в Go не так-то легко. Кроме того, node_exporter — довольно простое Go-приложение, по большей части использующее чисто гошные зависимости. Значит, случай с падением будет интересным. Особенно учитывая, что падение произошло в mallocgc, который вообще не должен падать при нормальных условиях.
После нескольких перезапусков стало ещё интереснее:
Так, уже интереснее. В этот раз сломался Sprintf. Шта?
Теперь проблемы начались и у сборщика мусора. Уже другой сбой.
Здесь можно было сделать два логичных вывода: либо у меня серьёзная проблема с оборудованием, либо в бинарнике баг, связанный с повреждением памяти. Сначала я решил, что первое маловероятно, потому что ноутбук обрабатывал одновременно очень разные виды нагрузок и не было ни малейших признаков нестабильной работы, которые можно было бы связать с оборудованием (у меня падало ПО, но это никогда не происходило случайно). Поскольку бинарники Go вроде node_exporter связаны статично и не зависят от каких-то иных библиотек, я скачал официальный релиз бинарника и попробовал сделать так, чтобы большая часть моей системы не представляла собой переменную. Но всё равно получил сбой:
Совершенно новый сбой. Похоже, проблема с node_exporter или одной из его зависимостей, так что я заполнил отчёт о баге на GitHub. Быть может, разработчики уже сталкивались с подобным? Надо привлечь их внимание и послушать, что они скажут.
Неудивительно, что сначала предположили: дело в оборудовании. Вполне обоснованно, ведь я столкнулся с проблемой только на одном компьютере. Все остальные мои машины прекрасно гоняли node_exporter. У меня не было других доказательств железного сбоя на этом хосте и даже объяснений, что именно могло вызвать падение node_exporter. Memtest86+ ещё никому не вредил, поэтому я решил им воспользоваться.
А затем случилось это:

Вот что бывает, когда пользуешься ширпотребом для серьёзных задач
Опа! Битая память. Точнее, один бит. Я прогнал тест целиком и нашёл этот бит и несколько ложноположительных в тесте № 7 (в нём перемещаются блоки, что может привести к размножению единственной ошибки).
Дальнейшее тестирование в Memtest86+ показало, что тест № 5 в режиме SMP быстро обнаруживает проблему, но обычно не при первом проходе. Проблема всегда возникала с одним битом по одному и тому же адресу. Похоже, слабая ячейка памяти, которая при нагреве начинает барахлить. Логично: рост температуры увеличивает утечки в ячейках памяти, и повышается вероятность, что какая-нибудь маргинальная ячейка начнёт сбоить. 
Чтобы было понятнее: это один плохой бит из 274 877 906 944. Прекрасное соотношение! У жёстких дисков и флеш-памяти коэффициент ошибок гораздо выше — в этих устройствах битые блоки помечаются ещё на заводах, а во время эксплуатации новые битые блоки незаметно для пользователей помечаются и переносятся в резервную зону. У оперативной памяти таких механизмов нет, так что битый блок остаётся навечно.
Увы, вряд ли этот бит приводит к сбоям в моём node_exporter. Приложение использует очень мало памяти, так что вероятность попадания в плохой бит (причём раз за разом) крайне мала. Проблема этого бита проявилась бы, в худшем случае приводя к битому пикселю в изображении, или выпадению одного символа в письме, или невыполнению какой-нибудь инструкции, или редкому segfault, если на плохой бит пришлось что-то серьёзное. Тем не менее это означает долгосрочные проблемы с надёжностью, и поэтому серверы и прочие устройства, для которых важна надёжность, используют ECC RAM — память с коррекцией ошибок.
У меня в ноутбуке такой роскоши нет. Но зато я могу пометить битый блок как плохой и сказать ОС, чтобы она его не использовала. У GRUB 2 есть малоизвестная возможность изменять схему памяти, передающуюся в загруженное ядро ОС. Не стоит покупать новую линейку памяти из-за одного плохого бита (к тому же DDR3 уже устарела, и велика вероятность, что новая память тоже будет со слабыми ячейками).
Но я могу сделать ещё кое-что. Раз ситуация ухудшается с ростом температуры, что будет, если я нагрею память?


100 °C
Установив температуру фена на 130 °C, я прогрел два модуля памяти (остальные два прячутся под задней крышкой, всего в моём ноутбуке четыре слота). Поигравшись с очерёдностью модулей, я нашёл ещё три слабых бита, которые были разбросаны по трём модулям и проявлялись только при нагреве.
Ещё я обнаружил, что расположение ошибок оставалось практически неизменным даже при переставлении модулей: верхние биты адреса были одни и те же. Всё дело в чередовании оперативной памяти: вместо того чтобы каждому модулю присваивать цельную четверть адресного пространства, данные распределяются по всем четырём модулям. Это удобно, потом что на область памяти можно наложить маску, охватывающую все возможные адреса для каждого плохого бита, и не беспокоиться о том, что всё перепутается, если поменять модули местами. Я выяснил, что если наложить маску на сплошную область в 128 Кб, то этого хватит для закрытия всех возможных перестановок адресов для каждого плохого бита. Но округлил до 1 Мб. Получилось три мегабайтных выравненных блока для маскирования (один из них покрывает два плохих бита, а всего я хотел покрыть четыре):
Маски я задал в /etc/default/grub с помощью синтаксиса маскирования адресов:
Затем быстро выполнил grub-mkconfig и выключил 3 Мб памяти с четырьмя плохими битами. Конечно, не ECC RAM, и всё же надёжность моей ширпотребовской памяти должна стать выше, ведь я знаю, что остальные ячейки ведут себя стабильно при нагреве до 100 °C.
Как вы понимаете, node_exporter продолжил падать, но мы же знали, что эта проблема была несерьёзной, верно?
В этом баге меня раздражало то, что причина явно была в повреждении памяти, которое рушило исполняемый код. Это очень трудно отладить, потому что нельзя предсказать, что именно будет повреждено (проблема плавающая), и нельзя поймать плохой код за руку.
Сначала я попробовал разделить пополам доступные релизы node_exporter и включать/выключать разные сборщики, но это ничего мне не дало. Пытался запускать экземпляр под strace. Падения прекратились, что свидетельствовало о состоянии гонки. Позднее оказалось, что strace-экземпляр тоже падает, но этого пришлось ждать гораздо дольше. Я предположил, что дело в распараллеливании. Задал GOMAXPROCS=1, чтобы Go использовал только один системный поток выполнения. Падения снова прекратились, так что у меня появились аргументы в пользу проблемы распараллеливания.
К тому времени я собрал большое количество логов падений и начал подмечать некоторые закономерности. Хотя места и порядок падений сильно менялись, но в целом сообщения об ошибках можно было разделить на три группы, и одна из ошибок возникала чаще одного раза. Я залез в поисковик и наткнулся на проблему в Go № 20427. Вроде бы она относилась к другой части Go, но приводила к таким же segfault и случайным сбоям. Тикет был закрыт без анализа, потому что ситуацию не удалось воспроизвести в Go 1.9. Никто не знал, в чём корень проблемы, она просто прекратилась.
Я взял код из отчёта об ошибке, который приводил к сбоям, и запустил у себя. О чудо, через секунды получил падение. Попал! Так гораздо лучше, чем часами ожидать падения node_exporter.
Это не приблизило меня к отладке на стороне Go, но позволило тестировать решения гораздо быстрее. Попробуем с другой стороны
Сбои возникают на моём ноутбуке, но не на других моих компьютерах. Я попробовал воспроизвести баг на всех серверах, к которым был лёгкий доступ, и не столкнулся ни с одним падением. Значит, на моём ноуте есть что-то особенное. Go статично связывает бинарники, так что остальное пользовательское пространство не имеет значения. Остаётся два кандидата: железо или ядро ОС.
Из железа мне доступны только мои компьютеры, но зато я могу протестировать ядра. Первый вопрос: будет ли падение в виртуальной машине?
Я сделал минимальную initramfs (initial ram file system), чтобы очень быстро запускать в виртуальной машине QEMU код воспроизведения сбоев без установки дистрибутива или загрузки полной версии Linux. Моя initramfs была построена на линуксовом scripts/gen_initramfs_list.sh и содержала файлы:
/init — входная точка Linux-initramfs, в моём случае это был простой скрипт оболочки для запуска теста и измерения времени:
/bin/busybox — статично связанная версия BusyBox, обычно используется в минимизированных системах вроде моей для предоставления всех основных утилит Linux-оболочки (включая саму оболочку).
Initramfs можно сделать так (на основе исходного дерева ядра Linux), где list.txt — вышеприведённый список файлов:
А QEMU может напрямую загружать ядро и initramfs:
В консоли ничего не отобразилось… затем я сообразил, что даже не скомпилировал в ядре на моём ноутбуке поддержку серийного порта 8250. Блин. У него же нет физического серийного порта, верно? Ладно, быстренько пересобрал ядро с поддержкой порта (и скрестил пальцы, чтобы не поменять ничего важного), попробовал снова, успешно загрузил и запустил воспроизводящий баг код.
Упал? Ага. Хорошо, сбой можно воспроизвести в виртуальной машине на том же компьютере. Попробовал ту же QEMU-команду на своём домашнем сервере, на его собственном ядре, и… ничего. Затем скопировал ядро с ноутбука, загрузил и… сбой. Всё дело в ядре. Это не железо.
Я понимал, что придётся скомпилировать много ядер, чтобы выловить проблему. Решил делать это на своём самом мощном компьютере: довольно старом 12-ядерном Xeon с 24 потоками исполнения (он уже не работает, к сожалению). Скопировал исходный код сбойного ядра, собрал и протестировал.
Не упало.
Чё?
Поскрёб в голове, проверил, падает ли оригинальный бинарник ядра (упал). Неужели всё-таки железо? Неужели имеет значение, на каком компьютере я собираю ядро? Попробовал собрать на домашнем сервере, вскоре случилось падение. Если собрать одно ядро на двух машинах, то падения есть, а если собрать на третьей — падений нет. Почему так?
На всех трёх машинах стоит Linux Gentoo с изменениями Hardened Gentoo. Но на ноутбуке и домашнем сервере стоит ~amd64 (нестабильная версия), а на Xeon-сервере — amd64 (стабильная). То есть GCC разные. На ноуте и домашнем сервере стоял gcc (Gentoo Hardened 6.4.0 p1.0) 6.4.0, а на Xeon — gcc (Gentoo Hardened 5.4.0-r3 p1.3, pie-0.6.5) 5.4.0.
Но ядро на домашнем сервере, почти той же версии, что и на ноуте, собранное с помощью того же GCC, не воспроизвело падения. Получается, что важны оба компилятора, использованные для сборки ядра, а также само ядро (или его конфигурация)?
Для выяснения я скомпилировал на домашнем сервере точно такое же дерево ядра с моего ноутбука (linux-4.13.9-gentoo), оно падало. Потому скопировал .config с домашнего сервера и скомпилировал его — не падало. Получается, нужно учитывать разницу в конфигурациях и компиляторах:
Два конфига, один хороший, второй плохой. Конечно, они сильно различались (я предпочитаю включать в конфигурацию своего ядра только те драйверы, что нужны на конкретном компьютере), так что пришлось последовательно пересобирать ядро, выискивая различия.
Решил начать с «точно плохого» конфига и стал выкидывать из него разные вещи. Поскольку воспроизводящий сбой код падал через разные промежутки времени, проще было тестировать на «всё ещё падает» (просто дожидаясь падения), чем на «не падает» (сколь придётся ждать, чтобы подтвердить отсутствие сбоя?). На протяжении 22 сборок ядра я настолько упростил конфигурацию, что ядро лишилось поддержки сети, файловой системы, основы блокового устройства и даже поддержки PCI (причём в виртуальной машине продолжало прекрасно работать!). Мои сборки занимали теперь меньше 60 секунд, а ядро стало примерно на 3/4 меньше обычного.
Затем я перешёл к «точно хорошей» конфигурации и убрал всё ненужное барахло, проверяя, не падает ли воспроизводящий код (это было труднее и дольше, чем предыдущий тест). У меня было несколько ложных веток, когда я изменил что-то (не знаю что), рушащее воспроизводящий код. Я ошибочно пометил их как «не падающие», и, когда столкнулся с падениями, пришлось возвращаться к предыдущим собранным ядрам и выяснять, когда же именно появились падения. Я собрал семь ядер.
В конце концов я сузил зону поисков до небольшого количества различающихся опций. Некоторые привлекали внимание, например CONFIG_OPTIMIZE_INLINING. После осторожной проверки я убедился, что виновницей была именно эта опция. Если её выключить, то воспроизводящий код падает, а если включить, сбоев нет. Включение этой опции позволяет GCC определять, какие функции inline действительно нужно инлайнить, а не делать это принудительно. Это объясняет и связь с GCC: поведение инлайнинга наверняка различается в разных версиях компилятора.
Что дальше? Мы знаем, что к падениям приводит опция CONFIG_OPTIMIZE_INLINING, но она может менять поведение каждой функции inline во всём ядре. Как вычислить корень зла?
Есть идея.
Замысел в том, чтобы компилировать одну часть ядра с включённой опцией, а другую часть — с выключенной. После проверки можно понять, какое подмножество вычислительных модулей ядра содержит проблемный код.
Вместо перечисления всех файлов объектов и выполнения бинарного поиска и я решил воспользоваться хешами. Написал скрипт-обёртку для GCC:
Он на основе имени файла объекта генерирует хеш SHA-1, затем проверяет один из первых 32 битов хеша (определяются по переменной среды $BIT). Если бит равен 0, собирается без CONFIG_OPTIMIZE_INLINING. Если равен 1, собирается с CONFIG_OPTIMIZE_INLINING. Я выяснил, что ядро содержит около 685 файлов объектов (помогли мои усилия по минимизации), так что для уникальной идентификации требовалось примерно 10 битов. У подхода с использованием хеша есть одна особенность: можно беспокоиться только о сбойных вариантах (бит равен 0), поскольку гораздо сложнее доказать, что данная сборка ядра не падает (падения вероятностны и могут происходить далеко не сразу).
Я собрал 32 ядра, по одному на каждый бит префикса SHA-1, это заняло всего 29 минут. Затем начал тестировать ядра и при каждом падении определял те регулярные выражения возможных хешей SHA-1, что имели на определённых позициях нулевые биты. После восьми падений (и нулевых битов) я вычислил четыре файла объектов, ещё пара оказалась под подозрением. После десяти падений совпадение было одно.
Код vDSO. Ну конечно.
vDSO на самом деле не является кодом ядра. Это маленькая библиотека общего использования, которую ядро кладёт в адресное пространство каждого процесса. vDSO позволяет приложениям выполнять особые системные вызовы без выхода из пользовательского режима. Это значительно повышает производительность, при этом ядро при необходимости может менять подробности реализации этих системных вызовов.
Иными словами, vDSO — компилируемый GCC код, собираемый с ядром и в конце концов связываемый с каждым приложением в пользовательском пространстве. Это код пользовательского пространства. Теперь понятно, почему важны версии ядра и компилятора: дело не в самом ядре, а в библиотеке, идущей с ядром! Go использует vDSO для повышения производительности. Также в Go есть (довольно безумная, как я считаю) политика переизобретения собственной стандартной библиотеки, так что для вызова vDSO он не использует код стандартной Linux-библиотеки glibc, а выкатывает собственные вызовы (и системные тоже).
Как переключение CONFIG_OPTIMIZE_INLINING влияет на vDSO? Посмотрим на машинный код.
С CONFIG_OPTIMIZE_INLINING=n:
С CONFIG_OPTIMIZE_INLINING=y:
Любопытно, CONFIG_OPTIMIZE_INLINING=y должен позволять GCC инлайнить меньше, а на деле компилятор инлайнит больше: в этой версии vread_tsc оказалась инлайненной, в отличие от версии с CONFIG_OPTIMIZE_INLINING=n. Но vread_tsc вообще не помечена как inline, так что GCC повёл себя целиком в рамках дозволенного, хотя на первый взгляд это и не так.
Но что плохого в инлайнинге функции? В чём проблема? Давайте посмотрим на неинлайненную версию…
Почему GCC выделяет больше 4 Кб в стеке? Это не выделение памяти в стеке, это стековый зонд (stack probe), а точнее — результат свойства GCC -fstack-check.
В Linux Gentoo Hardened -fstack-check включено по умолчанию. Это сделано для закрытия уязвимости Stack Clash. Хотя -fstack-check — старое свойство GCC и не предназначалось для этой задачи, но всё же успешно закрывает уязвимость (мне сказали, что нормальная защита от Stack Clash появится в GCC 8). Но есть побочный эффект: каждая функция, не относящаяся к листьям дерева ядра (функция, вызывающая функции), перед указателем стека размещает 4 Кб стекового зонда. То есть код, скомпилированный с -fstack-check, потребует как минимум 4 Кб памяти в стеке, если это не функция-«лист» (или функция, в которой каждый вызов инлайненный).
Go любит маленькие стеки.
Похоже, 104 байтов на всех не хватит. Уж точно не моему ядру.
Надо сказать, что спецификация vDSO не упоминает о необходимости использования максимального стека, так что это исключительно Go виноват в том, что он делает ошибочные предположения.
Это полностью объясняет все симптомы. Стековый зонд — это orq, логическое ИЛИ с 0. Холостая инструкция, но эффективно зондирующая целевые адреса (если они не распределены, возникает segfault). Но в коде vDSO не было segfaults, почему же Go ломается? Дело в том, что ИЛИ с 0 не совсем холостая инструкция. Поскольку orq не атомарная инструкция, процессор считывает адрес памяти и записывает в него. Возникает состояние гонки. Если в других процессорах параллельно выполняются другие потоки, orq может в то же время прервать запись в память. Поскольку запись выполнялась вне границ стека, то наверняка она вторглась в стеки других потоков выполнения или в случайные данные, и когда звёзды выстраивались нужным образом, то запись в память прерывалась. В том числе и поэтому GOMAXPROCS=1 имеет отношение к описанной проблеме, этот параметр не позволяет двум потокам выполнения одновременно выполнять код Go.
Как исправить? Я оставил это на усмотрение разработчикам Go. Они решили увеличить стек перед вызовом функций vDSO. Это немножко уменьшило скорость (наносекунды), но вполне приемлемо. После сборки node_exporter с исправленным Go падения прекратились.
¯\_(ツ)_/¯
=====
Довольно примечательный случай внедрения видеоконференцсвязи на одном промышленном предприятии
2017-12-07, 10:36
Сервер
Выполняемые функции СВКС
Применение дублирования и резервирования
Cisco Unified Communications Manager (CUCM)
Управление вызовами.
Управление устройствами.
Ведение адресного справочника
Резервирование одним сервером CUCM с ролью subscriber. Резервирование обеспечивает автоматический переход пользователей на резервный сервер в случае отказа основного сервера в соответствии со штатным техническим решением Cisco
Cisco IM&Presence
Обмен мгновенными сообщениями и статусами доступности
Дублирование резервным сервером IM&Presence с ролью subscriber. Автоматический переход пользователей на резервный сервер в случае отказа основного сервера в соответствии со штатным техническим решением Cisco
Cisco Expressway-C
Интеграция с внешними сетями и абонентами, подключаемыми по VPN
Дублирование дополнительным сервером Expressway-C. Автоматическая балансировка нагрузки при использовании обоих серверов. В случае отказа одного из серверов используются функции другого в соответствии со штатным техническим решением Сisco.
В случае отказа одного из серверов его лицензии остаются доступны другому в течение 2 недель
Cisco Expressway-E
Интеграция с внешними сетями
Дублирование дополнительным сервером Expressway-E. Автоматическая балансировка нагрузки при использовании обоих серверов. В случае отказа одного из серверов используются функции другого в соответствии со штатным техническим решением Сisco.
В случае отказа одного из серверов его лицензии остаются доступны другому в течение 2 недель.
Внешние интерфейсы серверов подключаются к сетям разных операторов связи, предоставляющих услуги доступа к сети Интернет
TMS
Планирование конференций
Не дублируется
CMS
Проведение многоточечных конференций, интеграция сLync
Частичное дублирование функций существующим аппаратным MCU. В случае выхода сервера CMS из строя становятся недоступны следующие функции:
— интеграция с MS Lync;
— подключение в конференцию с помощью браузера;
— программные клиенты Cisco Meeting App
Пользователь
=====
Как мы помогаем быстро аттестоваться по требованиям ИБ тем, кто переезжает в облако
2017-12-07, 10:15
Обрабатываемые ПДн
Объём ПДн
Тип актуальных угроз
Угрозы 1-го типа
Угрозы 2-го типа
Угрозы 3-го типа
Специальные категории ПДн
Более 100 тыс.
УЗ 1
УЗ 1
УЗ 2
Менее 100 тыс.
УЗ 1
УЗ 2
УЗ 3
Специальные категории ПДн сотрудников оператора
Любой
УЗ 1
УЗ 2
УЗ 3
Биометрические ПДн
Любой
УЗ 1
УЗ 2
УЗ 3
Иные категории ПДн
Более 100 тыс.
УЗ 1
УЗ 2
УЗ 3
Менее 100 тыс.
УЗ 1
УЗ 3
УЗ 4
Иные категории ПДн сотрудников оператора
Любой
УЗ 1
УЗ 3
УЗ 4
Общедоступные ПДн
Более 100 тыс.
УЗ 2
УЗ 2
УЗ 4
Менее 100 тыс.
УЗ 2
УЗ 3
УЗ 4
Общедоступные ПДн сотрудников оператора
Любой
УЗ 2
УЗ 3
УЗ 4
Параметр сегмента
Защищенный
Закрытый
Среда виртуализации
OpenStack-KVM, VMware
VMware
Назначение
Размещение информационных систем, не предъявляющих специализированных требований к информационной безопасности.
Размещение информационных систем компаний и организаций, в том числе участвующих в процессах обработки данных держателей банковских карт
Размещение государственных информационных систем (ГИС) и информационных систем персональных данных (ИСПДн) с наивысшими требованиями к информационной безопасности
Соответствие законодательству РФ по информационной безопасности
● Приказ ФСТЭК России №21
● Payment Card Industry Data Security Standard (PCI DSS) — стандарт безопасности данных индустрии платёжных карт;
● Положение Банка России №382-п «О требованиях к обеспечению защиты информации при осуществлении переводов денежных средств и о порядке осуществления Банком России контроля за соблюдением требований к обеспечению защиты информации при осуществлении переводов денежных средств»;
● Стандарт Банка России (СТО БР ИББС)
● Приказ ФСТЭК России №17
● Приказ ФСТЭК России №21
● Закон РФ №149-ФЗ
● Закон РФ №152-ФЗ
● Закон РФ №242-ФЗ
Сертификация / Аттестация
Сертификат соответствия (в качестве сервис-провайдера) требованиям Стандарта безопасности данных индустрии платёжных карт (PCI DSS)
Аттестат соответствия инфраструктуры сегмента «Закрытый» требованиям по защите информации для размещения информационных систем и систем, обрабатывающих персональные данные до 1-го класса/уровня защищённости
Информационная безопасность
Применяемые средства защиты позволяют выполнить следующие требования, предъявляемые:
● к информационным системам персональных данных до 3-го уровня защищённости включительно;
● стандартом PCI DSS;
● Положением Банка России №382-П
Применяемые средства защиты позволяют выполнить требования, предъявляемые:
● к информационным системам до 1-го класса защищённости включительно;
● к информационным системам персональных данных до 1-го уровня защищённости включительно.
Применяемые аппаратные и программные средства обеспечения информационной безопасности обладают сертификатами ФСТЭК России и/или ФСБ России
Физическая безопасность
Обеспечение физической безопасности ИТ-инфраструктуры:
● контроль и управление доступом в дата-центр;
● охранно-пожарная сигнализация;
● автоматическое пожаротушение;
● видеонаблюдение;
● сейфовые стойки
Обеспечение физической безопасности ИТ-инфраструктуры:
● контроль и управление доступом в дата-центр;
● охранно-пожарная сигнализация;
● автоматическое пожаротушение;
● видеонаблюдение;
● сейфовые стойки
Сетевой доступ
● Публичный Интернет;
● Через защищённое VPN-подключение поверх публичного Интернета (IPSec VPN — реализуется программными средствами платформы виртуализации);
● VPN канал
● Через отдельный, защищённый аппаратными средствами криптографической защиты канал связи;
● Через защищённое VPN-подключение поверх публичного Интернета (IPSec VPN с криптозащитой по ГОСТ
Облачная платформа
=====
Preview документов в программе на Python
2017-12-16, 10:12
Пользователь
=====
Как построить сообщество. Перевод книги «Социальная архитектура»: Глава 1. Инструментарий
2017-12-19, 20:08
Мой инструментарий социального архитектора состоит из 20 инструментов, каждый из которых соответствует какому-либо аспекту сообщества или группы. Их можно использовать двумя способами. 
Во-первых, с их помощью вы можете делать измерения существующего сообщества, оценивая его по шкале от нуля и выше. 
Во-вторых, вы можете использовать их для создания сообщества, при этом прилагая усилия там, где они наиболее необходимы. 
Спасибо Сергею Даньшину за помощь с переводом.
(Если у вас есть предложение, как перевести какой-то термин/фразу более ёмко, пишите в личку, хочется, чтобы эта книга стала полезной для многих сообществ.)
Мы рассмотрим эти инструменты поочередно и увидим, как они работают в разных сообществах. Для начала несколько общих советов о создании сообщества. Будьте предельно честны с собой и с другими. Главное для вас – это преодолеть собственные предубеждения и пристрастия, а уже потом – те, что присущи вашим коллегам. 
Каким бы инструментом я вас ни снабдил, вы захотите адаптировать его и подогнать к собственным нуждам. Социальная архитектура является все еще молодой наукой, и многие из моих инструментов будут слишком громоздкими или неполными.
Вот лучший способ, как поступить:
Отправная точка создания любого сообщества – формулировка его миссии. Она определяет цели, которые мы разделяем, еще до того, как присоединиться к проекту. Это как заголовок сайта или рекламный лозунг фильма. Например, заголовок у Reddit звучит так: «главная страница интернета» – амбициозная миссия, которая, тем не менее, выполнена. Слоган Фейсбука: «помогает вам связаться и поделиться с людьми в вашей жизни». 
СОВЕТ: используйте вашу миссию в качестве слогана на сайте, в рекламе, в презентациях и т.д. Если вы инвестируете деньги в ваше сообщество, подумайте о том, чтобы зарегистрировать формулировку миссии как торговую марку.
Без четкой миссии онлайн-сообщество не будет расти. Друзья, которые начали заниматься проектом, могут быть все согласны с тем, чего они хотят достичь, но каждому новичку придется гадать, что они имеют в виду. Люди будут заблуждаться в своих догадках и со временем могут изменить свое мнение. Будут возникать разногласия, путаница и разочарование по мере понимания людьми того, что их тяжелый труд был впустую, т.к. остальная часть группы движется в другом направлении. 
Реакция людей на миссию компании не должна быть «да, это благоразумно», а наоборот: «вы же это не серьезно, так?!». Миссия Википедии – «бесплатная энциклопедия, которую любой может редактировать», – хороший тому пример. Это было изначальной целью, поэтому все прочие, кроме считанных идеалистов, отнеслись к этому как к чему-то невозможному и бредовому. На это и был расчет Википедии: чтобы эти идеалисты однажды взошли на борт. Невозможные цели привлекают правильных людей к молодому проекту. 
СОВЕТ: Меняйте миссию по мере взросления вашего сообщества. Вначале вы захотите привлечь молодых идеалистов и первопроходцев, потом – лидеров, а потом – первых адептов, широкую общественность, дальнейших последователей. Каждая из этих групп стремится к разному. Поняв это, соответственно измените миссию. 
Чтобы сформулировать хорошую миссию, ориентируйтесь на одну основную проблему, которой посвящен ваш проект. Reddit, например, решает проблему того, как получить новости из интернета при слишком большом количестве ресурсов с интересной информацией. Его «основная страница» представляет собой цифровую газету 21-го века. Википедия решает проблему аккумулирования знаний миллионом умов. Слова «любой может редактировать» – так же, как и «Глас народа – глас Божий», – говорят о том, что если и возможно найти истину, то только сообща. 
СОВЕТ: При намерении сделать что-либо, много или мало, всегда старайтесь начать с определения проблемы, которую вы хотите устранить. Только когда у вас будет четкая и реальная проблема, с наличием которой все согласны, только тогда приступайте к обсуждению возможных ее решений. Решение вымышленной проблемы схоже с группой без четкой миссии. У вас может быть несколько миссий, случайно или преднамеренно. Если миссии тянут сообщество в разных направлениях, это может кончиться плохо. Например, рост группы может потребовать вложений, что вступит в противоречие с позицией по вопросу прибыли. Если бы Википедия стала коммерческой организацией, с рекламой и комплектом высокооплачиваемых менеджеров, то это, по-вашему, привело бы к ее расширению или упадку? 
В случае с ZeroMQ наша миссия звучала так: «Быстрейшая. Передача сообщений. Всегда». Это хорошее и почти невозможное решение проблемы, насчет существования которой мы все соглашались, а именно: доступные на тот момент технологии были медленными и неповоротливыми. В то же время, мы с моим партнером-основателем Мартином расходились в целях. Он хотел создать лучший из возможных программных продуктов, я же хотел создать крупнейшее из возможных сообществ. По мере того, как росло число пользователей, его драматическое изменение, что сломало существующие приложения, причинило усиливающуюся боль. 
В данном случае мы смогли сделать всех счастливыми (Мартин ушел, чтобы работать над созданием новой библиотеки, названной «Nano»). Тем не менее, если вы не можете разрешить противоречия, касающиеся миссии, они могут серьезно навредить проекту. Проекты могут вынести многие споры, а вот разногласия между основателями довольно травмоопасны. 
СОВЕТ: Если основатели согласны, что «успех» определяется как «максимально возможное число участников», то в последующие годы это может помочь в сохранении целеустремленности. Это также облегчает измерение вашего успеха по мере развития. 
Определившись с миссией, вам нужно протестировать ее в реальном мире. Это значит, вам нужно дать краткий, но убедительный ответ на ту проблему, на которую вы нацелились. Я называю это «посевом». Этот процесс преследует две основные цели. Во-первых, начать собирать идеалистов и первопроходцев (в основном тех, кто был настолько безумен, чтобы поверить вам) в сообщество. Во-вторых, доказать или опровергнуть вашу миссию. 
Проекты могут кончиться неудачей по многим причинам. Но главная причина — основополагающая идея или миссия были не настолько удивительными, как того ожидали люди. Неудача – нормально, даже отлично, если только она не стоила нескольких лет вашей жизни. Посадить семечко и показать его только нескольким людям не достаточно, потому что большинство людей не будет критиковать. Из жалости. Однако попросите их потратить хотя бы несколько часов своего времени на то, чтобы сделать проект лучше, и если они не скажут «да», тогда вы поймете их настоящее отношение. 
СОВЕТ: Привлеките к «посевному» проекту внимание публики и вдохновляйте людей присоединяться к нему с самого начала. Если люди вовлекаются в проект, скорее их продвигайте. А если этого не происходит, то считайте это знаком того, что ваша миссия может быть ложной. Используйте «посевной» проект, чтобы создать сообщество. 
Когда люди соглашаются помогать вам, нужно обеспечить им место для совместной работы. Вам нужна «платформа для сотрудничества». Две моих самых любимых: Wikidot для информационных сообществ и GitHub для проектов по разработке ПО. Платформа должна быть бесплатной. С ней должно быть легко и в учебе и в работе. Ваш посевной проект должен быть виден анонимным участникам. Он должен работать для кого угодно, вне зависимости от его или ее возраста, пола, образования или местоположения. 
Все это позволяет потенциальным заинтересовавшимся незнакомцам зайти и посмотреть на вашу работу, и если им она приглянется и они почувствуют в ней вызов, то смогут постепенно вовлекаться в проект. Вы хотите работать над вашим посевным проектом публично и говорить о вашем новом проекте с самого начала. Это значит, что люди смогут делать предложения и чувствовать вовлеченность с самого первого дня.
Если мы как основатели группы выбираем тех, с кем будем работать, мы создаем основание для предвзятого выбора. Намного легче работать с теми милыми, умными людьми, которые соглашаются с нами, чем с теми идиотами и критиками, которые выражают свое несогласие. А когда вы соглашаетесь со мной, вы подтверждаете все те мои иллюзии и допущения, которые, как я знаю по собственному опыту, могут оказаться ложными самым удивительным способом.
Со временем увеличение количества людей, которые разделяют те же неверные допущения и предубеждения, может привести к гибели проекта. Например, при разработке программных протоколов требования к крупным компаниями могут сильно отличаться от требований к маленьким open source командам. Поэтому если комитет по протоколу состоит полностью из крупных компаний, то результат их деятельности будет неприемлем для массового рынка.
Решением является свободный доступ для всех заинтересованных, какой бы безумной и непохожей ни была бы их точка зрения. Это дает нам в перспективе широкое и разностороннее сообщество – предшественник умной толпы. В ZeroMQ мы никогда не отворачивались от тех, кто хотел участвовать. Я втягиваю людей, даже если их вклад в общее дело мал или неверен. Сообщество важнее, чем продукт. 
Когда сообщество посевного продукта созреет, участники захотят создать его второе поколение. Как социальный архитектор, вы должны руководить этим так, чтобы усилия умной толпы были направлены на разработку «реального» продукта. Возможно, где-то на этом этапе вы захотите найти хорошее доменное имя и сделать «приличный» веб-сайт. 
СОВЕТ: Если люди не присоединяются к вашему посевному проекту, не продолжайте заниматься им. Вместо этого разберитесь, что их останавливает, и устраните это. Начните заново с прополки. Не убивайте преждевременно побеги, людям требуется время, чтобы оценить то, что вы пытаетесь сделать.
Прозрачность очень важна для быстрого получения критики идей и прогресса в работе. Если несколько людей из команды отчаливают и работают над чем-нибудь вместе некоторое время, например пару дней, ничего страшного, но вот когда речь идет о неделях, тогда то, чем они занимаются, следует представить группе как свершившийся факт. Если один человек так поступает, то группа может просто отмахнуться от него. Но если двое или больше – становится сложным откреститься от плохих идей. Секретность и некомпетентность идут рука об руку. Группам, работающим втайне, не постигнуть мудрости. 
СОВЕТ: Когда один человек делает что-то в темном углу – это эксперимент. Когда двое или больше делают что-то в темном углу – это тайный заговор. 
В случае с ZeroMQ ушло несколько лет на то, чтобы создать по-настоящему открытую и прозрачную атмосферу. До этого главные участники работали тайно, публикуя свою работу только тогда, когда считали, что она готова к общественному обозрению. Но когда они это делали, остальному сообществу было сложно сказать «нет». И зачастую работа была не в тему… да, отличным решением проблемы, но до которой никому нет дела. В конце концов, мы недвусмысленно запретили подобные вещи. 
Иронично, что тайна кажется неотъемлемой в некоторых бизнес-моделях. Прибыль часто приходит от игнорирования потребителей. Большинство коммерческих предприятий, даже такие большие сообщества, как Twitter, зависят от строгого разграничения «их» и «нас». Однако цифровое общество лучше всего растет, когда масштаб приоритетней прибылей и когда относится к пренебрежению как к проблеме, требующей решения. Если ваши клиенты не допускаются до ваших внутренних процессов, то вам будет закрыт доступ к пониманию, где в них кроются ошибки. 
Деньги – забавная вещь. Слишком мало – и сообщество будет умирать с голоду (я вернусь к этому позже). Слишком много – начнется разложение. Необходимо понимать, почему каждый из участников вообще занимается этим. Какие у них экономические мотивы? Даже в добровольных сообществах каждый участник преследует свои интересы.
Мы в ZeroMQ изначально начали с малооплачиваемой группы и через два года пришли к добровольному сообществу, прагматично – если не сказать циничнее – умышленно потратив деньги и оказавшись вынужденными уволить разработчиков. Некоторые из них растворились в других компаниях, некоторые вернулись в качестве участников, и проект стал более захватывающим и веселым, чем был раньше. Люди работали над ZeroMQ, потому что им это было нужно для собственных проектов – потратив немного времени на его улучшение, они выигрывали или экономили в разы больше. 
Когда вы работаете на кого-то, то будете делать то, что он или она хочет. Когда вы работаете на себя, вы делаете то, что нужно вам. Это огромная разница. Люди с деньгами, но без навыков или вкуса, – шелуха общества. Мы презираем оплачиваемых участников Википедии, платных блогеров и модераторов на Reddit, потому что мы знаем, что выражаемые ими мнения почти по определению ложь. Будет ли блогер, проплаченный Голливудом, критиковать новый летний блокбастер?
Я не имею ничего против наемных сотрудников. Однако если вы нацелены на создание наиболее крупного, наиболее успешного сообщества, то вам нужны участники, который будут стараться по честным, понятным причинам. Если кинорежиссёр приходит на Reddit обсудить фильм – здорово. Если его маркетологи заходят, чтобы потереть критические комментарии, это отвратительно.
СОВЕТ: один бесплатный участник стоит десяти оплачиваемых сотрудников. 
Группе требуется много договоренностей, чтобы работать сообща. Я называю их «протоколами». Наверно, самый важный из них для творческого сообщества – возможность перерабатывать материал (ремиксабельность). Будь то музыка, искусство, изображения, видео, комментарии, программы или wiki-страницы, встанет следующий вопрос: «А что за авторская лицензия стоит за этим материалом, и как это затронет сообщество?».
Грубо говоря, есть три типа авторских лицензий: 
Пользователи предпочитают модель free to take, потому что она позволяет им использовать контент как угодно без обратных обязательств. Представьте себе диджея, который выпускает популярный трек по модели free to take. Потом компания делает ремикс и использует его в рекламе. И этот ремикс будет закрыт для использования. Теперь, диджей не сможет переработать этот ремикс и, возможно, не сможет даже проигрывать этот ремикс. 
Все же сообщества работают лучше с третьей моделью, т.к. тогда пользователи становятся участниками. С лицензией share-alike диджей смог бы использовать ремикс, ремикшировать его еще и превратить в дискотечный хит. Знания и идеи текут во всех направлениях, а не вытекают из сообщества в застойное болото. Это мощное течение, и это особенно важно для тех из нас, кто строит сообщества с минимальным бюджетом. Если вы являетесь крупной компанией, вкладывающей кучу денег в сообщество, то модель free to take вам подойдет лучше. 
СОВЕТ: Если каждый участник владеет тем, что он привнес в сообщество, а вы используете лицензию share-alike, вам не требуются переуступки авторских прав или возобновление лицензии от участников. 
Хорошие протоколы позволяют посетителям участвовать без предварительного одобрения. Они разрешают деструктивные конфликты и превращают их в полезные состязания. То, что анархисты могут присоединяться к умной толпе так же успешно, как и любой другой, объясняется тем, что толпа может разрабатывать свои собственные правила. Обычно эти правила касаются переработки материала, идентичности, рангов и т.д. Не важно, какая у них форма, хорошие правила просты, четки, ясно прописаны и всеми одобрены. 
Если вы создаете проект в области программного обеспечения, вы можете взять существующее руководство, например, протокол С4, который мы сделали для ZeroMQ. Или же вы можете начать с минимумом инструкций и добавлять их по мере определения тех проблем, с которыми сталкивается сообщество. К слову, так было и с руководством Википедии. Некоторые правила должны быть установлены с самого начала (например, об авторских правах и участии). Другие могут быть придуманы по мере необходимости (например, процедура разрешения конфликтов). Сложные, бесцельные или не прописанные правила отравляют группу. Они создают пространство для споров, путают людей и повышают стоимость входа в группу или выхода из нее. 
СОВЕТ: Пишите аккуратно ваши правила, начиная с лицензии на контент, и оценивайте, насколько они помогают людям. Изменяйте их по мере необходимости. 
Без органов власти правила не имеют силы. Основатели сообщества и основные участники являются де-факто их представителями. Если они злоупотребляют своим положением, они теряют участников – и проект умирает либо разветвляется в зависимости от различных правил. Власть должна быть масштабируемая (то есть быть способной охватывать деятельность группы любого размера) и допускать передачу по мере роста и изменения группы. 
Пока мы используем власть для сооружения игровой площадки, многие группы используют власть для контроля своих членов, держа их в группе и заставляя их соответствовать стандартам. Любимый прием в культах – наугад наказывать и вознаграждать людей, чтобы они были сбиты с толку и перестали задавать вопросы администрации. 
СОВЕТ: Назначайте самых активных участников на административные посты и побыстрее. У вас есть небольшой промежуток времени, чтобы успеть сделать это, иначе они уйдут в другие проекты.
Вы должны быть частью вашего сообщества, и вы должны соблюдать ваши собственные правила. Если вы замечаете за собой, что нарушаете их или хотите это сделать, значит, они неточны и требуют корректив. 
В сообществе ZeroMQ мы сражались из-за вопроса о том, кто мог определять правила, и в конце это привело к торговой марке и доменному имени. Человек или компания, которая владеет именем проекта, является верховной властью и может определять правила. Если они идиоты, то проект умрет. 
СОВЕТ: Если вы инвестируете деньги в сообщество, рассмотрите вариант использования торговой марки в США, чтобы иметь возможность предотвращать использование другими людьми похожих имитирующих названий, которые не имеют к вам отношения. Это стоит около 750 долларов. 
Членство должно быть символом объединения, а не служить удостоверением. Как часто отмечал Мистер Спок, эмоции не логичны. Некоторые группы руководствуются логически обоснованными целями, в других же правят эмоциональные факторы, такие как давление со стороны членов своего круга, стадный инстинкт и даже коллективная истерия. Определяющим моментом, видимо, служат отношения между группой и ее участниками. Мы может выявить это вопросом: участники «исключительно привержены» группе? Под исключительной приверженностью имеется в виду придание большего значения существованию группы, а не ее работе. Подобная приверженность заканчивается конфликтом с другими группами. 
СОВЕТ: Держитесь подальше от формальных моделей членства, особенно тех, которые стараются превратить людей в собственность группы. Позволяйте анонимное или не персонализированное участие. Поощряйте людей создавать свои конкурирующие проекты – пространство для экспериментов и для изучения нового.
Группы индустриальной эпохи, словно культы, владеют своими членами. Сотрудник принадлежит компании. Даже идеи, которые пришли вам в голову под душем, – тоже собственность вашего работодателя. А когда группа владеет своими участниками, то мотивирует их страхом, ненавистью, завистью и злостью, подменяя сознательные логичные мотивы. Страх исключения широко используется для подчинения людей одному стандарту: «Делай, что я говорю, или я уволю тебя!». 
СОВЕТ: Для определения того, насколько группа похожа на племя, просто начните конкурирующий проект. Если реакция на это отрицательная и эмоциональная, в группе доминирует родоплеменная парадигма. В группе со здоровой атмосферой аплодисментами встретят своих соперников. 
Некоторым людям нравится, когда им говорят, что делать. Лучшие участники и команды сами выбирают себе задачи. Успешное общество распознает проблемы и само организуется для их решения. Более того, оно делает это быстрее и лучше, чем любая иерархически управляемая структура. Это значит, что сообщество должно принимать помощь в любой области, без ограничений.
Распределение задач сверху вниз является антипаттерном с присущими ему многими слабостями. Он не дает индивидуумам действовать при обнаружении ими проблемы. Для него характерны феоды, где работа и необходимые ресурсы принадлежат отдельным людям. Он создает длинные коммуникационные цепочки, которые не позволяют реагировать быстро. Ему требуются прослойки менеджеров, просто чтобы соединить тех, кто принимает решения, с теми, кто будет выполнять работу. 
СОВЕТ: Пишите правила, чтобы повысить качество работы, подчеркивающие, что каждый может работать над тем, что ему интересно. 
В сообществе ZeroMQ мы избавились от назначения задач. Например, мы не принимали запросы о каких-либо особенных функциях. Если кому-то нужна была специальная функция, то он либо посылает нам патч, либо предлагает оплатить добавление изменений, либо он ждет. Это значит, что люди делают только те изменения, которые на самом деле нужно сделать.
СОВЕТ: Сообществам требуется иерархия полномочий. Однако она должна быть подвижной и строго делегированной. То есть выбирайте людей, с которыми вы работаете, и позвольте им выбирать тех, с кем они будут работать. Структура полномочий, словно жидкий цемент, она затвердевает и сковывает движения людей. Любая структура старается себя защитить. 
В разношерстной группе возникают конфликтующие мнения, и здоровая группа эти конфликты охватывает и перерабатывает. Критики, иконоборцы, вандалы, шпионы и тролли держат группу в напряжении. Они могут быть катализатором вовлеченности остальных участников. Википедия процветает благодаря, а не вопреки, тем, кто кликает «Edit» с целью превратить статью в мешанину. Это классический антипаттерн, подавляющий идеи и взгляды меньшинства, используя предпосылку, что они «опасны». К тому же это неизбежно подавляет новые идеи. Логика обычно в том, что слаженность группы важнее ее разнообразия. Потом же получается так, что на ошибки не реагируют, а лишь еще больше обособляются. На самом деле, группа может быть важнее, чем результаты ее деятельности, если она многообразна и открыта новым аргументам. Это трудный урок, который полезен и обществу в целом: нет опасных суждений, есть опасные ответы. 
То, как сообщества разбираются с троллями и вандалами, это одно. Разобраться с фундаментальными отличиями мнений – это другое. Ранее я говорил, что конфликтующие друг с другом миссии могут стать проблемой. Лучшее решение, которое я знаю, – это превратить конфликт в состязание. К примеру, браузер Google Chrome стал более легкой, более быстрой альтернативой Firefox, который становился раздутым и медленным. Тогда команда Firefox взялась за дело с умом, и теперь Firefox работает быстрее, чем Chrome. 
СОВЕТ: Если есть интересная проблема, сделайте так, чтобы несколько команд соревновались, пытаясь решить ее. Соревнование — очень веселая штука, и может породить лучшие решения, чем монополистический подход. Вы можете даже организовывать соревнования с призами. 
Все это хорошо: пытаться обратить конфликт в состязание. Однако вам необходимо обеспечить участников группы информацией о том, как хорошо они справляются. Лучшие инструменты, такие как GitHub, показывают точное число людей, которые наблюдают или отметили проект или начали проект-ответвление (отражены различные уровни интереса и приверженности). 
Конечно же, Сеть всегда была озабочена «хитами» и анализом траффика, который показывает популярность сайта или страницы. Это облегчает измерение успеха онлайн-проекта. В старые времена индустриальной эпохи команды получали отзывы о своей работе от начальства. Что превращалось в ужимки перед властью: вас больше наградят за послушность, чем за прилежность. Делать начальство счастливым ради того, чтобы вам повысили зарплату, – не здоровое отношение.
СОВЕТ: Если ваша платформа не поддерживает этого напрямую, найдите возможности информировать ваших участников о том, насколько хорошо развиваются их проекты. 
Существует много причин, по котором люди принимают участие в сообществах. Преобладающая мотивация – потребность в восхищении за достигнутый успех. Как индивидуумом, так и в составе команды. Успех относительное явление, поэтому нам требуется метрика, какой-то высокий балл, на который люди будут ориентироваться в своих стремлениях. 
В сообществе ZeroMQ мы не придавали большого значения балльной оценке, хотя участники и получают больше любви при большем вкладе в общее дело. Это записывается в их послужной список. Участие в ZeroMQ может помочь при поиске хорошей работы. 
Reddit, как многие другие сайты, использует «карму», которая показывает, сколько голосов получил аккаунт за свои публикации и поведение. Работает это довольно неплохо. Некоторые сайты не показывают всю карму, чтобы предотвратить попытки людей обойти систему и получить более высокий балл. Некоторые сайты, такие как StackOverflow, до крайности увлекаются «геймификацией», используя ордена, высокие баллы, достижения и т.д. Мне кажется, это отдает манипуляциями и отвлекает от миссии сообщества. Люди должны принимать участие, стремясь к успеху проекта, а не к большому количеству игровых баллов. 
Социальное обязательство – делать группы разных людей счастливыми – задача, приносящая огромное удовлетворение, и она не загрязняет планету. Индустриальное общество нацелено на материальные награды (выше зарплата, больше дом, лучше машина), увязанные с иерархической структурой. Оно эффективно, потому что все мы любим богатство или у нас комплекс неполноценности; какая бы ни была причина, желание сделать начальство счастливей значит принятие меньших рисков. 
СОВЕТ: Когда люди просят вас сделать что-то, а вы не знаете как, тогда объявите публично, что это «невозможно». Или предложите решение настолько нелепое и безнадежное, что настоящие эксперты от возмущения возьмутся за дело. 
В своей книге Сероуиеки (Surowiecki) объясняет, что катастрофа шаттла «Колумбия» произошла по причине бюрократии в иерархичной структуре управления NASA, из-за которой были проигнорированы мнения обычных инженеров. Если группа децентрализована, ее члены более независимы, они получают большее различных входных данных, и они с самого начала разнообразны. 
Если группа географически не разбросана, то она становится однородной, где все члены обладают схожими входными данными и триггерами. Схожесть позволяет меньшинству доминировать над настроем группы и отбрасывать неординарные идеи. Оно позволяет ему буквально запугивать или обманывать большинство, тем самым подчиняя его. Требование о том, чтобы все члены группы сидели в одном офисе, департаменте или здании является старым антипаттерном, который сложно преодолеть. Вот почему все культы такие сплоченные. 
СОВЕТ: Вам нужны собрания, чтобы добиться от группы работы? Это знак того, что у вас есть глубокие проблемы в совместной работе. Вы исключаете людей, которые физически не находятся рядом. 
Бывает сложно отойти от старой модели совместной работы «обсуждение-действие». И, конечно, вам будет легче, если вы собираете группы с самого начала, а не пытаетесь изменить уже существующие. 
Сообществу нужно пространство для роста. В реалиях интернета это обычно сайт или набор сайтов и сопутствующие структуры вроде списков эл. почты, блогов и т.д. Мы видим, что это становится очень дешевым или даже бесплатным способом создания «пространства» в цифровом обществе. Вопрос в том, могут ли индивидуумы создавать свои личные пространства внутри сообщества. Если да, будут ли они приносить больше пользы общему проекту? 
Свобода создания структуры раздражает людей, которые считают, что это вносит хаос и беспорядок. Однако если вы используете обычные структуры (смотрите следующий пункт), ущербу участникам от этого нет никакого. А вот что вредно, так это создание структуры исходя из необоснованного мнения о ее пользе людям. Когда я возглавил ассоциацию FFII в 2005 г., предыдущим президентом было создано несколько сотен списков эл. почты, так он отмечал те проекты, над которыми, по его мнению, люди должны были работать. Это не соответствовало тому, как люди хотели быть организованы, и было очень сложно удалить эти списки и создать новые, которые нам на самом деле были нужны. 
Конечно, группы индустриальной эпохи распределяли работу и ресурсы для ее выполнения. Любая новая инфраструктура – такая как сайт, список адресов эл. почты или вики – требует одобрения и решительности. Может даже потребоваться юридическая оценка авторских прав и патентов. Цена высока, поэтому люди неохотно идут на риск. Получается, что не экспериментируя и продолжая работать, они связывают себе руки.
В сообществе ZeroMQ требуется лишь один клик для создания нового проекта. В Википедии вы можете создать новую страницу, просто кликнув на «создать страницу». Оба проекта имеют механизмы защиты от случайного мусора. Википедия проводит довольно агрессивную чистку новых страниц. В ZeroMQ есть специальная процедура для внесения проекта в официальную организацию сообщества. 
СОВЕТ: Сделайте создание новых проектов для зарегистрированных пользователей максимально простым. Если проект создается пользователем, то не стоит беспокоиться насчет мусора. Если они находятся в общем пространстве, то вам могут потребоваться инструменты для очистки мусора и заброшенных проектов. 
По мере того как сообщество растет, направлять его становится сложнее. Если вы делаете единичный, постоянно развивающийся проект, состоящий из множества отдельных задач, то это становится все сложнее и сложнее со временем. Представьте себе средневековый замок. Эта проблема особенно остро стоит перед большими компаниями, развивающими проект, которые иногда забывают о затраченных средствах. 
Запутанность отпугивает людей – очень сложно становится разобраться. Решением является использование стандартных структур, выучив которые раз, вы можете распознавать всегда. Подойдет не любая структура. Нам бывает сложно выучить структуры глубже трех-четырех уровней. Однако мы с радостью исследуем очень широкие системы с тысячами или миллионами блоков, если эти блоки соответствуют отдельным задачам или проектам. 
Представьте себе город. 
Успешные онлайн-сообщества – это города, а не замки. Википедия состоит из нескольких вики со специфичным языком, многие разбиты на миллионы страниц (проектов), каждая структурирована секциями, обсуждением, историей, примечаниями и т.д. Несколько людей могут работать над одной страницей одновременно или один человек может медленно править или заботиться о дюжине или сотне страниц. 
GitHub управляет миллионами программных хранилищ («репозиториями»), сгруппированными по учетным записям пользователей или организаций, и каждый обладает своей структурой (файлы-исходники, документация и т.д.), что зачастую зависит от языка (Java-репозитории используют один стиль, С-репозитории – другой и т.д.). Один репозиторий может насчитывать несколько участников, люди могут работать с разным количеством репозиториев. Сообщество ZeroMQ является организацией, которая состоит из растущего числа проектов. 
СОВЕТ: Спроектируйте ваше сообщество как город поддающихся поиску проектов, где любой может начать новый проект – проекты представляют, наверно, дюжину работ людей, и у всех схожая структура, насколько это возможно. Бизнес тяготеет к замкам, которые неизбежно будут посвящены Важным Персонам, а не проектам, и, конечно же, не основным проблемам бизнеса. Эти организации всегда огромны и нестандартны. И нет никакой возможности разобраться в них, кроме как запоминать каждую их деталь. Но и тогда вы не сможете с легкостью прогуливаться по замку, поэтому мало толку изучать его планировку. 
Когда ZeroMQ только начинался, это был лишь один проект с единственной страницей README. Сегодня это сто или больше небольших проектов, каждый из которых имеет свою документацию, сообщество и динамику. Попасть в уже взрослый проект может быть трудно. Как я уже сказал, использование стандартных структур жизненно необходимо. Более того, вам потребуется проследовать по довольно специфической траектории при изучении, от легкого к более сложному, от стадии праздного посетителя до участника-эксперта. Считайте ваше сообщество компьютерной игрой, где сложность уровней возрастает соразмерно с выигрышем. Люди будут играть «в соответствии со своим уровнем». Если вы все делаете правильно, вы привлечете многих. Если неправильно, то эксперты будут скучать на легком уровне, а новичков отпугнет сложность на старте. 
СОВЕТ: Используйте классические инструменты обучения – презентации, видео, ответы на часто задаваемые вопросы (FAQ), обучающие материалы – чтобы люди могли начать. Вам будет легче, если вы состоите в сообществе, потому что тогда вы можете посмотреть, какие вопросы чаще всего задают начинающие. 
Иногда есть соблазн агрессивно агитировать людей вступить в сообщество. В конце концов, многим нравятся острые аргументы, особенно когда они уверены в своей правоте. Некоторые группы развиваются за счет своей враждебности и негатива по отношению к другим группам, особенно если еще есть и предыстория. Тон, который вы задаете, будучи основателем, сохранится на долгое время. Если вы продвигаете свое сообщество, нападая на конкурентов, вы привлечете определенно настроенных людей, и такой настрой получит развитие. Рано или поздно негатив обернется внутрь и может оказаться губителен для сообщества. 
СОВЕТ: Когда вы говорите о людях, продукции или организациях, будьте вежливы и не теряйте самоконтроль. Когда вы рекламируете продукцию или сообщество, говорите о проблемах, которые вы решили, а не о том, чем вы лучше конкурентов. 
На своем опыте знаю, что лучше задать позитивный тон с самого начала. Конкуренты – это благо, т.к. дают на возможность состязаться. Подражатели – тоже хорошо, потому что подтверждают, что рынку вы нужны. Тролли и вандалы – отлично, ведь они дают искренним людям дополнительный шанс доказать свою полезность. И так далее. Кажется, что это трудно, искать позитивную сторону во всем. Однако это всего лишь образ мышления.
СОВЕТ: Добро пожаловать всем, за исключением безнадежных смутьянов. Их немного, тех, которые просто не могут найти себе место в открытом, разнообразном сообществе. Вы можете попросить таких людей уйти, или, если необходимо, забанить их. 
Позитивный настрой характеризуется толерантностью и препятствует накалу страстей и возникновению споров. Так легче экспериментировать, делать ошибки и критически относиться к своей работе, а все это вместе взятое позволяет сообществу решать сложные задачи. 
Вы когда-нибудь задумывались, почему в человеке заложена потребность шутить, почему люди, которые никогда не смеются, кажутся странными и неприветливыми? Моя теория такова, что мы все используем юмор как способ разрядить ситуацию (что имеет очевидное преимущество для выживания). Люди не побьют шутника – только если шутка старая или плохо рассказана. Если серьезно, то юмор сводит на нет трайбализм и эмоции, и позволяет людям работать вместе, даже если они сильно отличаются друг от друга. Общая шутка может создать сильные связи, потому что она свидетельствует о схожести взглядов. Юмор является неотъемлемой частью общества и уменьшает стресс. 
СОВЕТ: Чем более серьезную тему затрагивает ваше сообщение, тем больше вам потребуется юмора. В моей книге о ZeroMQ мной написано много глупой чепухи вперемешку с тяжелым описанием технической части. Многим это понравилось. Если бы не алкоголь, то хмурое выражение лица индустриальной экономики никогда бы не сменялось улыбкой. Она себя очень серьезно воспринимает. Недостаток юмора в организации – явный признак того, что все там фундаментально убого. Хуже всего, что тогда группа становится уязвимой для конфликтов и расколов. 
Гоночные машины делаются быстрее за счет избавления от лишнего веса, а не увеличения мощности. Вы можете сделать ваше сообщество более легким, быстрым и гибким, строго следуя догме минимализма по отношению к вашей работе. Это может смахивать на леность, но часто бывает сложнее не делать что-то веселое, чем ввязаться в это без оглядки. 
Общее правило – делайте всегда тот минимум объема работ, которого будет достаточно, чтобы все работало. Остальное будете делать, когда люди начнут использовать вашу работу и жаловаться. Это относится как к вашему посевному проекту, так и к каждому изменению, которое вы вносите. Обратная связь – в большей степени, чем ваше собственное мнение, – лучший указатель того, где следует приложить усилия. 
СОВЕТ: Перфекционизм препятствует участию. Публикация на половину оконченной и содержащей баги работы – прекрасный способ привлечь людей к участию. Для больших эго это трудно принять, но изъяны лучше привлекают участников, чем совершенный труд, который привлекает пользователей. 
Культура минимализма может и должна распространиться в вашем сообществе. В прошлом мы обычно создавали юридические лица для серьезных проектов, чтобы была возможность владеть авторскими правами, торговыми марками и деньгами. Однако содержание юридических лиц дорого обходится и занимает много времени. Одна налоговая отчетность может стать непосильным бременем.
Одно из моих сообществ, Digistan, было разработано, развито и достигло своего результата (формирование нового поколения узаконенных шаблонов и политических аргументов для открытых стандартов) где-то за шесть месяцев. Все наши протоколы в ZeroMQ основываются на работе Digistan. Open Web Foundation, занимаясь теми же вопросами, потратило два года только на регистрацию юрлица, определение устава и выбор сотрудников. 
Если средств будет недостаточно, сообщество умрет от истощения. Если их будет слишком много, то, как я уже говорил, оно будет разлагаться. Здесь требуется чуткое равновесие. Мы можем мотивировать людей с помощью денег до определенной степени. После этого, только психопаты будут показывать пропорциональную реакцию. В этом и заключается дефект наивной теории капитализма о том, что «чем больше денег, тем лучше». В моем деле самыми вероломными оказывались те, кому я больше всего платил. 
Первое, что нужно сделать для сокращения расходов, – это отказаться от идеи с юрлицами, офисами и сотрудниками, если только вы правда в них не нуждаетесь. Они не только съедят любые ваши средства, но они будут препятствовать вашей работе по созданию только онлайн-сообщества. 
Второе: инвестируйте время и деньги в сообщество, только если нет другого выхода. Это может относиться к регистрации торговой марки, оплате хостинга или плате за выполнение такой работы, с которой больше никто не справится. И наконец: остерегайтесь людей, которые готовы взять на себе серьезные риски без требования соответствующего вознаграждения, – они склонны перегорать, о чем я расскажу в следующей главе. 
СОВЕТ: Каждый раз, собираясь потратить деньги на сообщество, поинтересуйтесь, не может ли кто-нибудь вам помочь с проблемой.
Продолжение следует...
Перевод книги «Социальная архитектура»: 
"К сожалению, мы не выбираем себе смерть, но мы можем встретить ее достойно, чтобы нас запомнили, как мужчин."
Питер Хинченс (Pieter Hintjens) – бельгийский разработчик, писатель. Занимал должность CEO и chief software designer в iMatix, компании, производящей free software, такие как библиотека ZeroMQ (библиотека берет на себя часть забот о буферизации данных, обслуживанию очередей, установлению и восстановлению соединений и прочие), OpenAMQ, Libero, GSL code generator, и веб-сервиса Xitami.
Подробнее тут: Тридцать пять лет я, как некромант, вдыхал жизнь в мертвое железо при помощи кода
Сайт Питера Хинченса
Статья в Википедии
Я, при поддержке Филтех-акселератора, планирую опубликовать на Хабре (и, может быть, в бумаге) перевод книги «Social Architecture». Имхо, это лучшее (если не единственное адекватное) пособие по управлению/построению/улучшению сообществ, ориентированных на создание продукта (а не на взаимный груминг или "поклонение" лидеру, спортклубу и пр).
Мысли и идеи Питера Хинченса на Хабре:
Если у вас есть на примете проекты/стартапы с высокой долей технологий, направленные на общественное благо в первую очередь и на получение прибыли как на вспомогательную функцию (например, как Википедия), пишите в личку или регистрируйтесь на программу акселератора.
Если вы пришлете ссылки на статьи, видео, курсы на Coursera по управлению/построению/улучшению сообществ, ориентированных на создание продукта, с меня шоколадка.
Строю реактивный ранец и конкурента Википедии
=====
Дайджест продуктового дизайна, ноябрь 2017
2017-12-07, 11:44
Дизайн-менеджер
=====
Что я узнал после 1000 code review
2017-12-07, 11:55
Пользователь
=====
Четыре релиза 1.0 от CNCF и главные анонсы про Kubernetes с KubeCon 2017
2017-12-08, 09:01
Open Source geek
=====
Созданы для ЦОД: новое поколение серверов Dell EMC PowerEdge и конвергентных систем
2017-12-07, 13:01
Команда блога Dell Technologies
=====
Улучшаем работу искусственного интеллекта в Nemesida WAF
2017-12-07, 13:37
Pentestit, CEO
=====
Как Фейсбук приобрел Инстаграм и почему это привело к открытию исходного кода React.js
2017-12-07, 16:23
Пользователь
=====
Двуликая локаль в преобразовании из строки в дробное
2017-12-07, 11:36
Программист С++
=====
Трудности обучения: как «лирику» подружиться с технологией
2017-12-07, 13:26
Пользователь
=====
Биткоин — коллос на глиняных ногах
2017-12-07, 12:05
Пользователь
=====
4 распространенные ошибки в дизайне, которые легко исправить
2017-12-07, 12:06
Предприниматель
=====
Сверточная сеть на python. Часть 2. Вывод формул для обучения модели
2017-12-18, 13:56
















































Пользователь
=====
Вечерний мехмат МГУ
2017-12-07, 18:32
User
=====
Как доставить интернет в отдаленные уголки планеты: проекты GCI, Google и Facebook
2017-12-07, 13:49
Пользователь
=====
Битва за сетевой нейтралитет: история вопроса
2017-12-13, 14:38
Пользователь
=====
«Притормози»: места на планете с самым «медленным интернетом»
2017-12-17, 16:16
Пользователь
=====
Анонс WPA3: Wi-Fi Alliance представил обновление безопасности
2018-01-11, 13:50
Пользователь
=====
От «Hello World» до приложения в App Store: советы новичкам
2017-12-07, 12:40
Материал может быть полезен для людей, которые хотели бы развиваться в сфере мобильной разработки на iOS.
Swift или Obj-C
По поводу того, что изучать из двух языков, сломано много копий. Не буду углубляться в теорию (поскольку не теоретик). Плюсы Swift: простота, ясность, отсутствие страшных квадратных скобочек повсюду. Плюсы Obj-C: используется в большинстве старых проектов больших компаний, а поскольку почти никто не берет на работу джуниоров, кроме больших компаний, шансы трудоустроиться со знанием Obj-C выше, если вы знаете только Swift. Минусы Swift: если вы используете open source библиотеки, то далеко не факт, что она заработает из коробки. Был даже случай, что библиотека FacebookLogin не компилировалась на одной из версий, пока разработчики аврально не пофиксили баг. Словом, Obj-C это океан стабильности, когда как Swift это бурлящее море.
Дизайн
Сама Apple советует уделять дизайну не менее 50% от всего времени, затраченного на проект. Это золотое правило, правда дизайн нужно понимать в более широком смысле, чем привыкло большинство людей — не только как макет, но и структуру всех процессов в приложении. В условиях жесточайшей конкуренции и больших рекламных бюджетов достаточно сложно придумать приложение с уникальным функционалом, поэтому дизайн и user experience — это наше все. Я установил на свой телефон первые 10 приложений со сходным функционалом и сделал анализ по большому количеству параметров, начиная от онбординга и описания процессов и заканчивая скоростью загрузки и весом бинарного файла (что очень важно, поскольку без Wi-Fi могут быть загружены приложения весом до 150 мегабайт). Этот анализ дал мне представление о том, каким должен быть проект, чтобы получить хотя бы какие-то загрузки.
Дизайн существующих приложений можно и нужно переосмыслять, ведь и многие русские писатели Золотого века переосмысляли европейцев, как например, Толстой делал с Гюго. При этом не самым лучшим решением является заимствование "сахарного" дизайна с Dribbble или Behance — там много хороших художников, но дизайн — это скорее про аналитику и утилитарность, чем про красоту и вычурность.
В идеале хорошо иметь макет перед началом разработки кода, но на практике это два параллельных процесса, и поэтому очень хорошо, когда разработчик сам владеет навыками работы в Sketch (или Affinity Designer, не так удобно, но тоже можно работать).
Архитектура
Когда проект создается не для себя, а для компании, то архитектура становится одной из немногих отдушин, в которых можно проявить творческий подход в узких рамках технического задания. Я проанализировал более 100 вакансий на Хедхантере и пришел к выводу, что большие проекты все больше склоняются к VIPER с различными вариациями. Диджитал-агентства любят придумать что-то свое. Но если проект ваш небольшой и вы работаете над ним в одиночку — не стоит бояться банального Model-View-Controller, разве что стоит добавить отдельный router, если приложение сетевое. Даже если Controller становится массивным — это не так плохо, если у вас все в порядке с логикой и неймингом функций и переменных. С другой стороны, есть мнение, что количество классов прямо влияет на скорость загрузки приложения. 
 Отправка в App Store 
Что касается непосредственно отправки в App Store, то нужно иметь в виду:
Пользователь
=====
Мастер-класс «Почему Стив Джобс любил шрифты» (Алексей Каптерев)
2017-12-07, 16:27

Привет, Хабр! Давно у нас в блоге не было расшифровок мастер-классов. Исправляемся. В этом посте вас ждет грандиозное путешествие в мир шрифтов от древнейших времен до наших дней. Если вы хотите понять, каким образом шрифты влияют на наши эмоции и наконец научиться отличать гуманистический гротеск от ленточной антиквы — добро пожаловать под кат. И да, там очень много картинок. Передаем слово автору.
Шутка, написанная гарнитурой Times, на 10 % смешнее той, что написана гарнитурой Arial. Почему? Чёрт знает. Лучшее объяснение, которое я видел: юмор ассоциируется с агрессией, с остротой, с остроумием — а Times выглядит более острым, чем Arial. 
Ещё один любопытный эксперимент, в котором участвовало 45 тыс. человек. Заходишь на сайт, тебе показывают статью Дэвида Дойча, британского физика. В статье автор пишет, что сегодня очень трудно внезапно умереть. Например, от инфекционного заболевания или в уличной драке. Лет сто назад это случалось намного чаще. Главный вывод статьи — сейчас мир безопасен как никогда. В среднем, конечно, ведь где-то постоянно идут локальные военные конфликты. 
И после прочтения статьи тебя спрашивали, согласен ли ты с утверждением, что мы живём в эру беспрецедентной безопасности? Варианты ответа: «да» и «нет». Следующий вопрос: насколько ты уверен в своём ответе? Варианты ответа: «не очень уверен», «довольно-таки уверен» и «полностью уверен». 
Подопытные не знали, что при заходе на сайт алгоритм случайным образом выбирает для них один из шести шрифтов. Авторы эксперимента хотели на большом количестве участников оценить влияние шрифтов на согласие с текстом и уверенность в ответе. 
Анализировались шрифты: 
Степень «убедительности» текста в зависимости от шрифта:
Между двумя крайними позициями разница в 5 %. Большинство людей вообще назовут Baskerville и Georgia одинаковыми шрифтами, однако по статистике один на 2 % «убедительнее» другого.
Если положить два текста рядом, то разница едва уловима. Но каким-то образом нашему мозгу один из них внушает больше доверия. Прямо 25-й кадр в действии! Вот в чём проявляется воздействие шрифтов на подсознание людей.
Вы слышали, что недавно Google поменяла два пикселя в логотипе? Учитывая, сколько людей заходят ежедневно на главную страницу поисковика и видят логотип, возможно, это не бессмысленный ход — потратить кучу денег на исследования и смену логотипа. Даже если вы не можете различить шрифты, они всё равно каким-то образом чуть-чуть на вас влияют. А уж если можете различить, то влияют довольно сильно.
Я не знаю, почему так происходит, как работает этот механизм. Для меня это проявление некой внутренней чувствительности. Типографика создает эмоциональную связь между автором и аудиторией. Эта связь возникает в любом случае, но её характер зависит от выбранной гарнитуры. 
Это можно назвать метакоммуникацией. Есть текст, а есть моё отношение к тому, что я пишу. 
Даже если взять подчёркнуто нейтральный, безликий шрифт, это тоже отношение. Только нейтральное, отстранённое. «Я не иду к вам в объятия»:
Здесь меня явно не любят, на меня хотят просто наорать:
Я часто бываю в Америке, там ситуация с гармоничностью шрифтов — в целом как у нас. Вот здесь человек явно хотел меня напугать, я к нему точно не приду.
В Европе со шрифтами получше. 
Типографика выражает отношение к аудитории. Вот пример объявления:
Пришедший за пивом развернётся и уйдёт во всех трёх случаях. Но в каком-то случае он вернётся потом обратно, а в каком-то нет. И нужно написать так, чтобы кому-то на подсознательном уровне понравилось.
Можно ли управлять влиянием типографики на людей? Я считаю, что можно, хотя и не до конца. Для меня это вопрос оптического разрешения. Чтобы можно было выделять тонкие детали, показывать какие-то мелочи. 
Вы смотрите, что-то различаете, а дальше спрашиваете себя: что я чувствую, когда вижу это? Затрагивает ли это в глубине души какие-то струны? 
От этой картинки некоторые люди почувствуют, насколько разный будет вкус кофе, и поймут, что ценник оправдан. 
Какой из этих логотипов Adidas ближе к настоящему? Сразу скажу, что все неправильные. У вас проблем не будет, все легко ответят, какой Adidas больше похож на реальный.
Здесь чуть сложнее определиться.
А вот здесь совсем сложно. Чувствуете, вроде что-то поймали, а потом всё больше и больше сомнений?
Вот настоящий:
Средний шрифт был взят за основу для логотипа и немного перерисован. Обычно логотипы так и делают: берут известный шрифт и изменяют какие-то мелкие элементы, чтобы логотип лучше работал.
А различить вот эти две фразы не способен почти никто.
Один из этих шрифтов называется Arial, а другой — Helvetica. О нём сняли фильм. Насколько мне известно, это единственный шрифт, о котором есть фильм, причём действительно интересный. Шрифтом Helvetica сделаны все эти логотипы. 
Про Arial говорят: 
«Нельзя создать хорошую типографику шрифтом Arial»
— Мэттью Баттерик
Я зашёл на сайт Артемия Лебедева, поискал по слову «Arial» и нашёл блестящий пример. 
В студию Лебедева прислали логотип на рецензию, и в ней написано:
«Шрифт Arial показывает плохое качество», «а вообще, есть простой способ оценить качество дизайна. Если используется Arial, дизайн плохой, нет — можно переходить к оценке».
Тут каждая буква чуть-чуть отличается, но буквально на пару пикселей. Например: 
И это разница между epic fail и epic win.
Почему, с какого перепугу? Дело в том, что если вы берёте два правила и применяете их к большому количеству букв, то результаты получаются совершенно разными. 
Ещё пример: где настоящая Lufthansa? 
Настоящая сверху. Посмотрите на сочетание F и T. «Quadratisch. Praktisch. Gut». Всё очень ровненько и по-немецки. А внизу какой-то бардак. 
Вверху — правильный Jeep, с квадратной челюстью, такой американский. А внизу — с открытым ртом. Не Jeep. Верхний вариант хороший, нижний — плохой. 
Надёжный признак провинциального дизайна — использование Arial в логотипах.
Для меня шрифты — это следы, оставляемые людьми. Сейчас шрифтами пользуются почти все, у кого есть компьютер. Я могу по этим следам что-то понять о человеке. Увидеть, что он говорит не то, о чём пишет. Это какое-то утерянное нами знание, позабытое. Мы перестали обращать внимание на мелкие подробности.
Давайте попробуем повысить оптическое разрешение. 
Думаю, все понимают разницу между печатным шрифтом и имитацией почерка. Почти все из вас различают шрифты с засечками и без засечек. А кто-то может различить антиквы и брусковые? Это две группы, на которые делятся шрифты с засечками. 
Антиквы делятся на старые, переходные и новые. Старые делятся на итало-французские и голландские.
Но должен сказать, что единой шрифтовой классификации не существует. В разных странах шрифты классифицируют по-разному. Я буду пользоваться своей, синтетической системой. 
Вопрос хороший, но ответить на него очень сложно. Красиво для кого? Для вас и для аудитории. Поэтому я буду использовать такую систему измерений. Стив Джобс в одной из презентаций говорил, что Apple — это компания, которая стоит на перёкрестке между технологиями и изящными искусствами. К технологиям ближе, но изящные искусства не забыты. И мы все находимся где-то на этом перекрестке, в этом диапазоне. Конечно, есть клинические технари и клинические гуманитарии. Но обе эти крайности уже давно госпитализированы. 
Я попробую вам показать, что у шрифтов тоже есть характер. Есть шрифты-технари, есть шрифты-гуманитарии. Может быть, вы выберете шрифт, который вам подходит по характеру. Или который отразит того, кем вы хотите быть в жизни
Вверху — идеализированный технарь, внизу — шрифт Pushkin, имитация почерка Пушкина. 
И второй перекрёсток — между старым и новым. Между бессмертной классикой и преходящей модой. Между прошлым и современным. Получается матрица 2 × 2. Посередине будут какие-то нормальные, читаемые шрифты, ровные пацаны, юристы, экономисты. Им, кроме бабла, ничего не нужно. Прагматики. 
Очевидно, что прагматичные шрифты мы используем для набора текста, шрифты с характером — для крупных надписей и логотипов. Вероятность совпадения этих двух задач очень мала. Есть очень мало шрифтов, которые и красивые, и хорошо читаются. Придумать и сделать такой шрифт — мечта каждого дизайнера. Очень редкий случай. 
Здесь один шрифт более читаемый, другой — менее. Если мы поменяем их местами и размерами, ситуация не изменится. 
Почему верхний нечитаем? Потому что красота — это повтор. Красота — это когда есть ровный, предсказуемый ритм. 
Посмотрите на буквы Ш и буквы И. В верхнем шрифте они одинаковые, а в нижнем шрифте разные. Поэтому нижний легко читается, а верхний — красивый. 
Можно сделать всё одинаково: было бы красиво, но не читалось. Есть какое-то простое правило повторения. Но если вы повторяете слишком часто, получается уже однообразно. Красиво — запоминается, практично — читается. Такой базовый конфликт существует в мире шрифтов.
Что хорошо для дорожного указателя, плохо для приглашения, и наоборот. Это очевидно. 
А здесь разница уже не так очевидна, но она есть. 
Один из шрифтов значительно лучше читается. Настолько, что в Америке потратили миллионы долларов на замену указателей по всей стране, перешли с одного шрифта на другой. Это нижний шрифт, он даже называется Clearview. Читается примерно на 12 % лучше, быстрее воспринимается. Люди реже пропускают свои повороты, меньше нервничают, меньше попадают в аварии. 
В Москве есть вот такое приключение: вы едете на машине, и вам нужно принять решение за три секунды — направо или налево?! А там развязка на Третьем кольце. 
Все буквы одинаковые, текст сливается в прямоугольный треугольник, и одно от другого не отличить. 
Классическое решение для таких случаев: 
За счёт выносного элемента у буквы Р нижнее слово начинает заметно отличаться от верхнего. Оно обретает другую визуальную форму, и люди лучше это считывают. Взрослые обычно воспринимают слово как иероглиф. Ты его уже видел раньше, и, опять встретив, сразу «глотаешь» знакомую форму. В верхнем варианте форма просто убита, всё одинаковое.
Если вам нужен «самый читаемый шрифт», то его нет. 
Если вы занимаетесь разработкой приложений, надо всё тестировать. Потому что на мобильных устройствах одни законы отображения текста, на настольных компьютерах — другие, на бумаге — третьи. То, что хорошо читается на указателе, будет плохо читаться в книге. Всегда лучше протестировать.
Но есть какие-то общие законы. 
Мы знаем, что на бумаге лучше читаются шрифты с засечками, на экране — без засечек. Мы также знаем, что переходная антиква и гуманистический гротеск читаются хорошо. А плохо читаются антиква нового стиля и геометрический гротеск. Если вас волнует читаемость, опирайтесь только на результаты тестирования. Соберите минимум 30 человек и встаньте у них над душой с секундомером. 
К сожалению, нельзя ограничиться одним экспериментом на все случаи жизни. Шрифты-прагматики скучные. Их цель: чтобы человек быстро прочитал и пошёл дальше. А всё остальное — интересное и запоминающееся. 
Я расскажу сначала об антиквах, у которых есть отчётливый исторический флёр. А потом — о гротесках, довольно современных шрифтах. 
Чтобы понять душу антиквы, прочувствовать её, нужно понять, что такое засечки. Для этого разберёмся, что такое буква. Вся современная письменность западной цивилизации базируется на финикийском алфавите. 
Обратите внимание: нет заглавных букв, только строчные. Никаких засечек, просто не до того. Мне кажется, что людей мучили такие проблемы: «Боже мой, нас всех сегодня убьют».
Потом появились римляне. У них было много свободного времени. Римляне писали на пергаменте тростниковым пером. 
Когда ведёшь тростниковое перо по бумаге, получается неравномерный штрих. И римляне стали писать красиво. Они начали добавлять декоративные штришки на концах букв для однородности, а значит, для красоты. Потом оказалось, что так ещё и удобнее читать. 
Возникают линии, по которым взгляду легко скользить не срываясь. Для бумаги и камня это оказалось очень важно. Получалось красиво, но медленно. Приходилось долго вырисовывать каждую букву. Поэтому текста было немного. 
То есть шрифты, которыми мы пользуемся, это наследники тростникового пера, той архаичной технологии. 
Очень долго — примерно девять веков — их не было. Но текстов становилось всё больше. А если долго и много пишешь, то из заглавных букв постепенно образуются строчные. 
Где-то сразу ясно, из каких заглавных букв какие строчные образовались. Где-то менее очевидно, но угадать можно. 
А где-то очень трудно понять, как образовывались строчные буквы.
Прошло 15 веков. По всей Европе писали вот так: 
В Италии чуть-чуть иначе, а в Германии, Англии, Франции писали готикой. Она очень красивая и довольно удобная для письма. Одна-единственная проблема: абсолютно нечитабельна. 
У Гутенберга уже появились строчные буквы. Заглавные буквы написаны красным. А потом был француз Николя Жансон, придумавший современную книгу. 
Его послал учиться к Гутенбергу французский король. Но когда Жансон приехал к Гутенбергу, у того уже отобрали печатный пресс. Так что, по всей видимости, у Гутенберга Жансон не учился, но много времени провел в Майнце и научился довольно прилично печатать. Даже создал собственные шрифты. 
Потом французский король умер, и Жансон решил не возвращаться, а уехать в Венецию. Там начинался Ренессанс, капитализм. Жансон взял рукописный почерк девятого века (в то время у Карла Великого был маленький Ренессанс), в котором встречались только строчные буквы...
… и добавил римские заглавные. 
К слову, Жансон был не первым, кто пытался провернуть такой трюк, но ему первому улыбнулась коммерческая удача.
Мне интересно, как он это придумал. «Давайте возьмём буквы, которые разделены девятью веками, просто соединим их и посмотрим, что получится». А получилась красота, которой мы до сих пор пользуемся. 
Вот пример ротунды, сделанной Жансоном: 
В основном ему заказывали исторические и церковные тексты. Сегодня есть очень похожие шрифты, например Adobe Jenson — оцифрованный шрифт пятнадцатого века, но без кириллицы. Она есть в русской версии шрифта, которая называется Centaur, или Venetian 301. Вы получаете весь вот этот флёр Ренессанса, той новизны, вечного исторического панка. 
Курсив — это актуальный во времена Жансона итальянский почерк. В Италии в то время писали так:
Это почерк жившего тогда человека по имени Никколо де Никколи. Обратите внимание: заглавных букв нет, только строчные. Кто-то взял его почерк и добавил заглавные буквы. А наклонные заглавные буквы придумали только 50 лет спустя во Франции. В ту эпоху так печатали поэзию.
Почему все эти шрифты называются «антиква»? Потому что таким рукописным почерком переписывали античных авторов. У антиквы вечный исторический флёр. 
Посмотрите на конструкцию буквы G. Я взял фрагмент изображения колонны императора Траяна. Видно, что потом G начала меняться, у неё появилась вертикальная ножка. А вот снимок из интернета: человек явно не разбирался, просто взял скучный газетный шрифт и пустил в производство. 
Где настоящая Wikipedia? 
Средняя. У неё характерная конструкция буквы W. Это был очень разумный, осмысленный выбор: раз мы энциклопедия, нам нужна энергия Возрождения, когда начался расцвет наук. 
А это выбор компании, которая хочет быть «гуманитарием среди технарей», которая очень много работает с образованием, заботится о красоте. И десять лет в рекламе Apple применялась антиква старого стиля, шрифт Garamond. Кстати, первый логотип Apple был очень угловатым, рубленым. Этакий робот. А потом они решили нести красоту в массы. 
Обратите внимание, что антиква времён Ренессанса невероятно корява. Дело в том, что она очень много унаследовала от рукописного текста, да и отливать типографские гарнитуры в то время умели плохо. Если вам нужно найти антикву именно старого стиля, а не нового, то наложите равнобедренный треугольник внутрь буквы А, если он ляжет с зазорами — это старый стиль. 
Потом пришли люди с линейками. Так происходит: как только художники что-то делают и кому-то это нравится, начинает продаваться, тут же приходят люди с линейками и начинают всё оптимизировать. Одним из них был францисканский монах Фра Лука Бартоломео де Пачоли, который изобрёл двойную запись, дебет и кредит. 
На ниве оптимизации шрифтов отметился Леонардо да Винчи. 
Дюрер очень много пытался конструировать буквы, искал идеальные пропорции. Как раз в те времена придумали золотое сечение.
Но всё это не сработало, не прошло проверку временем. Шрифты получились некрасивыми, ими никто не пользуется. Всё оказалось не так просто. Оптимизаторы хотели простого решения, а его не было. Постепенно технологии улучшились, начертание стало поровнее, но шрифты не изменились. Лишь стали чётче.
Следующая большая революция в типографике связана с именем Джона Баскервилла, художника и бизнесмена. 
Он очень долго учился каллиграфии, потом владел типографией. По мнению многих людей того времени, он делал лучшие шрифты в Европе. 
В первой букве ещё есть какие-то признаки «перьевого» прошлого, а вторая — совсем другая буква. В ней нет ни грамма рукописности. Это чистая технология. Тогда появились газеты, и требовалось, чтобы в строку влезало больше текста. Шрифты становились у́же и практичнее. Главное, чтобы читалось. Баскервилл всё заострил, сделал засечки отчётливее. До него это тоже делали, но не так хорошо.
Это два компьютерных шрифта, но верхний — рукопись, нижний — печать. 
В целом нижний текст читается лучше. Верхний более интересный, историчный, тёплый, душевный. Нижний — холодный, расчётливый, чёткий. Таких шрифтов довольно много. 
Century Schoolbook — это первый шрифт, который многие люди видят в жизни, потому что им набирают детские азбуки и книги. Он специально сделан для детей и очень читаемый. 
Люди с линейкой не остановились и добились успеха. У них получилась идеальная буква К. Ничего лишнего, всё очень четко. Автора звали Джамбаттиста Бодони. Кстати, он делал и кириллицу. Смотрите, какие яркие буквы. Очень много ненужных украшений, некоторая показушность. 
Слева Baskerville, справа Bodoni:
В Bodoni подчёркнуто красивые буквы. Такие прямо: «Посмотрите на меня». А Baskerville — рабочая лошадка. 
В шрифте понятие «контраст» означает разницу толщины внутренних элементов буквы. 
Джамбаттиста Бодони просто экстремально задрал контраст. И сегодня мы это видим на каждом шагу в газетных киосках, в высокой моде, на театральных афишах. 
Получилось очень холодно. Ренессанс — это тепло, душевно и гуманно, а высококонтрастные шрифты — все по линейке и лекалам, очень отстранённо. Вы заходите в дорогой магазин, и сразу видно, что вас там не ждут. И здесь такое же ощущение: «Отойди». Зато красиво. Вот первый выпуск журнала Vogue, конец девятнадцатого века, шрифт вообще ничем не отличается. 
Где настоящий Armani? 
Внизу. В кои-то веки я могу это легко объяснить. У Armani не такое настроение, как у Gucci. Gucci развратен, а Armani такой: «Чувак, отойди. Я красивый». Gucci смотрит вам в глаза. А в визуальных материалах Armani очень часто используются чёрно-белые снимки, идеально отстранённые образы, взгляд в даль. Это совсем другое настроение, другой шрифт. Всё очень продуманно. 
Теперь вы знаете, чем различаются старый стиль, новый и переходный. Самая корявая буква — это старый. Самая красивая — новый. Ни то, ни сё — переходная антиква. То есть посередине скучный, газетный шрифт, слева — тёплая ламповая красота, а справа — холодная цифровая. 
Где гуманистический, где переходный, а где шрифт нового стиля?
Самый высокий контраст у третьей буквы: сочетание очень широких и очень узких элементов. Это новый стиль. Самая корявая буква — вторая, старый стиль. Первая — ни то, ни сё, переходная антиква.
А как бы вы классифицировали это? 
Такая задача сегодня часто возникает в банковском секторе. Вы — банк, существующий с восемнадцатого века. С одной стороны, вам нужно транслировать надёжность, стабильность и историчность, что вы тут на века. С другой стороны — что вы удобны для молодёжи, что у вас онлайн-кабинет. Как увязать два этих противоположных послания? К слову, в сфере недвижимости такое же затруднение. 
Для таких случаев совсем недавно придумали ещё одну группу шрифтов. У них пока нет устоявшегося названия. Их очень условно называют «антиквы с треугольными засечками», хотя по факту засечки не треугольные, а скорее трапециевидные. 
В каком-то смысле это возврат к старым формам, попытка геометризировать первый шрифт, сделать его лучше. 
В первом случае нет засечки, потом засечка, потом засечка, и потом опять нет засечки: возврат к старому, но в более аккуратной, геометрической форме. 
Таких шрифтов довольно много. Если у вас на компьютере есть Word, то можете взять шрифт Constantia. Сбербанк пользуется Fedra. Ещё очень популярен Centro, бывший Agora. 
Антиква используется в следующих сферах:
История гротеска связана с именем Уильяма Кэзлона IV, правнука очень знаменитого типографа. Правнук решил сделать вот такой шрифт без засечек. 
Его очень сильно критиковали, и, честно говоря, действительно получилось довольно коряво. Мужчина реально ничего не понимал в шрифтах. Но постепенно он совершенствовал своё детище. 
Сначала такие шрифты называли «египетские», потому что по времени их появление совпало с египетским походом Наполеона. Они были ужасны. Я смотрю на эти шрифты и думаю: «Блин, кто так делает?» Мне трудно объяснить, почему. Просто какое-то общее отвращение. Посмотрите на А: кто её бил так долго, что она такая вся измученная? 
В то время в порядке вещей была антиква, люди привыкли к тому, что буква красива. А тут им вываливают такое. 
Создатели подобных шрифтов просто брали антикву и делали её моноконтрастной, оставляя конфигурацию без изменений. Выглядело очень странно. Публика дала этим группам шрифтов такие названия: Grotesque, то есть нелепо гротескный, и Gothic, то есть готский (а не готический), варварский. 
Потом началась промышленная революция, возникла потребность в инженерах. А инженерам нужны были чертежи, куда прекрасно ложились новые шрифты, антиквы на чертежах совершенно не работают, ничего не разобрать. И вторым «потребителем» гротескных шрифтов стала реклама. Вот рекламное объявление восемнадцатого века о продаже рабов. 
При всём уважении и любви к антиква-шрифтам, у них мало вариативности. Всё одинаковое. А с гротесками стало интереснее. Во-первых, людей перестали продавать. Во-вторых, читабельность получше. Но гротескные шрифты использовались только для заголовков. Никто не думал набирать текст такими шрифтами. 
Сегодня есть три группы гротесков:
Первые гротески выглядели так:
Мы этим до сих пор пользуемся. По большей части они применялись для подобных задач:
Затем в России произошла революция 1917 года. Возникло новое течение в шрифтах, исключительно русское веяние. Сначала от руки, очень аккуратно, нарисовали две буквы. Постепенно это превратилось в шрифт. Так в историю вошло понятие «русская типографическая революция»: супрематизм, дадаизм и прочее. «Давайте мы возьмём прямоугольник, и все буквы у нас будут как прямоугольник». Получилось интересно. И такого наделано довольно много. 
Кстати, школа «Сколково» до сих пор пользуется шрифтом Rodchenko.
В Германии Дюрер давно умер, но люди с линейками остались. Они сказали: прямоугольник нам не подходит, возьмём овал. Точнее, давайте всё сведём к форме стадиона.
Это шрифт DIN, создан в Германии в 1923 году для железнодорожных указателей. До сих пор жив. Благодаря DIN в латинице случилась своя революция. Люди поняли, что буква не обязана быть такой, какой её нарисовали в девятом или в четырнадцатом веке. Люди решили, что у них другой шрифт, нужна другая буква. Избавились от всех лишних элементов: засечек, хвостиков, недохвостиков. Максимальное упрощение, максимальная геометризация. 
Этот шрифт прошёл проверку временем. Выглядит очень современно. Нет ощущения старости, а ему реально сотня лет. 
Кстати, первую кириллическую версию этого шрифта сделали в 1943 году немцы для оккупированных территорий, потому что им потребовались железнодорожные указатели. 
Franklin Gothic Medium более традиционный, FF DIN Bold — более современный. Хочу ещё напомнить, что в Германии в то время возникло движение Bauhaus. Один из его адептов Пауль Реннер нарисовал шрифт Futura. Это очень красивый шрифт, в нём всё стремится к кругу. 
Futura до сих пор пользуется огромной популярностью. Немцы наделали уйму таких шрифтов. 
Сначала у самих немцев эти шрифты ассоциировались с русскими. Вот пародия на старый советский плакат, анонс выставки дегенеративного искусства, «жидобольшевистского»: bolschewismus und jüdischer. 
Немцы говорили, что это всё чужое, однако пользовались шрифтами очень охотно, в том числе и в пропаганде. Потому что это модернизм, некий идеал, стремление к упрощению, к рационализации. 
В нацистских плакатах было два шрифта: готическая немецкая фрактура и вот такой модерн. Читать почти невозможно, но в большом, плакатном размере работает очень хорошо.
Здесь написано: «Немец, пей немецкое вино».
Таких шрифтов очень много у вас на компьютере. Например, Century Gothic, который стал прототипом для логотипа Adidas. В одной из последних версий macOS есть очень красивый шрифт Avenir. Он был создан уже после войны, кажется, в 1970-х годах. 
Мы видим возвращение нормальной буквы А, но все остальные буквы по-прежнему стремятся к кругу. В 1960-е годы были популярны шрифты вроде Phosphate. Такой хипстерский ретростиль. Напишите на ценнике «бургер», и он будет стоить не сто рублей, а триста.
Германия, Россия, Америка:
В Америке решили, что им нравится не прямоугольник, а квадрат, но со скруглёнными краями. Хотя в целом идея та же.
Где настоящая Toshiba? 
Подсказка: там, где более геометрический шрифт. Сверху. У японцев вообще очень плохо с типографикой, потому что она для них чужая, западная типографика. Но в данном случае это неплохой выбор. Правильная Toshiba выглядит более техногенно, что ли. Нижний шрифт — это Arial. Мы помним, что нельзя делать хорошую типографику шрифтом Arial. 
После войны все поняли, что экстремальная геометрия в шрифтах — это очень негуманно. Слишком ровно. А люди не такие. И начался откат. Старый гротеск перерисовали в новый. 
Он точно такой же, просто красивый. Например, вернули букве а хвостик. 
Слева коряво, а справа — люди старались, просто идеальная буква. Обратите внимание на пространство внутри «пузика», это капля. А у левой буквы — смотреть невозможно.
До появления Helvetica среднестатистическая реклама выглядела как-то так:
А через десять лет после Helvetica всё поменялось. Стало намного чётче, точнее, жёстче, по делу, очень кратко. 
Шрифт Helvetica изменил плакаты. Сначала он воцарился в шрифтовых плакатах, а потом к ним добавили картинки. Сейчас в IKEA вы увидите такое:
IKEA — это современный, молодежный дизайн. Швейцарский дизайн 1960-х годов, до сих пор жив. 
А потом Джобс засунул Helvetica в наш карман. Интерфейсный шрифт на iPhone — это Helvetica. После Helvetica случилось только одно важное событие — возник гуманистический гротеск. 
Во всём мире рисовали геометрические шрифты. А в Англии люди подумали, что им не нравится геометрия. Они живут в бывшей великой империи. Они хотят возврата к Ренессансу. Люди опять взяли перья и попытались сделать шрифты без засечек, отталкиваясь от перьевого начертания. 
Видите, какое странное начало у буквы а? Это перо. Никто не проникся, кроме англичан. Они сказали, что обожают это и будут лепить везде, где только можно и нельзя. А весь остальной мир послал их на фиг и остался верен геометрии. 
У Джонстона, создавшего новый английский шрифт, был ученик Эрик Гилл. Он пытался что-то геометризировать. 
Но, по сути, он геометризировал антикву времён Ренессанса. То есть вернулся во времена Жансона и попытался заново перерисовать старые буквы, но не из девятого века, а из пятнадцатого. И получилось интересно. 
Чуть нелепая форма, но тёплая, ламповая, живая, прикольная. В Англии этот шрифт используется везде. А мир не оценил. 
Потом Эрик Шпикерманн, долго живший в Лондоне и проникшийся британскими шрифтовыми идеями, вернулся в Германию и сделал очень крутой шрифт, который сильно всё поменял. Это шрифт Meta. 
Почему Meta относится к гуманистическому гротеску? В фильме «Helvetica» Шпикерманн говорит: «Настоящему шрифту нужен ритм, нужен контраст, который идёт от рукописного текста… А у Helvetica нет ничего из этого». Шпикерманн ненавидит Helvetica. Его спросили, почему же этот шрифт так популярен? Шпикерманн ответил, что дурной вкус вездесущ. 
Эрик вернул старую форму букве g, но нарисовал её «правильно». 
В Helvetica очень ровная, нейтральная буква Р. А в Meta у вертикальной палочки два обрыва на концах: наверху и внизу. В этом нет никакого тайного смысла, просто перо неровно шло, а потом оборвалось. И в целом буква у́же, потому что рукописный почерк узкий, он не стремится растянуться. Всё это признаки гуманистического шрифта. Все гуманистические шрифты будут чуть-чуть шизанутыми. Буквы начинают отличаться друг от друга мелкими штрихами, и улучшается читаемость. 
Оказывается, что все предыдущие века развития шрифтов — не бессмысленны. Удалось создать действительно легкочитаемый шрифт. Все буквы немного срезанные, совсем другой рисунок. А Helvetica очень правильная, ровная. Сбербанк обожает Meta. 
Продуктовый шрифт Apple, которым они всё брендируют, называется Myriad Pro — это тоже гуманистический гротеск. Достаточно легко объяснить, почему. Смотрите, буква U чуть неровная. Почему? Перо так шло. 
Вот дорожный американский указатель. Здесь буква l, которая заметно отличается от буквы i. И в целом более узкие буквы и более широкие апертуры. 
Если смоделировать в Photoshop плохое зрение, то разница в читаемости шрифтов станет очевидной:
Apple обожала шрифты гуманистического гротеска. Впрочем, все интерфейсные дизайнеры их обожают. 
Слева — первый шрифт на Mac, потом ставший шрифтом на iPod, посередине и справа — второй и третий шрифты на Mac. Кстати, на Windows сейчас другой шрифт, но очень долго там тоже использовался Lucida. Очень читаемый шрифт, хорошо выглядит на плохих экранах. 
Вот iPod:
Экран плохой, можно было сделать шрифт формальным, но подошли с фантазией. Внутри есть вариативность на один пиксель, и хорошо читается, хорошо выглядит. 
МТС сейчас:
Segoe — интерфейсный шрифт на Windows. Droid Sans — это на Android. Ubuntu — это на Ubuntu. 
Все шрифты на Windows — это гуманистические гротески: Tahoma, Verdana, Corbel. 
Мы получаем три группы: технари, прагматики и гуманитарии. Первые — холодные и оптимальные, последние — тёплые и ламповые. 
Где настоящий iPhone? 
Посмотрите на букву P. Видите, она неровная сверху? Характерный признак гуманистического гротеска. На букве R очень хорошо видно. 
У геометрического гротеска почти всегда будет просто ровная нога вниз. Абсолютно без фантазии. У гуманистического — неровность, изгиб. 
У Arial тоже небольшой скос в начале. Но у Myriad есть наверху, у чаши, и ещё один в самом низу косой ножки. Зачем? А вот так вот. Чуть изящнее. И красиво, и лучше читается. И глаз это как-то замечает, и буквы, конечно, будут у́же. 
Есть шрифты брусковые. Почему брусковый — понятно: потому что весь как из брусков сложен. Такие шрифты появились в девятнадцатом веке, просто утолщили антиквы. Обычно их используют в рекламе для максимальной передачи эмоций, крика, сенсации.
Был когда-то немец Герман Цапф, каллиграф. Он рисовал очень красивые шрифты. Потом что-то на Цапфа нашло, и он отрезал засечки. И получилась ленточная антиква, как ленту положили. 
Многие говорят, что это гуманистический гротеск, но мне кажется, что это ленточная антиква. 
Если вы хотите что-нибудь прочитать про кириллические буквы, то есть книжка Юрия Гордона «Книга про буквы от Аа до Яя». Она толстая, большая и очень красивая. Там много о кириллице. У кириллицы своя судьба, вечно какая-то странная. Если вы прагматично настроенный человек, то можно почитать книгу «Типографика для юристов» (Typography for Lawyers) (сайт и книга). Там разумные рекомендации по выбору шрифтов и всё такое. То есть она просто написана дизайнером для дизайнеров. 
Если вам нравится какой-то шрифт, можете поставить себе приложение WhatTheFont. Для iPhone точно есть, для Android — не знаю. Фотографируете текст, программа отправляет его на сервер и возвращает ответ, какой это шрифт. 
Если вы хотите сочетать шрифты между собой, рекомендую зайти на сайт Fonts in use и поискать тот шрифт, что вам нравится. Если его там нет, не пользуйтесь им. Скорее всего, он плохой. 
Наконец, есть сайт Font Squirrel с кучей бесплатных шрифтов. Там есть поиск по кириллице. 
И последнее. CEO Samsung сказал: «Дизайн — большой приоритет для меня, потому что это то, что дифференцирует мой бизнес». На мой взгляд, поэтому Samsung всегда будет № 2 после Apple. Потому что Джобс так никогда не думал. Он просто любил, чтобы было красиво. Понимаете разницу? Там ищут смысл, а ему просто нравилось. Человек, которому просто нравится, всегда окажется на шаг впереди, потому что он будет делать бессмысленные вещи, которые выстреливают в долгосрочной перспективе. Мне кажется, это очень важно.
Видеозапись мастер-класса:
Пользователь
=====
Топ 10 примеров и антипримеров взаимодействия «разработчик- аудитория»: Часть первая
2017-12-07, 14:03
Разработчик Unity
=====
Новая жизнь старой пагинации
2017-12-07, 13:11
Ведущий продуктовый дизайнер
=====
10 лучших бесплатных CRM систем для бизнеса
2017-12-07, 14:11
Пользователь
=====
Ростелеком: чужие счета — это не спам
2017-12-07, 14:17
User
=====
Когда дворецкий — жертва
2017-12-07, 14:41





Project Manager
=====
Двоичный поиск в графах
2017-12-13, 11:03


Переводчик-фрилансер
=====
Kotlin Night Moscow — видео, фото, презентации
2017-12-07, 14:54
Вот и прошла первая встреча Kotlin Night в Москве при поддержке JetBrains. В офисе Avito собрался полный зал тех, кто интересуется этим языком программирования, ещё несколько сотен разработчиков присоединились к видеотрансляции, а Belarus Kotlin User Group даже организовали совместный просмотр митапа. Встреча удалась! Под катом делимся видеозаписями выступлений, ссылками на фотоотчёт и слайды. 

Александр Тарасов рассказал, почему для автоматизации экспериментов был выбран Kotlin, а не классические инструменты управления конфигурацией (такие, как Ansible), почему хороший DSL и инструментарий критически важны для этой задачи и какие проблемы пришлось преодолеть, чтобы всё работало, как изначально задумано.
Как сократить количество boilerplate-кода при помощи таких средств языка Kotlin, как интерфейсы с частичной реализацией и делегаты классов? Как с их помощью можно имитировать примеси (mixins) и какие изменения в языке могли бы существенно улучшить текущую ситуацию? Смотрите доклад! 
Доклад о возможностях и развитии технологии Kotlin Native, а также о перспективах продуктовой мультиплатформенной разработки, охватывающей backend, Android, iOS и web.
В докладе речь пойдёт о проблематике, назначении, области использования, примерах известных монад и чек-листе того, что необходимо, чтобы использовать их в работе с Kotlin. А также тех минусах, которые надо учесть при работе с монадами.
Спасибо всем, кто пришёл на встречу, посмотрел видео, прочитал этот пост.
Фотоотчёт мы выложили в Facebook.
Благодарим за поддержку компанию JetBrains. Следите за событиями, посвященными Kotlin, на официальной странице Kotlinlang.org, подписывайтесь на наш Timepad, чтобы раньше всех видеть анонсы мероприятий, которые Avito проводит для разработчиков.
И до новых встреч!
Пользователь
=====
Опрос для пользователей браузера Vivaldi
2017-12-07, 14:51
Зрю в корень, жгу глаголом
=====
Расширение Visual Studio для визуализации экземпляров пользовательских классов в режиме отладки. Часть 2
2017-12-18, 13:17
User
=====
Список полезных идей для высоконагруженных сервисов
2017-12-07, 15:53
Программист, в основном на Go
=====
Слои, Луковицы, Гексогоны, Порты и Адаптеры — всё это об одном
2017-12-07, 17:03
TeamLead | Backend Developer
=====
TeamCity 2017.2: 100 бесплатных билд конфигураций, Docker, .NET CLI, композитные билды и улучшения в Kotlin DSL
2017-12-07, 17:49
Пользователь
=====
Как мы учили умную розетку замерять мощность
2017-12-12, 11:45
Приветствуем! 
Мы — компания Rubetek, занимаемся разработками в области решений для умного дома.
В этой статье расскажем о том, как в ходе разработки одного из устройств нашей Wi-Fi линейки выбирали решение для точного измерения мощности подключенных электроприборов.

Наша Wi-Fi розетка
Что за девайс делали?

Wi-Fi розетка Rubetek — это устройство, предназначенное для удалённого управления электропитанием подключённых электроприборов мощностью до 3000 Вт. Управление розеткой осуществляется с помощью расположенной на корпусе кнопки, локально (смартфон — WiFi розетка, смартфон — WiFi роутер — Wi-Fi-розетка) и удаленно при подключении роутера к Интернету.
Состояние устройства изменяется по срабатыванию подключенных к розетке RF датчиков или с помощью заранее настроенных пользователем сценариев. 
Одна из ключевых функций устройства — контроль потребления электроэнергии подключенной нагрузки.
Для отображения уровня нагрузки розетка использует светодиодный индикатор, изменяющий цвет свечения в соответствии с мощностью, потребляемой нагрузкой.
Устройство позволяет подключать совместимые датчики с протоколом EV1527 (433.92 МГц, ASK) RF датчики (движения, открытия, протечки воды, утечки газа и дыма) и создавать сценарии взаимодействия между подключёнными устройствами.
Помимо этого, умная розетка совместима с платформой Apple Homekit и поддерживает управление с помощью голосового помощника Siri. Удаленное управление Homekit через интернет возможно только с помощью Apple TV, но локально управлять голосом можно с iPhone и без использования Apple TV. 
Первые попытки
На первом этапе основой для будущей розетки был выбран модуль Z-Wave (рис. 2).
Согласно техзаданию, розетка должна была измерять потребляемую электроэнергию в однофазных цепях, в диапазоне от 0 Вт до 3000 Вт., с погрешностью не более 1%. 

Первый прототип на модуле Z-Wave 
Расчет активной мощности, потребляемой нагрузкой, считался по формуле: 

где:
P — активная мощность, потребляемая нагрузкой
U — эффект. напряжения на нагрузке, равно напряжению сети
I — эффект. сила тока нагрузки
φ — фазовый сдвиг между током и напряжением на нагрузке
При разработке Wi-Fi розетки одним из главных требований была компактность изделия, поэтому использование нескольких корпусов микросхем усилителей и АЦП мы посчитали нерациональным. К тому же аналоговый сигнал на выходе усилителей был сильно “зашумлен”, уровень помех не позволял добиться заданной погрешности измерения мощности, потребляемой нагрузкой. Особенно это было заметно на усилителе, работающем в диапазоне от 0 Вт до 100 Вт. (рис.3). 

Осциллограмма с токового шунта АЦП Z-Wave модуля при мощности 10 Вт нагрузки
На этом этапе, для увеличения точности измерений и увеличения степени интеграции, необходимо было найти новое решение реализации измерения мощности. Было принято решение отказаться от модуля Z-Wave, из-за высокой стоимости и низкой производительности, и ОУ. Новой основой стал Wi-Fi MCU. 
Отказ от Z-Wave в пользу Wi-Fi был обусловлен очевидными преимуществами последнего:
наличие Wi-Fi в любом смартфоне, что позволяет непосредственно управлять розеткой из приложения, возможность подключения к роутеру для удаленного управления розеткой из любой точки мира. Это и определило дальнейший путь развития линейки наших умных устройств. 
В поисках решения 
Поиск вариантов реализации измерения мощности привел нас к специализированным измерительным RMS микросхемам.
Выбрали относительно недорогую RMS — STPM14A.

Прототип STPM14A модуля 
Данная RMS измеряет активную, реактивную и полную мощность, действующие значения тока, напряжения и частоты, поддерживает токовые трансформаторы и шунты, отличается низким энергопотреблением, высокой точностью измерений, надежностью передачи и хранения данных. В общем, STPM14 подходила нам по многим параметрам. 

Структурная схема STPM14A
Но не обошлось без минусов. 
При использовании STPM14A возникли трудности на стадии обработки данных: у этой измерительной микросхемы не имелось цифрового выхода, только частотный, что потребовало от Wi-Fi MCU ресурсов для обработки частотного сигнала. Наш Wi-Fi MCU и так был загружен массой других функций: обработка дискретных сигналов, взаимодействие с сервером, шифрование, таймеры, скрипты. Получилось так, что обработку частотного сигнала не было возможности реализовать без последствий для остального функционала. 
Нам пришлось вернуться к поиску альтернативных решений еще и потому, что STPM14A не имела цифрового выхода. Изучив несколько вариантов, мы решили остановиться на микросхеме STPM32. К Wi-Fi MCU STPM32 подключалась по UART. 
STPM32 представляет собой устройство обработки смешанных сигналов. Процессор состоит из аналогового и цифрового блоков. 
Аналоговый включает в себя: 
Цифровой блок состоит из каскада цифровой фильтрации, аппаратного цифрового сигнального процессора, буферных каскадов для входных сигналов и последовательных интерфейсов (UART или SPI).
Измерительная микросхема STPM32 содержала в себе два канала обработки аналоговых сигналов тока и напряжения, с последующим преобразованием в цифровой сигнал и высокоточный цифровой сигнальный процессор DSP, предназначенный для обработки оцифрованных сигналов в режиме реального времени. 
Главным и самым заметным недостатком, на фоне всех перечисленных преимуществ, оказалось то, что при работе с малыми сигналами, при максимальном усилении (16х), повышалась погрешность измерения тока. 

Структурная схема STPM32 
Как тестили?
Для проверки работоспособности и проведения тестирования мы разработали макетную плату.

А так выглядит прототип измерения мощности на STPM32
Тестирование происходило на различной нагрузке мощностью от нескольких единиц ватт, до 3,5кВт. В качестве нагрузки использовались лампы накаливания, ТЭНы (резистивная нагрузка), люминесцентные светильники и электроинструмент (резистивно-индуктивная нагрузка), электроприборы с импульсными источниками питания без корректора коэффициента мощности (резистивно-емкостная нагрузка).
В ходе макетирования на отладочной плате, мы получили положительные результаты, которые полностью нас устроили. STPM32 хорошо справлялась с поставленной задачей и могла быть использована в нашей умной розетке.
Уже позже выяснилось, что суммарная погрешность измерений устройства не соответствовала реальным показаниям. Эти данные мы получили посредством измерения показателей несколькими бытовыми ваттметрами разных ценовых категорий.
Для достижения заданной точности измерения мощности нагрузки решено было калибровать серийные изделия на этапе производства. Для этого был спроектирован и изготовлен специальный стенд (рис.6), который содержал в себе отдельный, откалиброванный с помощью эталонного измерителя мощности STPM32.
Стенд имел гальваническую развязку от сети для того, чтобы обеспечить электробезопасность. 
Каждое изделие после сборки подключалось к контактному устройству стенда.
Оператор запускал программу, которая обеспечивала электрическое тестирование розетки на предмет отсутствия ошибок при монтаже и прошивке. Помимо этого тестировался основной функционал розетки и происходила калибровка STPM32. 

Калибровочный стенд 
Использование стенда обеспечило высокую точность измерения мощности, технологичность и надежность изделия.
В процессе прошивки розетки закладывалась также цветовая палитра (пропорциональное соотношение красного, синего и зеленого цветов). Эта цветовая шкала используется в розетке светодиодным ободком для отображения мощности, потребляемой подключенной к устройству нагрузкой. 
Что в итоге? 
Это была первая наша попытка реализации подобного функционала. Несмотря на то, что вариантов мы перебрали немало, положительная сторона в этом тоже найдется — опытным путем мы определились с необходимой для работы микросхемой, изучив плюсы и минусы всех образцов.
Наша же Wi-Fi розетка, получив возможность замерять мощность, из обычного девайса, способного удаленно управлять электропитанием, превратилась в умное устройство, существенно облегчающее жизнь пользователя.
Впереди еще много работы, выше мы упоминали о целой Wi-Fi линейке, разработка которой ведется прямо сейчас. Помимо розетки для покупки уже доступен, например, Модуль управления. К выходу готовятся многие другие умные устройства.
О них мы непременно расскажем в следующих статьях.
Пользователь
=====
Сбои в работе сайта. Как эффективно организовать поддержку веб-ресурсов сторонними сервисами
2018-02-27, 09:28
User
=====
Как я придумывал и применял формат хранения результатов экспериментов Measurelook
2017-12-07, 19:26
В силу специфики научной деятельности мне нужно замерять время работы алгоритмов и строить по получившимся данным графики. Раньше процесс выглядел так: 
Первая проблема — просто посмотреть как прошел эксперимент занимало очень много времени.
Ладно, пережили, графики построили, время идет, готовим публикацию и выясняется, что в результатах экспериментов не сохранены некоторые параметры запуска алгоритма. Не доглядел. Это уже вторая проблема — хранение метаданных об эксперименте.
Меня как программиста всегда раздражала необходимость «ручной работы». Да график готов, но мы еще что-то вручную подвинем, там перекрасим, тут подрисуем. Каждый раз когда приходят новые данные этот процесс приходится повторять. Третья проблема — перестроение графиков должно быть полностью автоматизированным.
Для решения озвученных проблем я придумал формат хранения данных в JSON и назвал его Measurelook. В этой статье я расскажу о Measurelook и о его применении в подготовке научной публикации.

Формат и просмотровщик это два ключевых компонента Measurelook. Формат описывает схему хранения данных эксперимента. Просмотровщик это автономная веб-страница, визуализирующая данные эксперимента. Вы загружаете в нее файл с данными эксперимента и просмотровщик рисует по нему графики.
В просмотровщик входят также валидатор и мигратор. Валидатор проверяет с помощью JSON Schema корректность загружаемого файла. Мигратор автоматически обновляет версию файла с экспериментом. С одной стороны это обеспечивает обратную совместимость с актуальной версией просмотровщика. С другой стороны вам не обязательно обновлять свои исходники до новой версии формата, если старая вас устраивает.
Формат и просмотровщик работают вместе, но при их создании преследуются разные цели. Формат сделан так, чтобы исследователь мог сохранить нужные ему данные. Просмотровщик сделан для визуализации простых данных. Двухмерные графики он строит, но трёх- и более мерные нет. В описании формата я буду указывать, что может хранить формат и не может показать просмотровщик.
Measurelook описывает структуру JSON файла для хранения информации о вычислительном эксперименте.
Meta — произвольный JSON объект для хранения данных об эксперименте. Примеры данных: название реализации алгоритма, параметры компиляции, параметры аппаратного и программного окружения, название датасета. Сюда же можно вывести некоторые параметры запуска алгоритма, но об этом позже.
Прим.: просмотровщик в текущий момент не умеет обращаться с произвольными метаданными. Он предполагает, что все метаданные являются строками.
Пример:
Параметры алгоритма. Раньше моё представление об экспериментах было несколько наивным — один параметр меняем, другой измеряем, строим график. Ну хорошо, один меняем, несколько измеряем, строим несколько графиков. Но когда я ознакомился с диссертацией Теплова А.М. «Анализ масштабируемости параллельных приложений на основе технологий суперкомпьютерного кодизайна» концепция изменилась. В его исследовании строятся графики по двум изменяемым параметрам и одному измеряемому. Например, мы меняем размер входных данных и количество процессов, а измеряем производительность. Соответственно визуализироваться это должно поверхностью в трёхмерном пространстве.
Я решил, что это важное обобщение и выделил три группы параметров алгоритма: константы, изменяемые параметры и измеряемые параметры.
Константа это параметр алгоритма, который не изменяется в ходе эксперимента. Константа описывается тройкой: название, единицы измерения, значение. Хранить константы предпочтительнее в этом блоке, но может вам будет удобнее хранить их в метаданных. Пример: размерность датасета.
Изменяемый параметр это параметр алгоритма, который меняется в ходе эксперимента. Он описывается парой: название, единицы измерения. Пример: объем входных данных, количество потоков.
Измеряемый параметр это параметр алгоритма, который измеряется в ходе эксперимента. Ранее он так же описывался парой: название, единицы измерения. Позже было добавлено разделение на фиксированные и косвенные параметры. Фиксированный параметр это непосредственно измеренное в ходе эксперимента значение. Пример: время загрузки данных, время работы алгоритма. Косвенный параметр это параметр, вычисляемый как сумма значений фиксированных параметров. Для них указывается список фиксированных параметров. Пример: полное время работы алгоритма является суммой времени загрузки данных и времени работы алгоритма.
Прим.: в текущий момент просмотровщик умеет строить графики только по одному изменяемому параметру и нескольким измеряемым. Случай с двумя изменяемыми параметрами нужно рисовать в виде поверхности в трёхмерном пространстве либо трансформировать в серию графиков в двухмерном пространстве через перевод одного изменяемого параметра в измеряемый. Это реализуемо, но у меня не было такой потребности.
Примеры:
После описания параметров эксперимента переходим к замерам. Структура записи о замере:
Пример:
Прочие значения:
Пример:
Просмотровщик это веб-страница, на которой выводится основная информация об эксперименте и можно посмотреть первичные графики.
Реализованные функции:
JSON штука распространённая, формат простой. Полагаю организовать вывод данных в таком виде можно на многих языках программирования. Для себя я сделал вспомогательный принтер на языке Си с помощью bstring. Он опубликован вместе с примером использования на гитхабе (см. ссылки).
Засим я считаю, что завершил часть «Как это сделать?» и перехожу к части «Что с этим делать?»
Как это было у меня — есть много файлов, результатов замеров. Нужно их переработать в красивые графики.
Я это делал следующим образом: 
Предвосхищая вопросы:
В: Почему R, а не d3.js?
О: Потому что у меня был до этого небольшой опыт на R, и потому что программирование d3 мне субъективно показалось слишком низкоуровневым.
В: Почему R сохраняет svg, а не сразу pdf и png?
О: R это безусловно может, но когда я это пробовал, pdf он пытался печатать по-умному со шрифтами. По-видимому он не находил кириллицу и графики получались без надписей. Так что да, еще одна лишняя программа, зато гарантированно работает и я больше доверяю качеству преобразования Inkscape.
Из своего большого репозитория для рисования 8 видов графиков, я сделал маленький, рисующий один вид и опубликовал на гитхабе (см. ссылки).
Проект Measurelook решил поставленные перед ним задачи — организация хранения данных вычислительных экспериментов и простое средство предварительного просмотра. Также он послужил основой для решения третьей задачи — полностью автоматизированного перестроения графиков.
User
=====
Как детское увлечение стало любимым делом: три истории с Huawei Honor Cup
2017-12-11, 13:10

=====
Постмортем Shadow Tactics: Blades of the Shogun
2017-12-20, 12:32
Переводчик-фрилансер
=====
Торговля под присмотром: пример системы бизнес-мониторинга
2017-12-08, 15:20
User
=====
Исследование внутренних дефектов ПЛИС: ищем черную кошку в темной комнате
2017-12-07, 22:56
Пользователь
=====
Паттерны разработки: MVC vs MVP vs MVVM vs MVI
2017-12-07, 22:50
От переводчика: данная статья является переработкой английской статьи по паттернам разработки. В процессе адаптации на русский немало пришлось изменить.  Оригинал
Выбор между различными паттернами разработки, всегда сопровождается рядом споров и дискуссий, а разные взгляды разработчиков на это еще больше усложняют задачу. Существует ли решение этой идеологической проблемы? Давайте поговорим о MVC, MVP, MVVM и MVI прагматично. Давайте ответим на вопросы: “Почему?”, “Как найти консенсус?”
 Вопрос выбора между MVC, MVP, MVVM и MVI коснулся меня, когда я делал приложение для Warta Mobile вместе с моей командой. Нам было необходимо продвинуться от минимально жизнеспособного продукта к проверенному и полностью укомплектованному приложению, и мы знали, что необходимо будет ввести какую-либо архитектуру.
У многих есть непоколебимое мнение насчет различных паттернов разработки. Но когда вы рассматриваете архитектуру, такую как MVC, то, казалось бы, полностью стандартные и определенные элементы: модель (Model), представление (View) и контроллер (Controller), разные люди описывают по разному.
Трюгве Реенскауг (Trygve Reenskaug) — изобрел и определил MVC. Через 24 года после этого, он описал это не как архитектуру, а как набор реальных моделей, которые были основаны на идее MVC.
Я пришел к выводу, что поскольку каждый проект — уникален, то нет идеальной архитектуры.
Необходимо детально рассмотреть различные способы реализации, и подумать над преимуществами и недостатками каждой.
 
Очевидно, что масштабируемость (scalability) — возможность расширять проект, реализовывать новые функции.
Сопровождаемость (maintainability) — можно определить как необходимость небольших, атомарных изменений после того, как все функции реализованы. Например, это может быть изменение цвета в пользовательском интерфейсе. Чем лучше сопровождаемость проекта, тем легче новым разработчикам поддерживать проект.
Надежность (reliability) — понятно, что никто не станет тратить нервы на нестабильные приложения!
Важнейшим элементом здесь является разделение ответсвенности (Separation of Concerns): различные идеи должны быть разделены. Если мы хотим изменить что-то, мы не должны ходить по разным участкам кода.
Без разделения ответственности ни повторное использование кода (Code Reusability), ни тестируемость (Testability) практически невозможно реализовать.
Ключ в независимости, как заметил Uncle Bob в Clean Architecture. К примеру, если вы используете библиотеку для загрузки изображений, вы не захотите использовать другую библиотеку, для решения проблем, созданных первой! Независимость в архитектуре приложения — частично реализует масштабируемость и сопровождаемость.
У архитектуры MVC есть два варианта: контроллер-супервизор (supervising controller) и пассивное представление (passive view).
В мобильной экосистеме — практически никогда не встречается реализация контроллера-супервизора. 
Архитектуру MVC можно охарактеризовать двумя пунктами:

Диаграмма иллюстрирует идеологию паттерна. Здecь, представление определяет как слушателей, так и обратные вызовы; представление передает вход в контроллер.
Контроллер принимает входные данные, а представление — выходные, однако большое число операций происходит и между ними. Данная архитектура хорошо подходит только для небольших проектов.

Главная идея пассивного представления MVC — это то, что представление полностью управляется контроллером. Помимо этого, код четко разделен на два уровня: бизнес логику и логику отображения:
Нельзя трактовать Активити как представление (view). Необходимо рассматривать его как слой отображения, а сам контроллер выносить в отдельный класс.
А чтобы уменьшить код контроллеров представлений, можно разделить представления или определить субпредставления (subviews) с их собственными контроллерами. Реализация MVC паттерна таким образом, позволяет легко разбивать код на модули.
Однако, при таком подходе появляются некоторые проблемы:
Решение этих проблем кроется за созданием абстрактного интерфейса для представления. Таким образом, презентер будет работать только с этой абстракцией, а не самим представлением. Тесты станут простыми, а проблемы решенными.
Все это — и есть главная идея MVP.
Данная архитектура облегчает unit-тестирование, презентер (presenter) прост для написание тестов, а также может многократно использоваться, потому что представление может реализовать несколько интерфейсов.
С точки зрения того, как лучше и корректней создавать интерфейсы, необходимо рассматривать MVP и MVC только как основные идеи, а не паттерны разработки.

Создание use cases — это процесс выноса бизнес логики в отдельные классы, делая их частью модели. Они независимы от контроллера и каждый содержит в себе одно бизнес-правило. Это повышает возможность многократного использования, и упрощает написание тестов.

В примере на GitHub, в login controller, вынесен use case валидации и use case логина. Логин производит соединение с сетью. Если есть общие бизнес правила в других контроллерах или презентерах, можно будет переиспользовать эти use case’ы.
В реализации MVP есть четыре линейный функции, которые ничего не делают, кроме небольших изменений в пользовательском интерфейсе. Можно избежать этого лишнего кода, использовав view binding.
Все способы биндинга можно найти здесь.
Здесь простой подход: легко тестировать, и еще легче представить элементы представления как параметры через интерфейс, а не функции.
Стоит отметить, что с точки зрения презентера — ничего не изменилось.
Существует другой способ биндинга: вместо привязывания представления к интерфейсу, мы привязываем элементы представления к параметрам view-модели — такая архитектура называется MVVM. В нашем примере, поля email, password, и разметка определены с помощью связываний. Когда мы меняем параметры в нашей модели, в разметку тоже вносятся изменения.

ViewModel’и просты для написания тестов, потому что они не требуют написания mock-объектов — потому что вы меняете свой собственный элемент, а потом проверяете как он изменился.
Еще один элемент, который можно ввести в архитектуре, обычно называется MVI.

Если взять какой-либо элемент разметки, например кнопку, то можно сказать, что кнопка ничего не делает, кроме того, что производит какие-либо данные, в частности посылает сведения о том что она нажата или нет.
В библиотеке RxJava, то, что создает события — называется observable, то есть кнопка будет являться observable в парадигме реактивного программирования.
А вот TextView только отображает какой-либо текст и никаких данных не создает. В RxJava такие элементы, которые только принимают данные, называются consumer.
Также существуют элементы, которые делают и то и то, т. е. и принимают и отправляют информацию, например TextEdit. Такой элемент одновременно является и создателем (producer) и приемником (receiver), а в RxJava он называется subject.
При таком подходе — все есть поток, и каждый поток начинается с того момента, как какой-либо producer, начинает испускать информацию, а заканчивается на каком-либо receiver, который, в свою очередь, информацию принимает. Как результат, приложение можно рассматривать как потоки данных. Потоки данных — главная идея RxJava.
Несмотря на то, что внедрение разделения ответсвенности требует усилий, это хороший способ повысить качество кода в целом, сделать его масштабируемым, легким в понимании и надежным.
Другие паттерны, такие как MVVM, удаление шаблонного кода, MVI могут еще сильнее улучшить масштабируемость, но сделают проект зависимым от RxJava.
Также, следует помнить, что можно выбрать всего лишь часть из этих элементов, и сконфигурировать для конечного приложения в зависимости от задач.
Все исходники можно найти здесь.
Android разработчик
=====
Сколько английских слов надо выучить для свободного общения и чтения статей? (спойлер: 3000)
2017-12-08, 00:15
Пишем код, который меняет систему образования
=====
Мониторинг инженерной инфраструктуры в дата-центре. Часть 4. Сетевая инфраструктура: физическое оборудование
2017-12-08, 10:29
Операционный директор, DataLine
=====
SEO или AdWords? Что выгоднее для b2b-компании в 2017 году
2017-12-07, 23:47
Аналитик
=====
Обзор литературы по языку Python для начинающих
2017-12-08, 04:40
Веб-разработчик (Python/PHP)
=====
Поиск объекта на изображении при помощи перцептивного хэша
2018-08-07, 12:44
Пользователь
=====
DevDay на функционале
2017-12-08, 07:32
Developer Relations
=====
Зависимые события и статистические флуктуации или почему «водопад» умрет
2017-12-08, 08:19
Пользователь
=====
Вышел GitLab 10.2: Настраиваемые доски задач и GitLab Geo в общем доступе
2017-12-08, 08:08

В данном релизе мы добавили возможности по улучшению планирования, развертывания, надежности и многое другое.
На мой взгляд, у задач GitLab много общего с водой: они необходимы для жизни, но в большом количестве можно и утонуть.
В командной работе с общим обзором полезно его сужение только до задач, важных в определенном контексте. Ранее GitLab позволял применять фильтры для отображения задач связанных с определенным майлстоуном или меткой доски задач, однако, это было лишь временным решением. Возможно, вам приходилось сохранять в закладки и отправлять коллегам ссылки на доску задач..
В данном релизе мы вводим настройку доски задач, с помощью которой вы сможете сохранять выборку по майлстоунам, меткам, весу и исполнителям задач, что позволит каждому члену команды видеть одни и те же задачи.
Команды разработчиков всё чаще бывают распределены географически… Это одна из причин, по которым Git так популярен: он является распределяемым по своей природе — в вашем локальном репозитории содержатся копии каждого коммита, файла и ветки в истории проекта. Загружаемая история серьезно ускоряет процесс разработки.
Однако, если существует только один физический инстанс репозитория, возможна ситуация, в которой он расположен вдали от распределенных команд. Это замедляет загрузку большого количества маленьких файлов. Мы с радостью сообщаем, что в данном релизе GitLab Geo выпущен в общий доступ. GitLab Geo позволяет запускать read-only реплики GitLab (включая его интерфейс) в физической близости от удаленных команд.
Благодаря тому, что GitLab спроектирован как одно приложение, в нем содержится единое хранилище данных для репозитория исходного кода, отслеживания задач, CI/CD и мониторинга. Такой единый подход способствует наглядности, повышению эффективности и упрощению работы с проектом для всех участников.
GitLab является ядром разработки для множества команд, поэтому он должен работать безотказно вне зависимости от времени суток. В данном релизе мы с гордостью сообщаем, что PostgreSQL High Availability выходит в общий доступ, благодаря чему стало возможно с легкостью настроить и запустить кластер Postgres для GitLab. Простой процесс установки на основе Omnibus и автоматическая система отказоустойчивости позволят вашим разработчикам не отвлекаться на посторонние проблемы.
Суть:
Мы стремимся улучшать работу с GitLab Kubernetes в каждом релизе. В прошлом месяце мы упростили создание кластеров Kubernetes. Но ведь после создания кластера нужно еще и настроить дополнительные сервисы, например, внешний контроллер доступа (external access controller). Вам больше не нужно тратить на это время: в данном релизе мы добавили установщики в один клик для Tiller and Ingress. А в следующем месяце мы планируем добавить поддержку одновременного развертывания сразу нескольких кластеров. Мы стремимся к тому, чтобы каждое из ежемесячных обновлений было законченным и ценным продуктом.
В этом месяце мы добавили множество новых интересных функциональностей, например, ограничение авторства коммитов и повышение майлстоунов проекта до майлстоунов группы.
Далее в статье мы подробнее остановимся на этих и других нововведениях GitLab 10.2.
Приглашаем на наши встречи!
В течение последних нескольких релизов Travis работал над улучшением GitLab Pages, а в GitLab 10.2 он внес вклад в создание нового интерфейса страницы Pages для управления существующими доменами. Это нововведение позволяет запрограммировать ключевые задачи, например, обновление сертификата или перевод существующего домена на HTTPS.
Мы благодарим Travis за его вклад! В знак благодарности, мы отправили ему носки, стикеры, футболку и стакан GitLab, которые, похоже, пришлись ему по душе.
Многие команды используют общую доску задач для планирования и отслеживания рабочего процесса. Каждый член команды должен видеть на ней один и тот же набор задач. Ранее к доске задач можно было привязать только майлстоун, а теперь — майлстоун (включая вариант «нет майлстоунов»), метки, исполнителя и вес, что расширяет возможности вашей команды.
Такая конфигурация сохраняется вместе с доской, так что любой пользователь, который ее просматривает, увидит все эти фильтры. Вы можете настраивать и редактировать конфигурацию доски нажатием кнопки View scope (Просмотр скоупа) или Edit board (редактирование доски), в зависимости от вашего уровня доступа.
Документация по настройке досок задач.
В GitLab 10.1 появилась возможность подключения аккаунта Google к проектам и создания нового кластера Kubernetes прямо из страницы GitLab Cluster. В дальнейшем этот кластер можно использовать для развертывания приложений для ревью и сред разработки.
В GitLab 10.2 мы продолжили работу в этом направлении, добавив возможность установки Helm Tiller и Nginx Ingress для вашего кластера GKE в один клик, что еще больше сокращает временные затраты на разработку приложений.

Документация по установке приложений на кластеры GKE.
GitLab Enterprise Edition Premium предоставляет дополнительные возможности управления рабочим процессом и меры контроля среды разработки.
В версии 10.2 появилась возможность настроить правила пуша таким образом, что автором коммита может быть только тот же пользователь, что пушит изменения в репозиторий. Это позволит предотвратить попадание неавторизованного кода в ваш проект, а также повысит уровень контроля над процессом разработки.
Эту опцию можно подключить как для отдельных репозиториев, так и для всего инстанса GitLab, что позволит контролировать рабочий процесс на всем сервере.
Данное нововведение, в сочетании с возможностью отклонения неподписанных коммитов в EEP, предоставляет полный контроль над идентификацией и верификацией вносимых изменений.

Документация по правилам пуша.
Многие команды, использующие GitLab, распределены по различным географическим локациям, однако их инстанс GitLab находится в одном месте. GitLab Geo позволяет устранить связанные с этим сложности, ускоряя fetch-операции, такие как клонирование репозиториев.
Теперь GitLab Geo находится в общем доступе! После настройки GitLab Geo поддерживает синхронизацию вторичных read-only инстансов GitLab с основным. GitLab Geo можно использовать для доступа к Git, объектам LFS, задачам, вики и артефактам CI ближайшего инстанса GitLab.
Обратите внимание, хотя Geo и High Availability теперь находятся в общем доступе, их одновременное использование рассматривается как бета.
Важные изменения в GitLab 10.2:
Полный список изменений Geo в версии 10.2 можно посмотреть здесь.

Документация по GitLab Geo.
GitLab является одним из ключевых инструментов разработки для множества организаций: с его помощью поддерживается репозиторий кода, CI/CD, управление задачами и многое другое. Для того, чтобы быть уверенным в непрерывном функционировании GitLab, можно проводить его развертывание в конфигурации высокой доступности (high availability).
Мы с радостью сообщаем, что, начиная с GitLab 10.2, PostgreSQL High Availability находится в общем доступе. Пакет установки Omnibus включен в GitLab Enterprise Edition Premium, что заметно упрощает создание и настройку кластеров баз данных Postgres.
В случае отказа нода базы данных, кластер автоматически переключится на вторичный, что гарантирует непрерывность процесса разработки.

Документация по настройке баз данных для GitLab HA.
В GitLab 9.1 мы добавили канареечные развертывания в GitLab CI, что повысило уровень контроля над процессом запуска новых релизов. Суть такого подхода состоит в том, что перед глобальным запуском развертывание новой версии продукта проводится лишь на небольшом проценте нодов, благодаря чему изменения затрагивают небольшую долю пользователей, и, в случае непредвиденных осложнений, эти изменения можно легко исправить или откатить.
В GitLab 10.2 мы добавляем возможность отслеживания системных метрик таких «канареечных» версий, что позволит с легкостью сравнить реальную производительность различных версий. Теперь разработчики смогут быстро оценить влияние изменений на производительность и решить, можно ли проводить полномасштабное развертывание. А самое приятное, что это все можно сделать, не покидая GitLab!

Документация по оценке производительности канареечного развертывания.
Ревизия событий в GitLab Enterprise Edition помогает усилить контроль и улучшить отчетность. В GitLab Enterprise Edition Starter (EES) вы можете просматривать ревизию событий в каждом проекте или группе. GitLab Enterprise Edition Premium (EEP) включает централизованный лог, в котором все события проекта сосредоточены в одном месте.
GitLab 10.2 EES и EEP теперь включают дополнительные события для действий над проектами или группами. Например:
В GitLab EEP данные ревизии удаленного проекта или группы теперь сохраняются на сервере в общем логе и доступны администратору. Также теперь включается IP адрес пользователя, совершающего действие.
Документация по улучшенной ревизии событий.
Подгруппы — отличный способ организации проектов или команд в GitLab. С почти неограниченным наследованием групп вы можете создать структуру для отображения сложных репозиториев, микросервисов или даже обрисовать внутреннее устройство ваших команд разработки.
Мы провели капитальный ремонт страницы групп GitLab. Стало удобнее перемещаться и искать подгруппы и проекты. Теперь на странице групп вы будете видеть навигацию в виде расширяющегося дерева, которая поможет вам быстро найти то, что вы ищете, или обнаружить новые проекты и группы.

Документация по подгруппам и проектам на странице групп.
В этом релизе мы запускаем самую первую версию  Эпиков (Epics) как первую часть Управления портфолио. Эпики сделаны для того, чтобы позволить вам планировать и контролировать вашу работу на уровне фич, а не дизайна и деталей реализации задачи.
Эпики нацелены на работу на групповом уровне. После создания эпика с названием и (необязательно) описанием вы можете создать множество задач и связать их с эпиком. Это типичный процесс работы «сверху-вниз», где вы сначала планируете фичу на верхнем уровне, а потом разбиваете ее на меньшие задачи, чтобы ее проще было реализовать. И наоборот: вы можете применить подход «снизу-вверх», когда берете несколько существующих задач и объединяете их в один новый эпик. 
Каждая задача, входящая в группу эпика или любую из его подгрупп может быть привязана к этому эпику. Также у эпика есть дополнительные опции
планируемая дата начала (planned start date) и планируемая дата завершения (planned end date).
Эпики включены в Enterprise Edition Ultimate (EEU) и Золотой план подписки GitLab.com.

Документация по Эпикам.
Для каждого токена личного доступа (personal access token) можно выбрать только необходимые привилегии API. Это делает доступ в GitLab через API или через сторонние приложения более защищенным.
Приватные токены (private access tokens) ранее были запрещены; теперь их убрали окончательно.
После обновления все приватные токены станут токенами личного доступа, поэтому все существующие приложения продолжат работать как раньше.
Документация по токенам личного доступа, заменяющим приватные токены.
Когда вы переросли ваш проект, вы можете легко повысить майлстоуны проекта до майлстоунов группы. Просто зайдите на страницу майлстоуна нужного проекта и нажмите кнопку, чтобы сделать его майлстоуном группы. Так же, как и с метками, все проектные майлстоуны с тем же названием по всем проектам группы объединятся в один майлстоун группы. И все задачи и мерж-реквесты, ранее связанные с одним из объединённых майлстоунов, привяжутся к получившемуся майлстоуну группы. Проектные доски задач, связанные с предыдущим майлстоуном проекта теперь будут связаны с майлстоуном группы.

Документация о повышении майлстоунов проекта до майлстоунов группы.
Пуш-зеркалирование работает так: репозиторий отправляет все входящие изменения в другой, заранее заданный, репозиторий.
Администраторы теперь могут ограничить доступ к зеркалированию пушей так, чтобы оно было доступно только администраторам. Это поможет предотвратить автоматическое зеркалирование приватных проектов в другой репозиторий, который может оказаться внешним или небезопасным. 
Документация по зеркалированию репозиториев.
Настройки «предпочитаемой домашней страницы проекта» позволяют выбирать контент, который вы увидите на домашней странице любого открытого проекта. Вы можете выбрать активности проекта, список файлов и readme, либо только readme. 
В GitLab 9.0 была удалена возможность выбирать только readme. Теперь эту возможность вернули для тех, кто предпочитает минимальный обзор проекта.
Документация по readme-обзору проекта.
Мы продолжаем переводить GitLab на разные языки. В этот раз мы ‘экстернализировали строки на страницах «Contributors» и «Tag», позволив нашему сообществу переводчиков добавлять добавлять новые языки и строки в GitLab.
Если вы хотите поучаствовать в локализации GitLab, приглашаем вас присоединиться к нашему сообществу переводчиков.
Документация по улучшениям локализации.
GitLab Pages позволяет вам публиковать статичный сайт прямо из конвейера вашего проекта. Если вы хотите сделать его более профессиональным, вы можете настроить собственные домены для вашего контента и защитить их сертификатами.
В GitLab 10.2 стало возможным управлять собственными доменами для GitLab Pages через запросы API. Вы сможете автоматизировать подключение новых доменов и получение информации от существующих, а также обновлять данные домена — например, когда был обновлен SSL сертификат.
Документация по API для GitLab Pages.
В GitLab вы можете определять, какой проект — публичный или приватный — вы хотите создать. При работе с конвейером вам иногда нужна эта информация для совершения разных действий.
В GitLab 10.2 эта информация находится в переменной CI_PROJECT_VISIBILITY. С ее помощью вы можете определить, к примеру, разрешен ли публичный доступ к артефактам или образам Docker.
Очень удобно, когда у вас есть шаблон с общим доступом, который используется для разных проектов.
Документация о переменной для видимости проекта.
Документация по улучшениям Omnibus.
GitLab 10.2 включает Mattermost 4.3.2,
альтернативу Slack с открытым исходным кодом. Новый релиз включает поддержку планшетов в бета-версии, улучшения мобильного приложения и многое другое. В данной версии имеются обновления безопасности, рекомендуем обновиться.
Документация по GitLab Mattermost 4.3.2.
В этой версии также вышел GitLab Runner 10.2. Это проект с открытым исходным кодом, позволяющий запускать работы CI/CD и посылать результаты обратно в GitLab.
Полный список изменений находится в CHANGELOG GitLab Runner.
Документация по GitLab Runner 10.2.
Производительность — важная часть GitLab, позволяющая ему масштабироваться до сотен тысяч пользователей.
В версии GitLab 10.2 появился новый импортер GitHub, который использует Sidekiq для параллельного выполнения задач, что значительно уменьшает время, необходимое на импорт проектов из GitHub. Если будет достигнут лимит на количество запросов к GitHub, новый импортер приостановит работу и продолжит ее тогда, когда лимит сбросится. При этом ни один из обработчиков Sidekiq не прекратит работу.
Для маленьких проектов вроде https://github.com/yorickpeterse/oga время импорта уменьшилось от 5 минут до 30-60 секунд, а для таких больших проектов, как https://github.com/kubernetes/kubernetes, время импорта вместо нескольких недель превратилось в 6.5 часов.
Больше информации о новом импортере GitHub вы найдете в мерж-реквесте CE.
Кроме того, GitLab 10.2 включает еще 12 улучшений производительности, сфокусированных на ускорении загрузки мерж-реквестов. Кроме того, мы ускорили загрузку задач, а также уменьшили количество некоторых крайних случаев, которые значительно потребляют ресурсы сервера.
Полный список улучшений производительности в GitLab 10.2.
—-
Подробные release notes и инструкции по обновлению/установке можно прочитать в оригинальном англоязычном посте: GitLab 10.2 released with Configurable Issue Boards and GitLab Geo General Availability.
Перевод с английского выполнен переводческой артелью «Надмозг и партнеры», http://nadmosq.ru. Над переводом работали rishavant, sgnl_05 и nick_volynkin.
SDLC | ITSM | BPM | QA
=====
Производительность mdadm raid 5,6,10 и ZFS zraid, zraid2, ZFS striped mirror
2017-12-08, 12:15
Разрабатываем, внедряем поддерживаем и обучаем
=====
Рефлексия в C++14
2017-12-08, 11:10
Пользователь
=====
Как узнать, что будет делать программа для Linux, не выполняя её по-настоящему?
2017-12-08, 12:41
Пользователь
=====
Куй железо: горячий жаргон hardware-стартапов
2017-12-08, 13:18
Пользователь
=====
Что нового в DataGrip 2017.3
2017-12-08, 13:28
Marketing manager
=====
Kubernetes 1.9: обзор основных новшеств
2017-12-13, 10:14
Пользователь
=====
Как построить классификатор изображений на основе предобученной нейронной сети
2017-12-08, 14:05


Data Science
=====
Как и зачем определять голосовую почту
2017-12-11, 12:22
Программист
=====
Танчики в консоли, статья первая: «От спора к написанию кода»
2017-12-08, 14:40
Начинающий программист
=====
«Это хороший дизайн или нет?» — как я отвечаю на этот вопрос с помощью 3 категорий
2017-12-08, 14:44
Предприниматель
=====
Oracle Open World 2017: анонсы «Автономного AI»
2017-12-08, 15:01
User
=====
Python Meetup 14.11.2017: Python в Порту, Aiohttp и снова тесты
2017-12-13, 12:11
User
=====
Биллинговые системы будущего, и как они изменяют рынок связи
2017-12-11, 11:44
User
=====
Learn OpenGL. Часть 4.2. — Тест трафарета
2017-12-08, 16:09

Как только, фрагментный шейдер обработал фрагмент, выполняется так называемый тест трафарета, который, как и тест глубины, может отбрасывать фрагменты. Затем оставшиеся фрагменты переходят к тесту глубины, который, может отбросить еще больше фрагментов. Трафаретный тест основан на содержимом еще одного буфера, называемого трафаретным буфером. Мы можем обновлять его во время рендеринга для достижения интересных эффектов.
Трафаретный буфер, обычно, содержит 8 бит на каждое трафаретное значение, что в сумме дает 256 различных значений трафарета на фрагмент/пиксель. Мы можем установить эти значения на наш вкус, а затем отбрасывать или сохранять фрагменты, всякий раз, когда определенный фрагмент имеет определенное трафаретное значение.
Каждая оконная библиотека должна настроить буфер трафарета для вас. GLFW делает это автоматически, так что вам не нужно беспокоиться об этом, но другие оконные библиотеки могу не создавать трафаретный буфер по умолчанию, так что убедитесь, что просмотрели документацию вашей библиотеки.
Простой пример трафаретного буфера приведен ниже:
Сначала трафаретный буфер заполняется нулями, и затем область буфера, выглядящая как прямоугольная рамка, заполняется единицами. Отображаются только те фрагменты сцены, трафаретное значение которых равно единице, остальные — отбрасываются.
Операции трафаретного буфера позволяют нам установить разное значение для трафаретного буфера там, где мы отображаем фрагменты. Изменяя значения трафаретного буфера во время рендеринга, мы осуществляем операцию записи. В той же (или следующей) итерации рендеринга мы можем выполнить чтение значений из буфера, чтобы на основе прочитанных значений, отбросить или принять определенные фрагменты. Используя трафаретный буфер, вы можете дурачиться как угодно, но общая схема такова:
В итоге, можно сказать, что, используя трафаретный буфер, мы можем отбросить определенные фрагменты, основываясь на фрагментах других объектов сцены.
Вы можете включить трафаретное тестирование, включив GL_STENCIL_TEST. С этого момента все вызовы рендеринга будут, так или иначе, влиять на трафаретный буфер.
Обратите внимание, что вам нужно очищать трафаретный буфер на каждой итерации, так же как буфер цвета и глубины:
Для трафаретного буфера есть аналог функции glDepthMask, используемой для параметризации теста глубины. Функция glStencilMask позволяет нам установить битовую маску, которая будет участвовать в операции побитового И со значениями трафарета, записываемыми в буфер. По умолчанию, битовая маска равна единице, что не влияет на выходные данные, но если бы мы установили маску в 0x00, то все трафаретные значения в конце концов были бы записаны как нули. Эквивалентом настройки маски теста глубины glDepthMask(GL_FALSE) была бы следующая пара вызовов:
В большинстве случаев просто записывайте 0x00 или 0xFF в трафаретную маску, но неплохо было бы знать, что существует возможность устанавливать пользовательские битовые маски.
Также как и в тесте глубины у нас есть определенная возможность контролировать, когда трафаретный тест будет пройден, а когда — нет, и как это должно повлиять на трафаретный буфер. У нас есть всего две функции, которые мы можем использовать для настройки трафаретного тестирования: glStencilFunc и glStencilOp. 
Функция glStencilFunc(GLenum func, GLint ref, GLuint mask) имеет три параметра:
Итак, в случае нашего простого примера трафарета, который мы показали в начале, функция будет такой:
Это говорит OpenGL, что, всякий раз, когда трафаретное значение фрагмента равно ссылочному значению 1, фрагмент проходит тест и рисуется, в противном случае — отбрасывается. 
Но функция glStencilFunc описывает только то, что OpenGL следует делать с содержимым трафаретного буфера, а не то, как мы можем обновить буфер. Здесь нам на помощь приходит функция glStencilOp.
Функция glStencilOp(GLenum sfail, GLenum dpfail, GLenum dppass) содержит три параметра, с помощью которых мы можем определить действия для каждого варианта:
Затем, для каждого случая можно выполнить одно из следующих действий:
По умолчанию аргументы функции glStencilOp устанавливаются в (GL_KEEP, GL_KEEP, GL_KEEP), так что, независимо от результатов тестов, значения в буфере трафарета сохраняются. Стандартное поведение не обновляет трафаретный буфер, поэтому, если мы хотим писать в трафаретный буфер, то нам нужно установить по крайней мере одно действие отличное от стандартного для любого варианта.
Итак, используя glStencilFunc и glStencilOp мы можем точно установить когда и как мы хотим обновлять трафаретный буфер, и мы также определяем, когда трафаретный тест будет пройден, а когда — нет, то есть когда фрагменты должны быть отброшены.
Маловероятно, что вы полностью поняли, как работает трафаретное тестирование на основе предыдущих разделов, поэтому мы продемонстрируем полезный прием, который может быть реализован с помощью трафаретного тестирования. Это обводка объекта.

Нет нужды объяснять, что подразумевается под обводкой объекта. Для каждого объекта (или только для одного) мы создаем маленькую цветную рамку. Этот эффект особенно полезен, когда нам, к примеру, нужно выделить юнитов в стратегической игре, а затем показать пользователю, какие были выбраны. Алгоритм формирования обводки для объекта такой:
Этот процесс устанавливает содержимое буфера для каждого фрагмента объекта равным единице, и когда мы хотим нарисовать границы, мы, по сути, рисуем масштабируемые версии объектов, и там, где позволяет тест, масштабируемая версия рисуется (вокруг границ объекта). С помощью теста трафарета мы отбрасываем те фрагменты отмасштабированных объектов, которые наложены на фрагменты исходных объектов.
Для начала создаем очень простой фрагментный шейдер, который выводит цвет обводки. Мы просто устанавливаем жестко-закодированный цвет и вызываем шейдер shaderSingleColor:
Мы планируем включить обводку только для двух контейнеров, но не для пола. Поэтому сперва нужно вывести пол, затем два контейнера (с записью в буфер трафарета), а затем — увеличенные версии контейнеров (с отбраковкой фрагментов, наложенных на уже отрисованные фрагменты исходных контейнеров).
Сначала нам нужно включить трафаретное тестирование и установить действия, выполняемые при успешном или неудачном выполнении любого из тестов:
Если какой-нибудь из тестов провалится, то мы ничего не делаем, а просто оставляем текущее значение в трафаретном буфере. Если же и трафаретный тест и тест глубины пройдены успешно, то мы заменяем текущее значение трафарета на эталонное значение, установленное через glStencilFunc, которое мы позже установим в 1.
Буфер очищается заполнением нулями, а для контейнеров мы обновляем трафаретный буфер до 1 для каждого нарисованного фрагмента:
Используя GL_ALWAYS в функции glStencilFunc мы гарантируем, что каждый фрагмент контейнеров обновит трафаретный буфер с трафаретным значением 1. Так как фрагменты всегда проходят трафаретный тест, то трафаретный буфер обновляется ссылочным значением, везде, где мы их нарисовали.
Теперь, когда трафаретный буфер обновлен до единицы, там, где мы нарисовали контейнеры, нам нужно нарисовать увеличенные версии контейнеров, но, теперь отключив запись в трафаретный буфер:
Мы используем аргумент GL_NOTEQUAL в glStencilFunc, который гарантирует, что мы рисуем только те части объектов, которые не равны единице, таким образом, мы рисуем только те части объектов, которые находятся за пределами ранее нарисованных объектов. Обратите внимание, что мы также отключили тест глубины, чтобы элементы увеличенных контейнеров, например, их границы, не были перезаписаны полом.
Также убедитесь, что снова включили тест глубины.
Общий шаблон обводки объекта для нашей сцены выглядит как-то так:
Если вы понимаете общую идею, лежащую в основе трафаретного тестирования, этот фрагмент кода не должен быть слишком сложен для понимания. В противном случае попробуйте внимательнее прочитать предыдущие секции и полностью понять, что делает каждая функция, теперь, когда вы видели пример ее использования.
Результат применения алгоритма обводки к сцене из урока по тесту глубины выглядит так:

Проверьте исходный код здесь, чтобы увидеть полный код алгоритма обводки объекта
Можно заметить, что границы между двумя контейнерами перекрываются. Обычно, это именно то, что нужно (вспомните стратегические игры, когда мы выделяем несколько юнитов). Если вам нужна полная граница вокруг каждого объекта, нужно очищать трафаретный буфер для каждого объекта и немного пошаманить с настройкой теста глубины.
Алгоритм обводки объектов, который вы видели, довольно часто используется в некоторых играх для визуализации выбранных объектов (вспомните стратегические игры), и такой алгоритм может быть легко реализован в классе модели. Затем можно просто установить логический флаг в классе модели для рисования с границами или без них. Если проявить немного творчества, то можно сделать границы более органичными с помощью фильтров пост-обработки, например размытия по Гауссу.
С помощью трафаретного тестирования можно делать больше вещей, чем просто обводить объекты, например рисовать текстуры внутри зеркала заднего вида так, что они вписываются в рамку зеркала. Или рендерить тени в реальном времени с помощью техники shadow volumes. Трафаретный буфер обеспечивает нас еще одним прекрасным инструментом в нашем и без того обширном инструментарии OpenGL.
Пользователь
=====
Недельный спринт, анкета кандидата и картонный мужик
2017-12-08, 17:57
Делаю цифровые продукты
=====
Развеем мифы о неэффективности email-маркетинга
2017-12-08, 16:17
Пользователь
=====
Скриптуем на WebAssembly, или WebAssembly без Web
2017-12-08, 19:47
Представлять WebAssembly не нужно — поддержка уже есть в современных браузерах. Но технология годится не только для них.
WebAssembly — кроссплатформенный байткод. Значит, этот байткод можно запустить на любой платформе, где есть его виртуальная машина. И для этого вовсе не нужен браузер и Javascript-движок.
Далее — проверка концепции на прочность, инструментарий и первый скриптовый модуль.
Wasm-модули можно использовать в тех же случаях, что и скриптовые языки: для исполнения динамически загружаемой логики. Там, где используется Lua и Javascript. Но затраты на интерпретацию wasm меньше, чем у скриптовых языков, и на wasm можно применить больше оптимизаций. Ибо оптимизации эти делаются во время компиляции на исходной машине, а не интерпретации или JIT-компиляции на клиенте.
WebAssembly потенциально независим от исходного языка. В перспективе, скриптёр (тот, кто будет писать скрипты) может делать это на удобном для него языке, не привязываться к конкретному языку.
Кроме скриптовых языков технологию можно сравнить с LLVM-байткодом и Java-машиной.
Сравнение с LLVM-IR сделано уже в ходе разработки WebAssembly. Авторы аргументируют свой отказ ещё на этапе MVP так:
По сравнению с Java и её виртуальной машиной:
WebAssembly может занять в инфраструктуре портируемого кода собственную нишу, где ни скриптовые языки, ни LLVM-IR, ни JVM не решают задачи эффективно.
Идея использовать WebAssembly без web-окружения возникла из конкретной задачи. Необходимо создать модули с интерактивными графиками, которые бы работали на веб-сайте и в мобильном приложении. Первоначальный вариант решения: встроить Javascript-движок в мобильное приложение и передавать логику javascript-кодом. Но движки оказались достаточно массивными и сложными в устройстве (за исключением, пожалуй, JerryScript). Использование движка для одной небольшой задачи выглядело серьёзным оверинжинирингом. В этот момент мы пришли к выводу, что аналогичные модули на WebAssembly будут лучше из-за малого размера интерпретатора и более быстрой интерпретации.
Другой вариант использования: компилировать в WebAssembly шаблоны для веб-страниц. Для этого достаточно создать backend к любимому шаблонизатору. Такие шаблоны достаточно просто запускать как на сервере через интерпретатор, так и в браузере стандартными средствами. Создать backend к шаблонизатору проще, чем портировать любимый шаблонизатор на любимую систему. Формально, backend проще сделать для кода на C, который после будет компилироваться в wasm.
На сайте WebAssembly предлагаются и другие варианты использования:
Для реализации задуманного нам нужен экспериментальный модуль WebAssembly из LLVM версии 5 (текущей стабильной) или старше.
Будем использовать цепочку LLVM Webassembly backend -> LLVM байткод -> текстовое представление LLVM-IR -> Binaryen s2wasm -> Binaryen wasm-as
Сборка занимает от 15 минут до часа и требует много памяти (особенно, при make -j8)
Пример сборки C/C++ с помощью makefile
В качестве proof of concept можно использовать интерпретатор из WABT. Для подтверждения работы будем вызывать функцию из WebAssembly, которая вызывает функцию среды. Добавить импортируемые функции в WABT можно, например, вот так.
Запустить модуль можно так:
Эти же модули можно использовать и в вебе, если реализовать требуемые импортируемые функции. Например, вот так.
WebAssembly модули умеют импортировать и экспортировать функции, но создать библиотеку предстоит самостоятельно: стандарт не определяет никакой стандартной библиотеки.
Стандартная библиотека создается для интерпретатора и компилируется вместе с интерпретатором. Если задача требует исполнения wasm и нативно, и в браузере, вам нужно будет портировать вашу стандартную библиотеку на Javascript для совместимости. Например, для задачи интерактивных графиков написать cmath-совместимую обёртку и для интерпретатора, и для javascript.
Что включать в стандартную библиотеку — отдельный сложный вопрос. В случае со скриптовыми языками, вам дают уже готовую универсальную библиотеку. Из которой некоторые функции вы будете вынуждены выключить (например, прямой доступ к файловой системе). В случае с wasm вы можете создать строгий специальный API, которым скрипты будут ограничены в их песочнице.
WebAssembly — мощная технология, которую можно использовать и вне веб-окружения. Пока мы делаем только первые шаги и учимся использовать новые инструменты, проектировать системы по новым правилам. В будущем WebAssembly может стать одним из стандартов для портируемых исполняемых пакетов.
Текущее (на ноябрь 2017 года) состояние инструментов довольно слабое, но они уже пригодны для использования. Выявленные проблемы исправляются достаточно быстро. В этой статье мы хотели показать возможность отдельного применения WebAssembly. А куда копать дальше — в опросе.
В соавторстве с strelok2010
Весь код здесь.
Пользователь
=====
firebase.js ПРОСТО ОГРОМНЫЙ (и что мы можем с этим сделать)
2017-12-08, 17:05
Он действительно огромный — просто посмотрите на него:

Эта штука весит 103кб (в сжатом виде). Больше чем код приложения — интернет-магазин -(58kb) и сравнима со всем остальным кодом в vendor бандле (156kb) — включающем react, react-dom, react-router, moment.js, lodash и кучу других библиотек. Что еще хуже — firebase нужен не на всех страницах, и очень часто не нужен к моменту загрузку сайта.
Не слишком много, как оказалось. Включение отдельных модулей не работало (на тот момент в webpack@2) да и помогло бы не слишком сильно — все равно требовалось бы включить auth и database + модуль app(42kb + 40kb + 3kb) — что дало бы 83% исходного размера так или иначе. Кроме того, сами модули auth и database совершенно монолитны (наглядно видно на скриншоте выше) и уже сжаты лучше некуда.
Конечно. 103kb кода валяющиющийся мертвым грузом в бандле — это очень неприятно. Подумайте — люди жалуются на размер react и переходят на inferno/preact — а react+react-dom весит всего 39kb в сжатом виде и они работают не покладая рук.
Но раз невозможно уменьшить размер этого куска — мы просто отложим его загрузку до того момента, когда он будет реально нужен.
И сделаем это так что никто и не заметит :)
Для сервера это предельно просто:
Для клиента — чуть сложнее, но все равно просто:
Далее нужно сделать полиморфный импорт в webpack-конфиге:
Теперь require(firebaseImport$) вернет Promise который сразу выполнен на сервере и который лениво загрузит firebase на клиенте. После первой загрузки на клиенте этот импорт тоже станет 'выполненым', и последующие обращения к firebase будут уже почти мгновенными.
Далее нужно инициализировать клиент firebase:
И собственно всё. Конечно, теперь использование firebase стало более многословным:
Но результат того стоит:


front-end developer
=====
Приглашаем 15 декабря на Moscow CocoaHeads Meetup в Mail.Ru Group
2017-12-11, 15:52

Друзья, мы приглашаем iOS/macOS-разработчиков 15 декабря в московский офис Mail.Ru Group, где пройдёт очередная встреча сообщества CocoaHeads. Вас ждут три интересных доклада, традиционная викторина и, конечно, неформальное общение с докладчиками и остальными гостями. Программа встречи под катом.
Андрей Володин, Prisma AI, «Как стать GPU-инженером за час»
Современный мир не был бы таким, какой он есть, без GPU-вычислений. Современные консольные игры, VR, AR, криптовалюты, машинное обучение — всё это работает на горячих графических процессорах. Однако среди мобильных разработчиков видеокарты не пользуются большой популярностью: многие думают, что это очень сложно, а некоторые вообще не замечают, что iPhone вообще имеет видеокарту.
Цель доклада — познакомить широкие массы разработчиков с программированием графических процессоров, с прицелом на мобильные платформы и, конечно же, трендовые темы.
Дмитрий Куркин, Mail.Ru Group, «Распределённая сборка IPA»
Дмитрий расскажет, как кастомизировать приложение без пересборки. Обсудит особенности ручной сборки IPA и способы ускорения сборки приложения при помощи параллельной сборки компонентов.
Виктор Брыксин, Yandex, «Синее смещение: оптимизация запуска на платформе iOS»
Виктор расскажет о практических подходах к оптимизации длительности запуска приложения на примере Яндекс.Браузера для iOS. Рассмотрит такие вопросы, как выбор проектных и продуктовых метрик для проверки эффективности оптимизации, технические и архитектурные подходы к уменьшению длительности старта; а также Виктор поделится идеей, как можно заставить ваше приложение казаться быстрее, чем оно есть на самом деле.
Сбор участников и регистрация: 18:30
Начало докладов: 19:00
Адрес: офис компании Mail.Ru Group, Ленинградский проспект, 39, стр. 79. 
Для участия необходимо зарегистрироваться. Не забудьте паспорт или водительские права.
Пользователь
=====
Перспективы эволюции жестких дисков: передовые технологии и трудности реализации
2017-12-08, 17:54
Пользователь
=====
Отчет о старте Atos IT Challenge
2017-12-08, 18:03
Есть ли у вас та штука, что называется pet project или side project? Тот самый проект, который бы вы делали в свое удовольствие и для себя, для саморазвития или расширения портфолио. Лично у меня долгое время не было ничего, что можно было бы показать. Однако, в рамках стартовавшего этой осенью конкурса Atos IT Challenge 2018, у меня как раз появилась возможность начать такой проект.
Объясню: уже седьмой год подряд Atos выбирает некоторую горячую в мире IT тему и предлагает командам студентов всего мира придумать и реализовать идею, этой теме посвященную. Конкурс достаточно значителен: студенты, прошедшие в основной тур, будут работать полгода под руководством опытного ментора, и лишь в июне узнают о результатах. Призовой фонд на фоне такого объема работ не сногсшибателен: 10 000, 5 000 и 3 000 евро расположились на призовых ступеньках, но да, собственно, привлекают прежде всего не деньги.
Главным я считаю возможность прокачаться в новой для себя области, заодно применив это в реальном проекте. И претворяя это все в жизнь в компании своих товарищей, конечно.
“Чат-боты для жизни и бизнеса” — вот то ядро, на основе которого нужно придумать свою идею. “Чат” значит общение, речь и текст. Поэтому и без знаний NLP — “natural language processing”, или “обработки естественного языка” — никуда.
Как известно, студенты лучше всего воспринимают информацию в ночь перед экзаменом, и мы, руководствуясь той же логикой, решаем участвовать, а знания NLP совершенствовать по ходу. Такие вещи, как простая классификация текстов, нам уже давно подвластна, но в рамках конкурса нам предстоит разобраться и в Q&A-системах, алгоритмах реферирования, — в общем, все становится интереснее.
До первого декабря участники подавали заявки с описаниями идей, и, надо сказать, составить заявку оказалось делом непростым. От нас попросили и бизнес-модель будущего приложения, и техническую осуществимость, и архитектуру, и конечную выгоду для пользователей и наших партнеров. 
Мы — классические студенты-технари, и с бизнес-моделями были до конкурса и не знакомы толком. Однако, благодаря хорошей книге Остервальдера и Пинье «Построение бизнес-моделей» смогли изобразить что-то похожее на правду.
Заявки, конечно, видят, только судьи, однако участникам видны название и описание — summary — идей соперников. 
Мы — четверо студентов Воронежского госуниверситета. Четверо достаточно творческих людей, любящих искусство и музыку. Возможно, поэтому мы и стали отталкиваться не от бизнес-применений чатботов, а от далекой для денежной выгоды темы: живописи.
Было бы вам интересно поговорить с роботом о картине, что видите? Обсудить детали, поделиться впечатлениями? 
Для нас ответ очевиден: конечно, было бы интересно! Отлично, а как лучше всего общаться с системой, если не хочется смотреть в телефон? Конечно, голосом.
Так и была сформирована первичная идея: бот, с которым вы говорите об искусстве и том, что видите. 
Совершенство любого интерфейса в его отсутствии. Вы просто говорите, что думаете, и получаете ответ. Нет сценария, рамок или команд. Словом, “дзен”, — и в этом мы видим ценность и будущее подобного рода систем и приложений.
До 1 декабря заявки были поданы, и через неделю, 7 декабря, нам был открыт список идей других команд. Их оказалось очень много: 204.
Вот некоторые, которые нам понравились:
А вот… странные:
Читать описание каждой из идей, конечно, лень. А если программисту что-то делать лень, то он пишет для этого программу, не так ли?
Сначала все просто: импортируем библиотеки и скачиваем данные с сайта.
Наш промежуточный результат:

В колонке url как раз содержатся странички с каждой из идей. На них, впрочем, новой информации немного: состав команды и текст описания. Скачаем и их и присоединим к нашему исходному dataframe:
И финальный результат — в студию!

Ну что, посмотрим, с чем имеем дело? Что у нас со странами?

Ну и дела! Студентов из Индии не в пример больше. 

Действительно: почти половина всех участников — ребята из страны священных коров и экономичных космических программ.
Доли остальных участников по странам распределились вот так:

А как дела с университетами?

Да. Вот так. Неутешительно. Все три команды-участника из России из одного и того же университета — нашего. Где вы все, ребята, ну как так?..

Индию нам лишь удалось обойти лишь в силе дружбы: на графике видно, что наши команды в среднем более полные.
Ну что, приступим к самому мясному, — к тексту?
Как бота назовешь, о том он говорить и будет. Может, тренды этого года в тематиках ботов будут видны еще из названий проектов? Давайте посмотрим:
Уберем очевидно не относящиеся к теме слова, которые, однако, наверняка будут всплывать (“chat”, “chatbot”, “intelligent”, “system” и другие) и построим частотный словарь:

Правда уже проглядывается: в этом году популярны боты, призванные помогать со здоровьем. Их догоняют боты, посвященные путешествиям и покупкам. Очевидное ли применение? Здравое? Хорошая реализация проектов покажет.
Приступим теперь к обработке текстов описаний идей.

Из возникающих слов попарно образуются смешные, но, в общем-то, правильные утверждения: “пользователи используют”, “чатбот помогает” и “также система предоставляет”. Обращая внимание на самые встречаемые слова, можно легко представить, как выглядит средний текст с описанием: он о том, что проблемы решаются с помощью бота, проект технологичный и служит людям. Все правильно! Но надеюсь, что судьям хватит терпения дочитать эти две сотни альтруистских од, ..
.., если же терпения не хватит, с помощью знаменитой gensim можно попросить вывести краткое содержание всей этой массы:
И ключевые слова:
А потом я подумал, что могу заставить генеративные модели сделать новые описания новых идей! Погнали!
Особо классные моменты так и бросаются в глаза:
После того, как результаты настолько заинтриговали, можно найти оригинал идеи и посмотреть, в чем же состоит ее суть. 
Так как все любят облака слов (и не говорите, что вы не один из таких!), я не мог его вам не сделать.

Впереди нас ждет еще много работы, и мы надеемся, что ее результатом мы не будем разочарованы. Судьям предстоит не менее сложная работа: в заявленных темах встречаются и здравоохранение, и рекрутинг, и шопинг, и образование, и путешествия, и даже трейдинг и биткоины! 
А пока мы на позиции низкого старта. Подготовили твиттер-аккаунт для того, чтобы писать туда заметки о проходящей разработке. Кстати, рекламный мини-видеоролик даже сняли, если вам будет интересно.
Буду рад выслушать любую критику, комментарии к вашим услугам.
Пользователь
=====
«3 класса приходской школы» или учимся в Microsoft Virtual Academy (MVA)
2017-12-08, 18:56
Технический писатель
=====
10 самых трендовых и мертвых технологий, фреймворков и языков по версии Stack OverFlow. Коротко
2017-12-08, 18:58
PHP developer
=====
Генерация естественной речи в колл-центре 3CX, основанная на глубоком обучении
2017-12-08, 19:13
User
=====
ESET приняла участие в ликвидации ботнета Gamarue
2017-12-09, 13:57
User
=====
Бизнесу не нужны интернет-маркетологи или почему “Уволить своего интернет-маркетолога” — это лучший совет 2017 года
2017-12-08, 19:49
Маркетолог
=====
Переход из тестировщика в руководители проектов
2017-12-09, 11:22

Обычно на должность руководителя проектов в IT-компании требуются люди с опытом от 1 года. Поэтому часто неопытные менеджеры устраиваются на работу аналитиками, тестировщиками, иногда даже разработчиками.
Если хорошо себя проявить, то со временем вам будут доверять больше управленческих задач. При этом не всегда получается отказаться от старых обязанностей. Приходится совмещать две роли на проекте. Так и я, имея опыт в тестировании и аналитике, дополнительно стала получать задачи руководителя проекта. Со временем я полностью перешла в управление проектами.
В этой статье я делюсь наблюдениями и выводами. Как в одном человеке конфликтуют привычки тестировщика и обязанности руководителя проекта? С какими проблемами приходится сталкиваться? Какую пользу можно извлечь при таком переходе? Если хотите получить ответы на эти вопросы, добро пожаловать под кат.
Плохая новость: в начале перехода старые задачи никуда не делись, появились ещё и новые обязанности. Задач много, все они важные. Возникало огромное желание клонировать себя, чтобы выполнять их одновременно.

Увы, но задачи менеджмента гораздо важнее задач тестировщика, если вам приходится совмещать. Теперь вы ответственны за всех в команде, и откладывание управленческих задач чревато простоем других.
Что делать? Мне помогало составление списка задач, опираясь на вопрос: «Если отложу эту задачу, как это повлияет на проект?», – и соблюдение полученного плана. Теперь главное — не баги, а задачи, сроки и бюджет проекта.
Многие «переходящие в менеджмент» специалисты с этим сталкиваются. Времени на полноценное погружение в тестирование не остаётся. Тестирование либо откладывается на последний момент и тормозит разработку, либо страдает от некачественного исполнения. Оба варианта неприемлемы.

Что делать? Закрепите или развейте проблему фактами. Мне помог дотошный трекинг времени всех выполненных задач.
Проанализировав полученные результаты, я выявила два направления, над которыми нужно было поработать.
Первое направление: неэффективное выполнение определённого типа задач.
Например, у меня это были долгие переговоры с заказчиком и затянутые по времени митинги.
В таком случае нужно думать, как оптимизировать, экспериментировать. Пока не решите этот момент, нет смысла просить выделить ещё людей в помощь на проект.
Второе направление: задачи, которые каждый раз отодвигаются как неприоритетные, пока не начнут «подгорать».
В частности для меня, задач менеджмента становилось больше, так как команда росла. Задачи тестирования откладывались до последнего. Началась работа сверх нормы, чтобы успеть всё и везде. На меня отрицательно влияли систематические переработки. Я была неэффективна и особого энтузиазма не было.
Помогла расстановка приоритетов и трекинг времени. Не бойтесь консультироваться с руководством, правильно ли вы оцениваете важность задачи. Это нормально, так как у вас ещё мало опыта в менеджменте. Таким образом, вы покажете проблему изнутри.
Если начальство умеет слушать и ваши аргументы будут адекватными, то вам в помощь найдут специалиста. С прискорбием сообщаю очевидное: есть большая вероятность, что с вас снимут задачи тестировщика.
По инерции я была в курсе всего на проекте. С одной стороны, хорошо всё понимать. С другой стороны, начинался перегруз информацией, и был большой риск упустить что-то важное.

Раньше я наизусть могла рассказать, в каком месте какая может возникнуть ошибка и как она обрабатывается. Теперь на выяснение этого просто не хватало времени. И я корила себя за то, что не в курсе всей информации по проекту.
Что делать? Понять, что за детали теперь несут ответственность другие члены команды. Задача руководителя – знать, у кого можно быстро получить информацию при необходимости.

Я понимала, что могу проверить лучше, найти баг быстрее. У меня был постоянный соблазн перепроверить.
Что делать? Выдохнуть и дать команде самостоятельно заниматься задачами и решать возникающие проблемы, если время ещё есть. Задача руководителя – обеспечить комфортное выполнение задачи всем членам команды. Не надо мешать своей преждевременной помощью.
Я наблюдала за проектной деятельностью руководителей, которым повезло сразу начать свой путь с менеджмента. Они легко оперируют абстракциями, не пытаясь понять, что там внутри. Зато с помощью своих навыков тестирования, я выручала команду в кризисных ситуациях.

Что делать? Принимать всю ответственность на себя. Не винить кого-то конкретного. Учиться выражать обоснованное недовольство команде, когда это требуется. Транслировать претензии заказчика только в виде задач. Уметь защищать команду. Стараться понимать настроение в команде.
Я умела принимать ответственность на себя. Сложнее оказалось найти нужные слова, когда возникали проблемы. Надо хорошо разбираться в людях, чтобы неосторожным словом их не демотивировать. Это непросто сделать, ведь часто руководители проекта работают с теми людьми, которых им назначили на проект. К каждому нужен свой подход.
Кому-то из команды наверняка будет сложно перестроиться на то, что тестировщик стал руководителем, и нужно отчитываться перед ним. Увы, это исправит только время. Надо будет доказывать команде, что вы в состоянии принимать хорошие управленческие решения и можете контролировать процесс.

Нужно вовремя сказать «стоп» и отсеять то, что может подождать. Это может оценить только руководитель проекта, он в курсе всей финансовой стороны и отвечает за соответствие планам.
Что делать? Теперь всё нужно рассматривать через призму «время – деньги». Важно выпустить релиз вовремя. Иногда лучше обсудить проблему с заказчиком и совместно расставить приоритеты, нежели пытаться всё-всё-всё пофиксить.
В этом кроется большая пропасть между руководителем проекта и отделом тестирования. Тестировщики обнаруживают проблему и испытывают радость, что они выполнили свой долг. Руководитель боится обнаружить проблему, так как это затянет срок выполнения задачи.

Невозможно избежать всех проблем. Они возникают как в команде, так и со стороны заказчиков. Когда в команде появлялся недовольный, то это быстро распространялось на других.
Что делать? Не поддаваться панике, пресекать её безосновательное распространение на проекте. Сосредотачиваться на поисках решения. Спокойно относиться к недовольству и сохранять здравый рассудок. Меньше эмоций.
Недовольство – это нормально, потому что все хотят улучшить проект. Только у безразличных нет претензий. Надо учиться с этим работать.
В этом списке я обозначила те проблемы, с которыми сталкивалась сама при переходе из тестирования в менеджмент. Часть из них удалось преодолеть, по другим осознание пришло только после наблюдения за более опытными коллегами.
На должности руководителя проекта приходилось учиться жить с такими особенностями:
Полезные навыки, которые удалось приобрести:
Но это всё только личный опыт и наблюдения. Не бойтесь пробовать новое.
User
=====
Об итогах конкурса MERC-2017: интервью с победителями
2017-12-08, 21:23
User
=====
Взаимодействие веб-страницы с Ethereum
2017-12-08, 22:41
Учитель нейросетей
=====
Angular 5 (или 4): даунгрейдим компонент для использования в AngularJS
2017-12-09, 00:58
React + Python Developer
=====
Местоопределение Wi-FI источников в AR и котелок
2017-12-09, 07:06


Есть пеленгатор Wi-Fi точек доступа, описанный здесь, и устройство дополненной реальности. Требуется подсветить местоположение заданной Wi-Fi точки.
Кратко напомню, что пеленгатор работает по принципу холодно-горячо: показывает насколько хорошо мы нацелились на объект. 
Для того, чтобы определить местоположение источника сигнала, требуется пересечь несколько лучей-пеленгов, желательно из разных позиций. И тут начинаются проблемы. При чём, проблемы начинаются даже в двумерном случае (когда мы хотим местоопределять на карте): три и больше лучей ну никак не хотят пересекаться в одной точке. 
А в нашем случае даже два луча не хотят пересекаться. 
Итак, точного решения у нас не получится. Придётся искать приближённое.
Базовый алгоритм основан на разбиении пространства на кубики. Для каждого кубика мы будем хоронить хранить, сколько лучей через него прошло. Несколько самых популярных кубиков нарисуем. Мне больше нравятся шары, поэтому я буду рисовать описанный вокруг куба шар.
Так как у нас лучей относительно мало, то предвидятся сильно разреженные данные, поэтому будем их хранить в хеш-таблице. В качестве ключа у нас будет пара координат кубика, а в качестве значения — число прошедших лучей.
Каждый пеленг имеет координаты начала (местоположение пеленгатора) и направление. Ввиду ограничений суровой реальности, мы, разумеется, пересекаем не лучи, а отрезки. Мало того, что мы ограничены памятью компьютера, так ещё и не можем засечь Wi-Fi точки на бесконечном удалении от нас :(
Для того, чтобы отметить кубики, в компьютерной графике есть алгоритм Брезенхэма. Правда, экраны сейчас пока только двумерные, поэтому везде описан алгоритм Брезенхэма на плоскости, но ничего, адаптируем для 3D.
Прежде, чем читать про реализацию, посмотрите видео, как это работает:
Камера, к сожалению, не хотела фокусироваться на телефоне, но при желании можно увидеть, что название точки, которую мы ищем и найденной совпадают.
Легенда гласит, что когда яхта Петра I впервые подошла к берегам острова Котлин, охранявшие его шведские солдаты дали стрекача так быстро, что оставили после себя горящий костёр, на котором в котле готовилась еда. И именно из-за этого происшествия остров получил своё название.
Среди массовых устройств дополненной реальности, по прежнему, самые точные и доступные — это устройства на базе Google Tango. Поэтому, будем использовать их.
На вход нам подаётся текущая "поза" Tango устройства и событие, что в данный момент устройство с некоторой точностью направлено на источник сигнала. Для алгоритма Брезенхэма требуется получить начало и конец отрезка.
Но перед тем, как начать искать конец отрезка, посмотрим, получится ли найти заранее неизвестную Wi-Fi точку?
Для того, чтобы получить конец отрезка, требуется к z-компоненте точки начала прибавить его длину, а затем повернуть. Почему к z? Потому что в данной системе координат z — это "вперёд". 
Матрицу поворота мы берём из библиотеки Tango:
Самое время посмотреть, как мы находим Wi-Fi точку кафе Шахерезада:
Назначим координатой x ту, на которую самая длинная проекция отрезка нашего пеленга.
В алгоритме Брезенхэма каждый шаг идёт инкремент координаты x, а координата y увеличивается только если набежала достаточная ошибка. Мы же будем делать то же самое ещё и с координатой z. 
Функция addPoint, которая увеличивает счётчик проходящих через куб лучей:
Выбираем самые популярные кубики: сортируем, а затем берём первые несколько. В текущей реализации не более трёх.

Был рассмотрен подход к решению задачи пространственного местоопределения объекта по заданным пеленгам и предложена его реализация.
Весь исходный код доступен на GitHub.
Конструктивная критика от мастеров Котлина (и не только Котлина) приветствуется.
Спасибо за внимание!
Программист
=====
Решение задачи оптимизации многоступенчатых ракет
2017-12-10, 16:10
Разработчик учебных программ
=====
Rust vs. C++ на алгоритмических задачах
2017-12-09, 11:08
Software Developer
=====
Как заменить старые индексы и не сломать систему?
2017-12-11, 16:21
Пользователь
=====
Arrays, Collections: Алгоритмический минимум
2017-12-09, 15:18
Product manager (ex. team lead, programmer)
=====
Руководство по блокчейн для маркетолога
2017-12-09, 16:03
Основатель Блокнота инвестора (inotepad.ru)
=====
Websockets. Возможный подход по использованию
2017-12-09, 16:26
Пользователь
=====
Основы и способы информационной безопасности в 2017 году
2017-12-09, 17:20
User
=====
Лекция Яндекса: Advanced UI, часть вторая
2017-12-10, 11:00
Интернет щей
=====
Убунту OpenBox, установка и настройка
2017-12-09, 19:12
С возрастом всё больше становится жалко времени на бесплодные ёрзанья мышкой; всё больше раздражения вызывает загружаемый с каждой DE какой-то бесконечный хлам, который съедает время и ресурсы с малопонятными целями. И начинается традиционное нисхождение: от KDE к XFCE, потом к LXDE. Наверное, когда-нибудь я дойду и до голой консоли на десктопе. Но пока меня остановил OpenBox.
«CrunchBang своими руками» – так чаще всего называются подобные руководства. Некоторые пишут совсем коротко: «Посмотри конфиги CrunchBang и настрой по ним свой OpenBox!»

Я посмотрел конфиги CrunchBang. Только он уже не CrunchBang, а BunsenLabs называется. Установил его на своём ноутбуке для изучения – wi-fi не работает: заводится и тихо умирает. Думаю, это из-за старого ядра – наблюдал такое раньше. Попытался перенести конфиги в Убунту – не получилось: там половина команд начинается с префикса «bl-», понятно, что это будет несовместимым с другими дистрибутивами. Стало очевидно, что лёгкого пути не будет.

Изначально для моей «голой» системы было два варианта: Дебиан netinst или Убунту сервер. После того как новенький Дебиан 9.1 гордо отказался конфигурировать Mysql (точнее, Mariadb) – даже после угрожающих размахиваний напильником и чтения вслух гневных нецензурных заклинаний с ЛОРа – остался только один вариант.


Установил Убунту сервер 16-с-чем-то. На работе. Пришёл домой, чтобы тихо посидеть над OpenBox'ом, а на ноутбуке уже интернет не работает. Потому что на работе я в него перед установкой провод воткнул, чтобы долго не разбираться. А дома вай-фай. И что? Куда тыкать мышкой, чтобы выбрать беспроводную сеть? Пошёл на работу, воткнул провод, установил lubuntu-desktop, wi-fi появился, красота! Потом анекдот про двух ковбоев вспомнил: «А тебе не кажется, Джон, что мы оба забесплатно дерьма поели?» То есть Lubuntu я и так мог скачать и установить, сразу, готовую, а не таким хитрым способом. Задача была немного другая.


Путь оказался ещё менее лёгким. Пришлось разбираться с этими вашими /etc/network/interface'ами. Разбирался, читал, экспериментировал. Для начала узнал, как устанавливать сами «окна» – после установки «голой» системы, надо в консоли писать что-то вроде:
Вы спросите: ну ладно ещё Xarchiver, но причём тут Geany? – Так, для примера; там большой хвост ещё из разных предпочтений тянется от разных пользователей в интернете — я ведь не первый, кто пытается настраивать OpenBox для себя. Но это всё не очень принципиально. Важно то, что это неполный набор, неработающий!

В процессе экспериментов с некоторыми «голыми» debian-based ОС я выучил наизусть, что для возможности подключения к wi-fi мой файл /etc/network/interfaces должен выглядеть так:
Это меня и подвело – то, что выучил наизусть. Файл-то я скопировал, а wi-fi не работает. Потому что где-то в середине экспериментов у меня возник вопрос: вот я отказался от LXDE, так, может, и Lightdm для OpenBox'а не нужен? И ответ у меня получился положительный. То есть я его в итоге не стал устанавливать, совсем. Авторизуюсь в консоли, делаю там что-то, а потом вдруг мне хочется в окна мышкой потыкать, и я пишу сакраментальное:
Потому что у меня задачи такие – в основном работа с сервером; и иногда хочется эту работу сделать удобнее – ну, там, несколько вкладок в файл-менеджере использовать, или в соседнем окне фильм посмотреть. Я не проверял, устанавливаются ли «беспроводные инструменты» вместе с Lightdm или эти инструменты тянет за собой LXDE какое-нибудь, но без Lightdm и без LXDE – точно не устанавливаются. Поэтому более правильно команда установки должна выглядеть так:
Те, кто не знает ещё своего длинного зашифрованного ключа для вайфая, могут записать его в текстовый файл wpa-psk.txt так:
Ещё деталь: волшебное слово wlp1s0, для обращения к вашему сетевому интерфейсу, можно узнать, например, командой:
И там у вас может оказаться совсем другое слово – например, wlan0.
Когда же мне надоест руками править файл /etc/network/interfaces при переходе из одного здания в другое, я тоже знаю, что мне надо будет сделать – установить NetworkManager:
Потом создать файл в своём домашнем каталоге ~/.config/openbox/autostart (если его ещё нет) и дописать туда две строчки:
В сети описываются проблемы с апплетами для OpenBox'а, возникающие иногда из-за неправильной последовательности загружаемых модулей. Одно из решений я нашёл такое – вместо простого «tint2 &» написать:
Видимо, предлагаемая задержка в 2 секунды помогает автозагрузчику OpenBox'а пережить разный сторонний загружаемый бутор и потом без помех отобразить желанную иконку сетей, по которой можно щёлкать мышкой. На сайте wiki.archlinux.org я видел ещё более «медленную» конструкцию:
В целом, конечно, такая стратегия выглядит как-то сомнительно: мы же в итоге стремимся к «мгновенной» загрузке и выгрузке «иксов», иначе зачем нам этот путь упрощений – от KDE к XFCE, LXDE, OpenBox'у?

Куда делись из моей команды установки OpenBox Коньки (Conky)? Это такой апплет (или виджет?), который прямо на рабочем столе показывает, сколько у вас осталось памяти и не слишком ли перегрелся процессор. Я понял, что он мне так же не нужен, как и Lightdm, и Xcomppmgr из примера выше. В развитой мерфологии есть такая рекомендация: перед тем как пойти сдавать анализы в поликлинику, определите, что вы будете делать, если анализы положительные; подумайте, что будете делать, если анализы отрицательные; сравните ваши действия – если они одинаковы в обоих случаях, зачем вам вообще идти в поликлинику?


Зачем мне знать, сколько осталось памяти? Если комп уже начал сильно тормозить, я и так знаю, что делать, – например, закрыть все открытые окна («Если вы что-нибудь открыли – закройте»). И я знаю (чувствую пальцами), когда ноутбук перегревается, и тоже знаю, что делать, когда он начал перегреваться постоянно – например, можно попробовать отнести его на профилактику.


Ну, иногда бывает нужно посмотреть, кто конкретно больше памяти жрёт – Firefox или Chromium. Не для практической пользы, конечно, а чтобы вставлять потом умные замечания в дискуссии и базары на форумах. Но для этого постоянно работающий апплет не нужен, достаточно запустить на минутку в консоли команду: 
Как же жить без этого? Я бы вполне удовлетворился правкой файла /etc/default/keyboard:
Если бы не две проблемы: 1) не всегда правка этого файла помогает – конфиги клавиатуры во время загрузки оконной системы читаются ещё из нескольких тайных мест; 2) мне удобнее, когда в разных открытых окнах запоминается разная раскладка, а в конфиге /etc/default/keyboard такое поведение окнам назначить нельзя.
Поэтому устанавливаем ещё одну программу:
Впрочем, она уже записана в нашу общую «правильную» (Вариант 1) команду установки OpenBox. Потом создаём файл ~/.xxkbrc с примерно таким содержанием:
В управляющей полоске каждого окна будет отображаться флажок языка, и каждое окно запоминает текущую раскладку. И да, сами клавиши переключения раскладки надо записать в файл ~/.config/openbox/autostart в виде такой команды:
Мои наиболее частые действия на компьютере – открыть файл и что-нибудь написать в нём. Иногда это связный русский текст, иногда что-то вроде
Часто приходится открывать файлы по сети – хотя бы чтобы просто посмотреть, что там написано. Поэтому мне совершенно необходим такой «стек» программ: файл-менеджер – фтп, smb, или fish клиент – текстовый редактор с подсветкой и проверкой орфографии. И ещё желательно не вводить каждый раз пароли для удалённого доступа.
Такие задачи можно решать, например, с помощью связки программ Krusader – Kate. А Kwalletmanager для хранения паролей сам с ними устанавливается. Да. И ещё треть KDE, наверное. Но тут уж ничего не сделаешь. Пробовал Tux-commander – он открывает файлы по сети, но не сохраняет обратно, во всяком случае «из коробки»; да и в целом возможности явно беднее, чем у Krusader'а. Те же проблемы и у «родных» файловых менеджеров KDE, XFCE, LXDE. То есть кому-то это всё равно, а для меня – проблема.
Таким образом, команда установки удлиняется на пару пунктов: krusader kate krename kdiff3 unrar konsole. Конечно konsole – а иначе что будет открываться в Krusader'е при нажатии клавиши F2 (или, в новой версии, F9)?

Русификация самой системы мне не нужна: мне привычно и удобно в файл-менеджере набирать три буквы «doc», чтобы переместиться на папку Documents; а если папка будет называться «Документы», придётся переключать раскладку – уже не очень удобно. Ну, или в Krusader'е Alt+s – «Settings» и прочие обозначения уже привычны; как это будет по-русски, мне даже страшно подумать.
А вот проверка орфографии – хотя бы от элементарных опечаток – необходима. Её в Kate обеспечивает aspell (или ispell? всё время их путаю). К ним надо локальные настройки скачивать. В Либреофисе – hunspell, к нему тоже надо словари. Ну и словари для Firefox – отдельная песня. 

Итого, вместе с самим Либреофисом и Firefox'ом команда установки получится: 
Совсем забыл про mc – его в Убунту-сервере по умолчанию нет, и я его поставил первым, ещё в голой консоли. Geany – для страховки: не с любым файлом удобно работать в Kate. 


«Зачем же тебе l10n-ru, если тебе не нужна русификация интерфейса?» – обязательно спросит, гаденько усмехаясь, наблюдательный линуксоид. – Не для себя. Это моя старая боль в спине (или ниже?): установишь кому-то в очередной раз линукс – и тут же вопли: я ничего не понимаю! тут всё не по русски! Вот у меня уже и выработался рефлекс, можно сказать. Хотя вряд ли, конечно, при уводе очередного клиента с Виндовс я буду ставить ему OpenBox. Но готовиться надо: иногда попадаются пользователи, совершенно не способные постоять за себя и готовые послушно заучивать все эти Alt+F2 и Win+w.

Gmrun нужен, чтобы вызывать программы не мышкой из списка в меню, а непосредственно по Alt+F2, для чего в файл ~/.config/openbox/rc.xml должно быть, конечно, записано:
Например, захотите вы компьютеру сказать shutup! shutdown -h 0 – вот и Alt+F2 пригодится. Конечно, не очень удобно для вызова программы полностью набирать её название. В Gmrun по умолчанию работает автодополнение названий программ при нажатии клавиши Tab. Это поведение можно изменить – чтобы подсказки выскакивали сразу и сами, без Tab. Для этого надо создать в домашней папке файл .gmrunrc и записать в него строку: TabTimeout = 1. В этом же файле можно указать некоторые другие настройки; полный их список можно посмотреть в файле /etc/gmrunrc; а лучше скопировать этот файл в ~/.gmrunrc и внести на новом месте нужные изменения.
Gmrun Alt+F2 с автодополнением программ – это, в сущности, замена вызова меню по Alt+F3 в XFCE или Alt+F1 в KDE, только без нудного лазанья по ответвлениям вида «Настройки», «Система», «Интернет»… К сожалению, собственных «actions» OpenBox'а там нет. Например, нельзя через Gmrun вызвать команду Exit – выгрузить сам OpenBox и вернуться в консоль. Или Restart – обновить конфигурацию после правки конфигов. Поэтому несколько команд всё равно приходится заучивать отдельно: Ctrl+Alt+r – Restart, Ctrl+Alt+0 – Exit. А чтобы они работали, надо добавить в файл ~/.config/openbox/rc.xml следующие записи:
Возможно, такие штатные менеджеры, как Thunar или PCManFM умеют монтировать флэшки. Или даже авто-монтировать при вставке. Мой Krusader в «минимальном» OpenBox'е не осилил: устройство он показывает, предлагает смонтировать, но потом говорит, прав не хватает. В режиме рута монтирует, но русские буквы отображаются вопросиками. То есть нужно, видимо, что-то ещё там скриптами подстраивать или дополнительными утилитами. Я решил проще – добавил в /etc/fstab две строчки:
Наверное, надо ещё и sdb2 на всякий случай, но мне для моих флэшек хватает. Теперь Krusader монтирует и отмонтирует вставляемые флэшки правильно. Если у вас есть нужные для работы дополнительные разделы на локальных дисках, их тоже лучше сразу прописать в /etc/fstab. 

Остался вопрос: зачем в конце получившейся команды установки OpenBox (Вариант 2) написано mpv? С учётом того что мне ещё надо настраивать на моём Убунту сервере LAMP или QEMU, этот вопрос кажется мне каким-то мелким, не стоящим внимания.
Гораздо интереснее узнать, в чём я ошибся в своей настройке OpenBox и какие варианты настроек ещё бывают; что я забыл. Скажем, иногда надо видеть два окна сразу — например, рисунок клавиатуры и текстовый редактор (для изучения «слепого» набора). Можно ведь аккуратно разместить эти окна на экране клавиатурными сокращениями. А потом как-то заставить OpenBox запомнить это расположение окон. И вообще сохранить всю сессию. То есть тут есть над чем работать.
P.S. Чуть не забыл: если вы не используете NetworkManager, строчку auto wlp1s0 в файле /etc/network/interfaces лучше закомментировать и использовать для подключения к сети команду в консоли: 
(Вместо wlp1s0 — название вашего беспроводного интерфейса). Иначе загрузка Убунту может «зависнуть» минут на 5 в поисках несуществующей точки доступа к wi-fi.
UPD. Флешки монтировать руками, конечно, довольно неудобно. В итоге решил эту проблему установкой pcmanfm: видимо, он устанавливает необходимые утилиты, и после его установки krusader тоже нормально монтирует подключаемые устройства.
Заодно и network-manager установил. Но польза от периода работы без него осталась: теперь я знаю, как подробно посмотреть, почему не подключается wi-fi (а с network-manager'ом щёлкаешь по иконке — и ничего не происходит, и не видно, где проблема).
Ещё в процессе работы сформировался набор приложений, которые постепенно доустанавливались. В итоге общая команда установки на Убунту Openbox, утилит и рабочих программ сильно выросла. Я разделил её на две части. После выполнения первой части можно выдернуть сетевой провод и дальше работать по wi-fi.
Скорее всего, wpasupplicant и wireless-tools не нужно устанавливать отдельно, если устанавливаем сразу network-manager — это как бы варианты.
В arch linux нужно ещё отдельно устанавливать пакет kio-extras, иначе не будет работать sftp в Krusader'е.
Synaptic удобен для поиска — когда точно не знаешь название программы. Lm-sensors установил от страха — когда вдруг показалось, что ноутбук греется. И ещё: не ищите замечательный справочник zeal (ссылку на который дал Shtucer) на других линукс-платформах — кажется, такой готовый пакет есть только для Убунту.
php, javascript, postscript, линукс
=====
Мульти-арные функции в Java
2017-12-09, 20:40








Системная Архитектура, Программирование
=====
Язык Lua и Corona SDK (1/3 часть)
2017-12-09, 21:19
User
=====
Жизнь в Unity Asset Store. Кратко
2017-12-10, 00:52
Программист
=====
Фэйковый дизайн
2017-12-13, 22:49
Пользователь
=====
Язык Lua и Corona SDK (2/3 часть)
2017-12-10, 02:50
User
=====
Dagger 2 для начинающих Android разработчиков. Dagger 2. Часть 1
2017-12-10, 07:44
Android devepoler
=====
Разреженные матрицы: как ученые ускорили машинное обучение на GPU
2017-12-10, 16:17
Пользователь
=====
Непрерывная интеграция и развертывание Docker в GitLab CI
2017-12-10, 13:06
DevOps Software Engineer
=====
Введение в VxLAN
2017-12-10, 17:02
Сетевой инженер
=====
Ускорение сайта. Как понять, актуально ли это для вашего сайта
2018-03-05, 15:29
User
=====
Дайджест интересных материалов для мобильного разработчика #233 (4 декабря — 10 декабря)
2017-12-10, 14:49
Пользователь
=====
Как читать техническую литературу: советы резидентов Quora, Reddit и Hacker News
2017-12-10, 17:19
Пользователь
=====
Прототип платежной криптосистемы. Авантюрный проект
2017-12-10, 16:54
Разработчик
=====
Selenium: Накачиваем Мышцы
2017-12-12, 10:53
Прошло довольно много времени с момента нашей последней статьи об эффективной Selenium-инфраструктуре. Если вы находитесь в самом начале непростого пути Selenium — советую ознакомиться с нашими статьями про масштабируемый Selenium (часть I, часть II), Selenoid — универсальный инструмент для автоматизации тестов в браузерах (раз, два), Selenium под Windows (ссылка). Если вам больше нравятся мотивирующие рассказы — посмотрите видео моего доклада про масштабируемый Selenium на SeleniumConf Berlin 2017.

С момента публикации последней статьи в нашем сообществе произошло много интересного. Сегодня я хочу рассказать о самых важных возможностях, добавленных в наши инструменты за последние месяцы.
Наш Selenium балансировщик получил несколько крутых улучшений.
При добавлении таких флагов любые браузеры из файла test.xml будут доступны без указания пароля.
Обратите внимание на протокол ws://, означающий WebSocket. Используя VNC-клиента, поддерживающего передачу данных через веб-сокеты (например, noVNC) вы можете увидеть экран браузера любой запущенной сессии. По-умолчанию Ggr ожидает, что VNC-сервер запущен на стандартном порту 5900, но это можно перенастроить.
Наиболее многочисленные изменения были сделаны в нашем, как говорят, "флагманском" open-source продукте — Selenoid. Selenoid — это полноценная замена Selenium-хаба, запускающая браузеры в Docker контейнерах:
Теперь вы имеете гораздо больше возможностей по настройке окружения браузера. Например, в конфигурационном файле вы можете задавать любые переменные окружения, записи файла /etc/hosts и значение shmSize для операционной системы внутри контейнера, где будет запущен браузер:
Это позволяет, например, переопределять для каждой версии браузера часовой пояс или добавлять хосты из внутренней сети без изменения настроек DNS. В дополнение к этим настройкам вы можете изменять некоторые параметры отдельно для каждой запущенной сессии при помощи capabilities.
1. Если вы запускаете несколько тестов параллельно, то, чтобы различить их в Selenoid UI, вы можете указать капабилити name с произвольной строкой внутри:
Вот как это выглядит в UI: 

2. Если тестируемое приложение также запускается в Docker контейнере — вы можете автоматически связать (link) контейнер с браузером с контейнером приложения, указав его имя:
3. Иногда требуется переопределить содержимое /etc/hosts только для одного теста. Это можно сделать так:
4. Наконец, чтобы переопределить часовой пояс для одной сессии — укажите:
Мы добавили два важных улучшения в логирование:
1. Если браузеры запускаются в контейнерах — любые логи, отправленные в централизованное хранилище логов (такое как Amazon CloudWatch или Google Cloud logging), могут теперь быть помечены произвольной меткой. Значение метки указывается при помощи capability name, о которой я писал в предыдущем разделе.
2. Если вы запускаете Selenoid без Docker — вы, наконец, можете включить логи веб-драйверов в логи Selenoid. Для того, чтобы это заработало, нужно добавить флаг -capture-driver-logs при старте Selenoid:
Мы пересобрали все образы с браузерами, добавив поддержку всех UTF-8 локалей и дополнительные шрифты для правильного отображения таких символов как:
Мы полностью переделали внешний вид Selenoid UI — графического веб-интерфейса для Selenoid.

Наиболее часто используемые возможности такие как статистика использования браузеров и кнопки для просмотра сессий теперь располагаются на главной странице. Реже используемый экран выбора capabilities был унесен на отдельную вкладку.

Экран запущенного браузера и логи сессии теперь показываются на одной экране рядом друг с другом.
Configuration Manager — маленькое приложение, значительно упрощающее установку наших инструментов, стало еще более удобным. Теперь поддерживается возможность настройки Selenoid для работы с Microsoft Edge и Safari. Свежие версии CM работают без проблем на Windows 10. Вы также можете переопределить порт, на котором слушает Selenoid и Selenoid UI, например, чтобы одновременно запустить Selenium server и Selenoid.

Мы также сделали вывод CM приятным для глаза, раскрасив логи разными цветами.
Вишенкой на торте является недавно добавленная возможность записывать видео браузерных сессий. В дополнение к возможности смотреть на экран браузера в реальном времени в Selenoid UI теперь стало возможно записать видео с экрана браузера и сохранить его в файле формата H264. Видео выглядит примерно так:
Для того, чтобы записать видео, просто добавьте одну capability в тесты:
По-умолчанию все записанные видео называются <sessionID>.mp4, где <sessionID> — уникальный идентификатор браузерной сессии, который можно легко вытащить из логов теста. Если вы хотите использовать свое имя — добавьте capability videoName:
Selenoid автоматически предоставляет доступ к каталогу с видео по HTTP. Чтобы открыть файл в браузере, используйте URL:
Чтобы посмотреть весь список файлов — сотрите имя файла:
Я надеюсь теперь у вас есть гораздо больше мотивации, чтобы настроить в своей команде действительно эффективную инфраструктуру Selenium. Поверьте, запуск тестов в браузерах может быть безболезненным! Если у вас есть какие-то вопросы — не стесняйтесь писать нам на почту, в Telegram-канал поддержки или отправляйте вопросы на StackOverflow тег. Если вы уже используете Kubernetes в своих процессах — вас также может заинтересовать наш новый продукт — Moon, который был специально разработан для развертывания эффективного кластера Selenium в Kubernetes и поддерживает все лучшее, что есть в Selenoid.
До новых встреч.
Разработчик
=====
Создание собственных PHP функций в Laravel проекте
2017-12-10, 17:18
В Laravel есть много отличных функций которые упрощают работу с массивами, путями, строками, маршрутами, и прочими вещами — например любимая функция dd().
Вы можете создать и свои собственные функции для вашего Laravel приложения и PHP пакета, используя Composer для автоматического импорта их.
Если вы новичок в Laravel или PHP давайте пройдемся по всему процессу создания собственных PHP функций которые будут автоматически подгружаться в Laravel'ом.
Для начала вы должны включить функции в контекст вашего Laravel приложения. В зависимости от ваших предпочтений, вы можете организовать хранение ваших файлов с функциями там где вы хотите, вот несколько предложеных мест:
Я предпочитаю хранить их так app/helpers.php в корне пространства имен приложения.
Для использования ваших функций, вам нужно загрузить их в рантайм (жизненный цикл приложения). В начале моей карьеры я часто видел этот код в начале файла:
PHP функции не могут автоматически подгружаться. Однако, у нас есть более лучшее решение с использованием Composer нежели использование require или require_once.
Если вы создадите новый Laravel проект вы увидите параметры autoload и autoload-dev в файле composer.json:
Если мы хотим добавить свой файл с функциями, то в Composer для этого есть параметр files (который состоит из массива путей к файлам) который вы можете определить внутри параметра autoload:
Когда вы добавили новый путь в параметр files, вам нужно обновить авто-загрузчик выполнив:
Теперь при каждом запросе файл helpers.php будет подгружаться автоматически так как Laravel загружает Composer’овский авто-загрузчик в public/index.php:
Определение функций задача не сложная, хотя есть несколько предостережений. Все функции в Laravel обернуты специальной проверкой которая исключает вероятность коллизий:
Хотя тут может быть подвох, потому что мы можем выполнить функцию в ситуации когда она уже была определена, до того как мы ее назначили.
Я предпочитаю использовать function_exists для проверки моих функций, но если вы назначаете функцию в контексте своего приложения, вы можете отказаться от function_exists для проверки.
Пропустив проверку вы увидите коллизию всегда когда ваша функция будет переопределять другую, это может быть полезно. 
На практике, коллизии происходят не так часто как можно подумать, но вы должны быть уверены в том что название вашей функции не слишком общее. В дополнение вы можете добавить префикс в название вашей функции что понизит шанс коллизий.
Мне всегда нравилось как в RoR (Ruby on Rails) сделаны функции для путей и ссылок если вы определили маршрут ресурса. К примеру, для ресурса photos будут добавлены функции new_photo_path, edit_photo_path, и т.д.
Когда я использую ресурсную маршрутизацию в Laravel, я добавляю несколько функций которые упрощают работу с марщрутами в шаблонах. В моей реализации я добавляю функции которым я передаю Eloquent модель и которые возвращают маршрут на ресурс, например:
Здесь показано как вы можете определить функцию show_route в вашем файле app/helpers.php (другие будут похоже):
Функция plural_from_model() это всего лишь код который помогает получить имя ресурса основываясь на соглашениях именования.
Например, тут мы получаем имя ресурса на основе модели:
Используя эти соглашения вы можете определять маршруты для ресурсов в файле routes/web.php:
После этого в вашем шаблоне вы можете использовать функции так:
И на выходе вы получите такой HTML код:
Ваши Composer пакеты могут также использовать ваш файл с функциями, для любых функций которые вы хотите сделать доступными, в проекте используемый ваш пакет.
Вы будете использовать тотже подход для файла composer.json, определяя параметр files как массив ваших файлов с функциями.
Обязательно добавьте function_exists() в проверку вашей функции для того чтобы проект использующий ваш код не поломался из-за колизий имен.
Вы должны использовать правильные имена для ваших функций, которые будут уникальными, так же стоит подумать о использовании короткого префикса, если вы боитесь что название ваших функций слишком общее.
Источник: https://laravel-news.com/creating-helpers
Пользователь
=====
ReactOS 0.4.7: Павел Дуров больше не Пюыщн
2017-12-10, 19:25
This isn't a Windows screenshot, by the way... @ReactOS 0.4.7 is looking pretty good! pic.twitter.com/gCuXcFAT1X
it-евангелист
=====
Пять идей «на вооружение», или Впечатления от московского «Гейзенбага»
2017-12-10, 22:49
Программист
=====
Вычисляем точный адрес любого пользователя по номеру телефона или адресу электронной почты
2017-12-11, 01:31
User
=====
Разработка инди-игры одним человеком (история, советы)
2017-12-10, 21:35
Программист
=====
Необязательные аргументы в функциях Go
2017-12-10, 22:04
Технический менеджер
=====
Новая уязвимость в Android позволяет злоумышленникам изменять приложения, не затрагивая их подписи
2017-12-10, 22:34
User
=====
Почему дизайн Go плох для умных программистов
2017-12-10, 23:02
На протяжении последних месяцев я использую Go для имплементаций Proof of Concept (прим.пер.: код для проверки работоспособности идеи) в свободное время, отчасти для изучения самого языка программирования. Программы сами по себе очень просты и не являются целью написания статьи, но сам опыт использования Go заслуживает того, чтобы сказать о нем пару слов. Go обещает быть (прим.пер.: статья написана в 2015) массовым языком для серьезного масштабируемого кода. Язык создан в Google, в котором активно им пользуются. Подведя черту, я искренне считаю, что дизайн языка Go плох для умных программистов.
Go очень просто научиться, настолько просто, что введение заняло у меня один вечер, после чего уже мог продуктивно писать код. Книга по которой я изучал Go называется An Introduction to Programming in Go (перевод), она доступна в сети. Книгу, как и сам исходный код на Go, легко читать, в ней есть хорошие примеры кода, она содержит порядка 150 страниц, которые можно прочесть за раз. Сначала эта простота действует освежающе, особенно в мире программирования, полного переусложненных технологий. Но в итоге рано или поздно возникает мысль: "Так ли это на самом деле?"
Google утверждает, что простота Go — это подкупающая черта, и язык предназначен для максимальной продуктивности в больших командах, но я сомневаюсь в этом. Есть фичи, которых либо недостает, либо они чрезмерно подробны. А все из-за отсутствия доверия к разработчикам, с предположением, что они не в состоянии сделать что-либо правильно. Это стремление к простоте было сознательным решением разработчиков языка и, для того, чтобы полностью понять для чего это было нужно, мы должны понять мотивацию разработчиков и чего они добивались в Go.
Так для чего же он был создан таким простым? Вот пара цитат Роба Пайка (прим.пер.: один из соавторов языка Go):
Что? Так Роб Пайк в сущности говорит, что разработчики в Google не столь хороши, потому они и создали язык для идиотов (прим.пер.: dumbed down), так чтобы они были в состоянии что-то сделать. Что за высокомерный взгляд на собственных коллег? Я всегда считал, что разработчики Google отобраны из самых ярких и лучших на Земле. Конечно они могут справиться с чем-то посложнее?
Быть простым — это достойное стремление в любом дизайне, а попытаться сделать нечто простым трудно. Однако при попытке решить (или даже выразить) сложные задачи, порой необходим сложный инструмент. Сложность и запутанность не лучшие черты языка программирования, но существует золотая середина, при которой в языке возможно создание элегантных абстракций, простых в понимании и использовании.
Из-за стремления к простоте в Go отсутствуют конструкции, которые в остальных языках воспринимаются как что-то естественное. Вначале это может показаться хорошей идеей, но на практике выходит многословный код. Причина этому должна быть очевидна — необходимо, чтобы разработчикам было просто читать чужой код, но на самом деле эти упрощения только вредят читаемости. Сокращения в Go отсутствует: либо много, либо ничего.
К примеру, консольная утилита, которая читает stdin либо файл из аргументов командной строки, будет выглядеть следующим образом:
Хотя и этот код пытается быть как можно более общим, принудительная многословность Go мешает, и в результате решение простой задачи выливается в большой объем кода.
Вот, к примеру, решение той же задачи на D:
И кто теперь более читабельный? Я отдам свой голос D. Его код куда более читаемый, так как он более явно описывает действия. В D используются концепции куда сложнее (прим.пер.: альтернативный вызов функций и шаблоны), чем в примере с Go, но на самом деле нет ничего сложного в том, чтобы разобраться в них.
Популярное предложение для улучшения Go — это обобщенность. Это хотя бы поможет избежать ненужного копирования кода для поддержки всех типов данных. К примеру, функцию для суммирования списка целых чисел можно реализовать никак иначе, кроме как ее копипастой ее базовой функции для каждого целого типа, другого способа нет:
И этот пример даже не работает для знаковых типов. Такой подход полностью нарушает принцип не повторять себя (DRY), один из наиболее известных и очевидных принципов, игнорирование которого является источником многих ошибок. Зачем Go это делает? Это ужасный аспект языка.
Тот же пример на D:
Простое, элегантное и прямо в точку. Здесь используется функция reduce для шаблонного типа и предиката. Да, это опять же сложнее варианта с Go, но не столь уж сложно для понимания умными программистами. Который из примеров проще поддерживать и легче читать?
Я полагаю, читая это, программисты Go будут с пеной во рту кричать: "Ты делаешь это не так!". Что же, есть еще один способ сделать обобщенную функцию и типы, но это полностью разрушает систему типов!
Взгляните на этот пример глупого исправления языка для обхода проблемы:
Эта имплементация Reduce была позаимствована из статьи Idiomatic generics in Go (прим.пер.: перевод не нашел, буду рад, если поможете с этим). Что же, если это идиоматично, я бы не хотел увидеть не идиоматичный пример. Использование interface{} — фарс, и в языке он нужен лишь для обхода типизации. Это пустой интерфейс и все типы его реализуют, позволяя полную свободу для всех. Этот стиль программирования до ужаса безобразен, и это еще не все. Для подобных акробатических трюков требуется использовать рефлексию времени выполнения. Даже Робу Пайку не нравятся индивиды, злоупотребляющие этим, о чем он упоминал в одном из своих докладов.
Я бы взял шаблоны D вместо этой чепухи. Как кто-то может сказать, что interface{} более читаем или даже типобезопасен?
У Go есть встроенная система зависимостей, построенная поверх популярных хостингов VCS. Поставляемые с Go инструменты знают об этим сервисах и могут скачивать, собирать и устанавливать из них код одним махом. Хотя это и здорово, есть крупная оплошность с версионированием! Да действительно, можно получить исходный код из сервисов вроде github или bitbucket с помощью инструментов Go, но нельзя указать версию. И снова простота в ущерб полезности. Я не в состоянии понять логику подобного решения.
После вопросов о решении этой проблемы, команда разработки Go создала ветку форума, в которой изложили, как они собираются обойти этот вопрос. Их рекомендация была просто однажды скопировать весь репозиторий себе в проект и оставить "как есть". Какого черта они думают? У нас есть потрясающие системы контроля версий с отличным теггированием и поддержкой версий, которые создатели Go игнорируют и просто копируют исходные тексты.
По-моему мнению, Go был разработан людьми, которые использовали Си всю свою жизнь и теми, кто не хотел попытаться использовать что-то новое. Язык можно описать как Си с дополнительными колесиками(ориг.: training wheels). В нем нет новых идей, кроме поддержки параллелизма (который, кстати, прекрасен) и это обидно. У вас есть отличная параллельность в едва ли годном к употреблению, хромающем языке.
Еще одна скрипучая проблема в том, что Go — это процедурный язык (подобно тихому ужасу Си). В итоге начинаешь писать код в процедурном стиле, который ощущается архаичным и устаревшим. Я знаю, что объектно-ориентированное программирование — это не серебряная пуля, но было бы здорово иметь возможность абстрагировать детали в типы и обеспечить инкапсуляцию.
Go был разработан, чтобы быть простым и он преуспел в этой цели. Он был написан для слабых программистов, используя в качестве заготовки старый язык. Поставляется он в комплекте с простыми инструментами для выполнения простых вещей. Его просто читать и просто использовать.
Он крайне многословный, невыразительный и плох для умных программистов.
Спасибо mersinvald за правки
/dev/usr
=====
Digital-мероприятия в Москве c 11 по 17 декабря
2017-12-10, 23:48
Подборка Telegram-канала @mоs_events

DMC Light 11.12.2017
III Digital Marketing Week
«Взломать рост» — лекция vc.ru
Internet of Things From Zero to Hero
Вся правда о продаваемом бизнесе
Как смотреть кино: интенсив
MegaFon Big Data Challenge
Интернет вещей в России
User
=====
PHP-Дайджест № 121 (20 ноября – 10 декабря 2017)
2017-12-11, 01:04
Свежая подборка со ссылками на новости и материалы. В выпуске: PHP 7.2.0, Symfony 4 и другие релизы, предложение из PHP Internals, материалы по фреймворкам, асинхронный PHP, порция полезных инструментов, и многое другое. Приятного чтения!
https://t.co/rORz8xdCQp is a single PHP file called "index.php" generating $2,342.04 in a day. No frameworks. No libraries. 
 Спасибо за внимание!

Если вы заметили ошибку или неточность — сообщите, пожалуйста, в личку.
Вопросы и предложения пишите на почту или в твиттер.

Прислать ссылку
 Поиск ссылок по всем дайджестам

← Предыдущий выпуск: PHP-Дайджест № 120

PHP
=====
Дайджест свежих материалов из мира фронтенда за последнюю неделю №292 (4 — 10 декабря 2017)
2017-12-11, 01:11

Просим прощения за возможные опечатки или неработающие/дублирующиеся ссылки. Если вы заметили проблему — напишите пожалуйста в личку, мы стараемся оперативно их исправлять. 
User
=====
30-часовой хакатон Яндекс.Погоды, или как предсказать осадки по сигналам от пользователей
2017-12-13, 16:21
Интернет щей
=====
React, Drag&Drop и performance
2017-12-11, 06:28
fullstack разработчик
=====
Инструкции и полезная документация Check Point
2017-12-11, 07:48
Network Security
=====
Обзор напольного сервера Fujitsu PRIMERGY TX1330 M3
2017-12-11, 15:56
Пользователь
=====
Кейс. SMS, работающие на лояльность клиентов
2017-12-11, 10:09
Пользователь
=====
Найдена уязвимость во всех версиях Windows, которую не закрывает ни один антивирус
2017-12-11, 10:12
User
=====
Самая быстрая и энергоэффективная реализация алгоритма BFS на различных параллельных архитектурах
2017-12-11, 12:19
В названии статьи не поместилось — данные результаты считаются таковыми по версии рейтинга Graph500. Также хотелось бы выразить благодарность компаниям IBM и RSC за предоставленные ресурсы для проведения экспериментальных запусков во время исследования.

Поиск в ширину (BFS) является одним из основных алгоритмов обхода графа и базовым для многих алгоритмов анализа графов более высокого уровня. Поиск в ширину на графах является задачей с нерегулярным доступом к памяти и с нерегулярной зависимостью по данным, что сильно усложняет его распараллеливание на все существующие архитектуры. В статье будет рассмотрена реализация алгоритма поиска в ширину (основного теста рейтинга Graph500) для обработки больших графов на различных архитектурах: Intel х86, IBM Power8+, Intel KNL и NVidia GPU. Будут описаны особенности реализации алгоритма на общей памяти, а также преобразования графа, которые позволяют достичь рекордных показателей производительности и энергоэффективности на данном алгоритме среди всех одноузловых систем рейтинга Graph500 и GreenGraph500.
В последнее время все большую роль играют графические ускорители (ГПУ) в неграфических вычислениях. Потребность их использования обусловлена их относительно высокой производительностью и более низкой стоимостью. Как известно, на ГПУ и центральных процессорах (ЦПУ) хорошо решаются задачи на структурных, регулярных сетках, где параллелизм так или иначе легко выделяется. Но есть задачи, которые требуют больших мощностей и используют неструктурированные сетки или данные. Примером таких задач являются: Single Shortest Source Path problem (SSSP) — задача поиска кратчайших путей от заданной вершины до всех остальных во взвешенном графе, задача Breadth First Search (BFS [1]) — задача поиска в ширину в неориентированном графе, Minimum Spanning Tree (MST, например, зарубежная и моя реализации) — задача поиска сильно связанных компонент и другие. 
Данные задачи являются базовыми в ряде алгоритмов на графах. На данный момент алгоритмы BFS и SSSP используются для ранжирования вычислительных машин в рейтингах Graph500 и GreenGraph500. Алгоритм BFS (breadth-first search или поиск в ширину) является одним из наиболее важных алгоритмов анализа на графах. Он используется для получения некоторых свойств связей между узлами в заданном графе. В основном BFS используется как звено, например, в таких алгоритмах, как нахождение связных компонент [2], нахождение максимального потока [3], нахождение центральных компонент (betweenness сentrality) [4, 5], кластеризация [6], и многие другие.
Алгоритм BFS имеет линейную вычислительную сложность O(n + m), где n — количество вершин и m — количество ребер графа. Данная вычислительная сложность является наиболее оптимальной для последовательной реализации. Но данная оценка вычислительной сложности не применима для параллельной реализации, так как последовательная реализация (например, с помощью алгоритма Дейкстры) имеет зависимости по данным, что препятствует ее распараллеливание. Также производительность данного алгоритма ограничена производительностью памяти той или иной архитектуры. Поэтому наибольшее значение имеют оптимизации, направленные на улучшение работы с памятью всех уровней. 
Рейтинг Graph500 был создан как альтернатива рейтингу Top500. Данный рейтинг используется для ранжирования вычислительных машин в приложениях, которые используют нерегулярный доступ к памяти, в отличие от последнего. Для тестируемого приложения в рейтинге Graph500 пропускная способность памяти и коммуникационной сети играют наиболее важную роль. Рейтинг GreenGraph500 является альтернативой рейтинга Green500 и используется в дополнении к Graph500. 
В Graph500 используется метрика — количество обработанных ребер графа в секунду (TEPS — traversed edges per second), в то время как в GreenGraph500 используется метрика — количество обработанных ребер графа в секунду на один ватт. Таким образом, первый рейтинг ранжирует вычислительные машины по скорости вычисления, а второй — по энергоэффективности. Данные рейтинги обновляются каждые полгода. 
Алгоритм поиска в ширину был придуман более 50 лет назад. И до сих пор проводятся исследования для эффективной параллельной реализации на различных устройствах. Данный алгоритм показывает на сколько хорошо организована работа с памятью и коммуникационной средой вычислителей. Существует достаточно много работ по распараллеливанию данного алгоритма на x86 системах [7-11] и на ГПУ [12-13]. Также подробные результаты выполнения реализованных алгоритмов можно увидеть в рейтингах Green500 и GreenGraph500. К сожалению, алгоритмы многих эффективных реализаций не опубликованы в зарубежных источниках.
Если выбрать только одноузловые системы в рейтинге Graph500, то мы получим следующие некоторые данные, которые представлены в таблице ниже. Результаты, описанные в данной работе, помечены жирным шрифтом. В таблицу включались графы с количеством вершин более 225. Из полученных данных видно, что на текущий момент не существует более эффективной реализации с использованием только одного узла, чем предложенная в данной статье. Более подробный анализ представлен в разделе Анализа полученных результатов. 
Для оценки производительности алгоритма BFS используются неориентированные RMAT графы. RMAT графы хорошо моделируют реальные графы из социальных сетей, Интернета. В данном случае рассматриваются RMAT графы со средней степенью связности вершины 16, а количество вершин является степенью двойки. В таком RMAT графе имеется одна большая связная компонента и некоторое количество небольших связных компонент или висящих вершин. Сильная связность компонент не позволяет каким-либо образом разделить граф на такие подграфы, которые помещались бы в кэш память.
Для построения графа используется генератор, который предоставляется разработчиками рейтинга Graph500. Данный генератор создает неориентированный граф в формате RMAT, причем выходные данные представлены в виде набора ребер графа. Такой формат не очень удобен для эффективной параллельной реализации графовых алгоритмов, так как необходимо иметь агрегированную информацию по каждой вершине, а именно — какие вершины являются соседями для данной. Удобный для этого представления формат называется CSR (Compressed Sparse Rows).
Данный формат получил широкое распространение для хранения разреженных матриц и графов. Для неориентированного графа с N вершинами и M ребрами необходимо два массива: X (массив указателей на смежные вершины) и А (массив списка смежных вершин). Массив X размера N + 1, а массив А — 2 * M, так как в неориентированном графе для любой пары вершин необходимо хранить прямую и обратную дуги. В массиве X хранятся начало и конец списка соседей, находящиеся в массиве А, то есть весь список соседей вершины J находится в массиве A с индекса X[J] до X[J+1], не включая его. Для иллюстрации на рисунке ниже слева показан граф из 4 вершин, записанный с помощью матрицы смежности, а справа – в формате CSR.

После преобразования графа в CSR-формат, необходимо проделать еще некоторую работу над входным графом для улучшения эффективности работы кэша и памяти вычислительных устройств. После выполнения описанных ниже преобразований, граф остается все в том же CSR-формате, но приобретет некоторые свойства, связанные с выполненными преобразованиями. 
Введенные преобразования позволяют построить граф в оптимальном виде для большинства алгоритмов обхода графа в формате CSR. Добавление новой вершины в граф не приведет к выполнению всех преобразований заново, достаточно следовать введенным правилам и добавлять вершину так, чтобы общий порядок вершин не нарушался.
Для каждой вершины выполним сортировку по возрастанию ее списка соседей. В качестве ключа для сортировки будем использовать количество соседей для каждой сортируемой вершины. После выполнения данной сортировки, выполняя обход списка соседей у каждой вершины, мы будем обрабатывать сначала самые тяжелые вершины — вершины, имеющие большое количество соседей. Данную сортировку можно выполнять независимо для каждой вершины и параллельно. После выполнения данной сортировки, номера вершин графа в памяти не изменяется. 
Для списка всех вершин графа выполним сортировку по возрастанию. В качестве ключа будем использовать количество соседей для каждой из вершин. В отличие от локальной сортировки, данная сортировка требует перенумерации полученных вершин, так как меняется позиция вершины в списке. Процедура сортировки имеет сложность O(N*log(N)) и выполняется последовательно, а процедура перенумерации вершин может быть выполнена параллельно и по скорости работы сопоставима с временем копирования одного участка памяти в другой. 
Занумеруем вершины графа таким образом, чтобы наиболее связные вершины имели наиболее близкие номера. Данная процедура устроена следующим образом. Сначала берется первая вершина из списка для перенумерации. Она получает номер 0. Затем все соседние вершины с рассматриваемой вершиной добавляются в очередь для перенумерации. Следующая вершина из списка перенумерации получает номер 1 и так далее. В результате данной операции в каждой связной компоненте разница между максимальным и минимальным номером вершины будет наименьшей, что позволит лучшим образом использовать маленький объем кэша вычислительных устройств.
Алгоритм поиска в ширину в неориентированном графе устроен следующим образом. На входе подается начальная за ранее неопределенная вершина в графе (корневая вершина для поиска). Алгоритм должен определить, на каком уровне, начиная от корневой вершины, находится каждая из вершин в графе. Под уровнем понимается минимальное количество ребер, которое необходимо преодолеть, чтобы добраться из корневой вершины в отличную от корневой вершину. Также для каждой из вершин, кроме корневой, необходимо определить вершину родителя. Так как у одной вершины может быть несколько родительских вершин, то в качестве ответа принимается любая из них. 
Алгоритм поиска в ширину имеет несколько реализаций. Наиболее эффективная реализация — итерационный обход графа с синхронизацией по уровню. Каждый шаг представляет собой итерацию алгоритма, на которой информация с уровня J переносится на уровень J+1. Псевдокод последовательного алгоритма представлен по ссылке. 
Параллельная реализация базируется на гибридном алгоритме, состоящий из top-down (TD) и bottom-up (BU) процедур, который был предложен автором данной статьи [11]. Суть данного алгоритма заключается в следующем. Процедура TD позволяет обойти вершины графа в прямом порядке, то есть, перебирая вершины, мы рассматриваем связи  как родитель-потомок. Вторая процедура BU позволяет обойти вершины в обратном порядке, то есть, перебирая вершины, мы рассматриваем связи  как потомок-родитель. 
Рассмотрим последовательную реализацию гибридного алгоритма TD-BU, псевдокод которой показан ниже. Для обработки графа вершин нам необходимо создать дополнительные два массива-очереди, которые будут содержать в себе набор вершин на текущем уровне — Qcurr, и набор вершин на следующем уровне — Qnext. Чтобы выполнять более быстрые проверки существования вершины в очереди, необходимо ввести массив посещенных вершин. Но так как нам в результате работы алгоритма необходимо получить информацию о том, на каком уровне располагается каждая из вершин, этот массив может быть использован и в качестве индикатора посещенных и размеченных вершин. 
Последовательный гибридный алгоритм BFS:
Основной цикл работы алгоритма состоит из последовательной обработки каждой вершины, находящейся в очереди Qcurr. Если в очереди Qcurr больше не остается вершин, то алгоритм останавливается и ответ получен. 
В самом начале алгоритма начинает работать процедура TD, так как в очереди содержится всего одна вершина. В процедуре TD мы для каждой вершины Vi из очереди Qcurr просматриваем список соседних с этой вершиной Vk и добавляем в очередь Qnext тех из них, которые еще не были помечены как посещенные. Также все такие вершины Vk получают номер текущего уровня и родительскую вершину Vi. После завершения просмотра всех вершин из очереди Qcurr запускается процедура выбора следующего состояния, которая может либо остаться на процедуре TD для следующей итерации, либо сменить процедуру на BU. 
В процедуре BU мы просматриваем вершины не из очереди Qcurr, а те вершины, которые еще не были помечены. Данная информация содержится в массиве уровней Levels. Если такие вершины Vi еще не были размечены, то мы проходим по всем ее соседям Vk и если эти вершины, которые являются родителями для Vi, находятся на предыдущем уровне, то вершина Vi попадает в очередь Qnext. В отличие от процедуры TD, в данной процедуре можно прервать просмотр соседних вершин Vk, так как нам достаточно найти любую родительскую вершину.
Если выполнять поиск только процедурой TD, то на последних итерациях алгоритма список вершин, который необходимо обработать, будет очень большим, а неразмеченных вершин будет достаточно мало. Тем самым процедура будет выполнять лишние действия и лишние обращения в память. Если выполнять поиск только процедурой BU, то на первых итерациях алгоритма будет достаточно много неразмеченных вершин и аналогично процедуре TD, будут выполнены лишние действия и лишние обращения в память. 
Получается, что первая процедура эффективна на первых итерациях алгоритма BFS, а вторая — на последних. Ясно, что наибольший эффект будет достигнут, когда мы будем использовать обе процедуры. Для того, чтобы автоматически определять, когда необходимо выполнять переключение с одной процедуры на другую, воспользуемся алгоритмом (процедура change_state), который был предложен авторами данной статьи [11]. Данный алгоритм по информации о количестве обработанных вершин на двух соседних итерациях пытается понять характер поведения обхода. В алгоритме вводится два коэффициента, которые позволяют настраивать переключение с одной процедуры на другую в зависимости от обрабатываемого графа. 
Процедура смены состояния может переводить не только TD в BU, но и обратно BU в TD. Последняя смена состояния полезна в том случае, когда количество вершин, которые необходимо просмотреть, достаточно мало. Для этого вводится понятие нарастающего фронта и затухающего фронта размеченных и неразмеченных вершин. Следующий псевдокод, представленный ниже, выполняет смену состояния в зависимости от полученных характеристик на конкретной итерации обхода графа в зависимости от настроенных коэффициентов alpha и beta. Данная функция может быть настроена на любой входной граф в зависимости от factor (под factor понимается средняя связность вершины графа). 
Функция изменения состояния:
Описанные выше концепции гибридной реализации алгоритма BFS применялись для параллельной реализации как для ЦПУ подобных систем, так и для ГПУ. Но есть некоторые отличия, которые будут рассмотрены далее.
Параллельная реализация для ЦПУ систем (Power 8+, Intel KNL и Intel х86) была выполнена с использованием OpenMP. Для запуска использовался один и тот же код, но для каждой платформы выполнялись свои настройки для директив OpenMP, например, задавались разные режимы балансировки нагрузки между нитями (schedule). Для реализации на ЦПУ с использованием OpenMP можно выполнить еще одно преобразование графа, а именно — сжатие списка вершин соседей. 
Сжатие заключается в удалении незначащих нулевых битов каждого элемента из массива А, причем данное преобразование делается отдельно для каждого диапазона [Xi, Xi+1). Происходит уплотнение элементов массива А. Такое сжатие позволяет сократить количество используемой памяти для хранения графа примерно на 30% для больших графов порядка 230 вершин и 234 ребер. Для более маленьких графов экономия от такого преобразования пропорционально увеличивается в силу того, что уменьшается количество бит, которое занимает максимальный номер вершины в графе.
Такое преобразование графа накладывает некоторые ограничения на обработку вершин. Во-первых, все соседние вершины должны обрабатываться последовательно, так как они представляют собой сжатую, закодированную определенным образом последовательность элементов. Во-вторых, необходимо выполнять дополнительные действия по распаковке сжатых элементов. Данная процедура не является тривиальной и для ЦПУ Power8+ не позволила получить эффекта. Причиной может быть плохая оптимизация компилятора или отличная от Intel работа аппаратуры.
Для того, чтобы выполнять параллельно одну итерацию алгоритма, необходимо создать свои очереди Qth_next для каждого потока OpenMP. А после выполнения всех циклов, выполнить объединение полученных очередей. Также необходимо локализовать все переменные, по которым есть редукционная зависимость. В качестве оптимизации процедура TD выполняется в последовательном режиме, если в очереди Qcurr количество вершин меньше заданного порога (например, меньше 300). Для графа разного размера, а также в зависимости от архитектуры, данный порог может иметь разные значения. Параллельные директивы располагались перед циклами (foreach Vi in Qcurr) в случае процедуры TD, и (foreach Vi in G) в случае процедуры BU.
Параллельная реализация для ГПУ была выполнена с использованием технологии CUDA. Реализация процедуры TD и BU существенно отличаются в случае использования ГПУ, так как существенно отличается алгоритм доступа к данным во время выполнения той или иной процедуры. 
Процедура TD была реализована с помощью динамического параллелизма CUDA. Данная возможность позволяет переложить некоторую работу, связанную с балансировкой нагрузки, на аппаратуру ГПУ. Каждая вершина Vi из очереди Qcurr может содержать абсолютно разное за ранее не известное количество соседей. Из-за этого при прямом отображении всего цикла на набор нитей, возникает сильный дисбаланс нагрузки, так как CUDA позволяет использовать блок нитей фиксированного размера. 
Описанная проблема решается путем использования динамического параллелизма. В начале запускаются мастер-нити. Данных нитей будет столько, сколько вершин содержится в очереди Qcurr. Затем каждая мастер-нить запускать столько дополнительных нитей, сколько соседей имеется у вершины Vi. Таким образом, количество используемых нитей формируется в динамике во время выполнения программы в зависимости от входных данных. 
Данная процедура не удобна для выполнения на графическом процессоре из-за необходимости использовать динамический параллелизм. При большом суммарном количестве нитей, которые необходимо создать из мастер-нитей, возникают большие накладные расходы. Поэтому переключение на процедуру BU выполняется раньше, чем на ЦПУ. 
Процедура BU является более благоприятной для выполнения на ГПУ, если проделать некоторые дополнительные преобразования данных. Данная процедура существенно отличается от процедуры TD тем, что проход выполняется над всеми подряд идущими вершинами графа. Таким образом организованный цикл позволяет выполнить некоторую подготовку данных для хорошего доступа к памяти. 
Преобразование заключается в следующем. Известно, что соседние нити одного варпа выполняют инструкции синхронно и параллельно. Также для эффективного доступа к памяти требуется, чтобы соседние нити в варпе обращались к соседним ячейкам в памяти. Для примера положим количество нитей в варпе равным 2. Если каждой нити сопоставить одну вершину цикла (foreach Vi in G), то во время обработки соседей в цикле (foreach Vk in G.Edges(Vi)) каждая нить будет обращаться в свою область памяти, что негативно скажется на производительности, так как соседние нити будут обрабатывать далеко расположенные ячейки в памяти. Для того, чтобы исправить положение, перемешаем элементы массива А таким образом, чтобы доступ к первым двум соседям из V0 и V1 осуществлялся наилучшим образом — соседние элементы располагались в соседних ячейках в памяти. Далее, в памяти таким же образом будут лежать вторые, третьи и т.д. элементы. 
Данное правило выравнивания применяется для группы нитей варпа (32 нити): выполняется перемешивание соседей — сначала располагаются первые 32 элемента, затем вторые 32 элемента и т.д. Так как граф отсортирован по убыванию количества соседей, то группы вершин, которые располагаются рядом, будут иметь достаточно близкое количество вершин соседей. 
Перемешивать таким способом элементы можно не во всем графе, так как во время работы процедуры BU во внутреннем цикле есть досрочный выход. Описанные выше глобальная и локальная сортировки вершин позволяют выходить из данного цикла достаточно рано. Поэтому перемешивается только 40% всех вершин графа. Данное преобразование требует дополнительной памяти для хранения перемешанного графа, зато мы получаем заметный прирост в производительности. Ниже представлен псевдокод ядра для процедуры BU с использованием перемешанного расположения элементов в памяти.
Параллельное ядро для процедуры BU:
Тестированные программы производилось сразу на четырех различных платформах: Intel Xeon Phi (Xeon KNL 7250), Intel x86 (Xeon E5 2699 v3), IBM Power8+ (Power 8+ s822lc) и GPU NVidia Tesla P100. Интересующие для сравнения характеристики данных устройств представлены в
таблице ниже.
Использовались следующие компиляторы. Для Intel Xeon E5 — gcc 5.3, для Intel KNL — icc v17, для IBM — gcc для Power архитектуры, для NVidia — CUDA 9.0.
В последнее время производители все больше задумываются о пропускной способности памяти. Как следствие этому, появляются различные решение проблемы медленного доступа к оперативной памяти. Среди рассматриваемых платформ, две имеют двухуровневую структуру оперативной памяти.
Первая из них — Intel KNL, содержит быструю память на кристалле, скорость доступа к которой порядка 400 ГБ/с, и более медленную привычную нам DDR4, скорость доступа к которой не более 115 ГБ/с. Быстрая память имеет достаточно маленький размер — всего 16ГБ, в то время как обычная память может быть до 384 ГБ. На тестируемом сервере было установлено 96 ГБ памяти. Вторая платформа с гибридным решением — Power + NVidia Tesla. Данное решение базируется на новой технологии NVlink, которая позволяет иметь доступ к обычной памяти на скорости 40 ГБ/с, в то время как доступ к быстрой памяти осуществляется на скорости 700 ГБ/с. Количество быстрой памяти такое же, как и в Intel KNL — 16ГБ.
Данные решения схожи с точки зрения организации — имеется быстрая память маленького размера, и медленная память большого размера. Сценарий использования двухуровневой памяти при обработки больших графов очевиден: быстрая память используется для хранения результата и промежуточных массивов, размеры которых достаточно малы по сравнению с входными данным, а исходный граф читается из медленной памяти. 
С точки зрения реализации пользователю доступны следующие средства. Для Intel KNL достаточно использовать другие функции выделения памяти — hbm_malloc, вместо привычного malloc. Если программа использовала операторы malloc, то достаточно объявить один  define для того, чтобы использовать данную возможность. Для NVidia Tesla необходимо использовать также другие функции выделения памяти — вместо cudaMalloc использовать cudaMallocHost. Данные модификации кода являются достаточными и не требуют каких-либо модификаций в вычислительной части программы. 
Эксперименты проводились для графов разного размера, начиная от 225 (4 ГБ) и заканчивая 230 (128 ГБ). Средняя степень связности и тип графа брались из генератора графа для рейтинга Graph500. Данный генератор создает графы Кронекера со средней степенью связности 16 и коэффициентами A = 0.57, B = 0.19, C = 0.19.
Данного вида графы используются всеми участниками таблицы рейтинга, что позволяет корректно сравнивать реализации между собой. Значение производительности считается по метрике TEPS для таблицы Graph500 и TEPS / w для таблицы GreenGraph500. Для вычисления данной характеристики выполняется 64 запуска алгоритма BFS из разных стартовых вершин и берется среднее значение. Для вычисления потребления алгоритма берется текущее потребление системы в момент работы алгоритма с учетом потребления оперативной памяти. 
Следующая таблица иллюстрирует полученную производительность в GTEPS на всех тестируемых графах. В таблице указываются два значения — минимальная / максимальная достигнутая производительность на каждом из графов. Также в случае использования Intel KNL были получены результаты выполнения алгоритма при использовании только памяти DDR4. К сожалению, даже при использовании всех алгоритмов сжатия данных, не удалось запустить на предоставленном сервере граф с 230 вершинами на Intel KNL. Но учитывая стабильность работы Intel процессоров и технологичность Intel компиляторов, можно предположить, что производительность не изменится при увеличении размера графа (как это можно видеть для Intel Xeon E5).
Полученная производительность в GTEPS:
На графике ниже отображена средняя производительность протестированных платформ. Можно заметить, что Power 8+ показал не очень хорошую стабильность при переходе с графа размером 64 ГБ на 128 ГБ. Возможно, это связано с тем, что использовался двух сокетный узел из двух аналогичных процессоров, причем у каждого процессора было по 128 ГБ памяти. И при обработке большего графа часть данных размещалось в памяти, не принадлежащей сокету. На графике также не отображена производительность Tesla P100 на более маленьких графах, так как разница между самым быстрым ЦПУ устройством и ГПУ составляет примерно 10 раз. Данное ускорение резко сокращается, когда графы становятся настолько большими, что не помещаются в кэш и доступ к графу осуществляется через NVlink. Но, несмотря на данное ограничение, производительность ГПУ все равно остается больше всех ЦПУ устройств. Такое поведение объясняется тем, что CUDA позволяет лучше контролировать вычисления и доступ к памяти, а также лучшей приспособленностью графических процессоров к параллельным вычислениям. 

Таблица ниже иллюстрирует полученную производительность в GTEPS /w на всех тестируемых графах. В таблице указываются среднее энергопотребление при средней достигнутой производительности на каждом из графов. Резкое падение производительности и энергоэффективности при переходе с графа 228 на граф 229 на NVidia Tesla P100 объясняется тем, что быстрой памяти не хватает, чтобы поместить выровненную часть графа, к которой осуществляется наиболее частый доступ. В случае использования большего количества памяти (например, 32 ГБ) и увеличенного канала связи с ЦПУ NVlink 2.0 можно существенно повысить эффективность обработки графов большого размера.
Полученная энергоэффективность в MTEPS / w:
В результате проделанной работы были реализованы два параллельных алгоритма BFS для ЦПУ подобных систем и для ГПУ. Было выполнено исследование производительности и энергоэффективности реализованных алгоритмов на различных платформах, таких как IBM Power8+, Intel x86, Intel Xeon Phi (KNL) и NVidia Tesla P100. Данные платформы имеют различные архитектурные особенности. Несмотря на это, первые три из них очень похожи по строению. Благодаря этому на этих платформах можно запускать OpenMP приложения без каких-либо существенных изменений. Наоборот, архитектура ГПУ сильно отличается от ЦПУ подобных платформ и использует другую концепцию для реализации вычислительного кода — архитектуру CUDA. 
Были рассмотрены графы, которые получаются после генератора для рейтинга Graph500. Была исследована производительность каждой из архитектур на двух классах данных. К первому классу относятся графы, которые помещаются в наиболее быструю память вычислителя. Ко второму классу относятся большие графы, которые нельзя поместить в быструю память целиком. Для демонстрации энергоэффективности использовались метрики GreenGraph500. Минимальный граф, который учитывается в рейтинге GreenGraph500 в классе больших данных, содержит в себе 230 вершин и 234 ребер и занимает в исходном виде 128 ГБ памяти. В классе же малых данных допускается граф любого размера до 230 вершин и 234 ребер, причем в качестве результата принимается наиболее большой граф, который удалось вместить в память.
На данный момент среди всех одноузловых систем в рейтинге Graph500 и GreenGraph500 полученная реализация на NVidia Tesla P100 занимает лидирующие позиции как в классе малых данных (с производительностью 220 GTEPS и эффективностью 1235.96 MTEPS/w), так и в классе больших данных (с производительностью 41.7 GTEPS и эффективностью 177.45 MTEPS/w). Такая высокая энергоэффективность и скорость работы на больших графах была достигнута благодаря новой технологии NVLink, которая связывает ГПУ и ЦПУ между собой и доступна на данный момент только в серверах компании IBM с ЦПУ Power8+. 
В дальнейшем планируется исследовать возможность выполнения данного алгоритма на новой архитектуре NVidia Volta с использованием улучшенной технологии NVlink 2.0, а также планируется исследовать параллельную реализацию на нескольких ГПУ.
[1] E.F. Moore. The shortest path through a maze. In Int. Symp. on Th.
of Switching, pp. 285–292, 1959
[2] Cormen, T., Leiserson, C., Rivest, R.: Introduction to Algorithms. MIT Press,
Cambridge (1990)
[3] Edmonds, J., Karp, R.M.: Theoretical improvements in algorithmic efficiency for
network flow problems. Journal of the ACM 19(2), 248–264 (1972)
[4] Brandes, U.: A Faster Algorithm for Betweenness Centrality. J. Math. Sociol. 25(2),
163–177 (2001)
[5] Frasca, M., Madduri, K., Raghavan, P.: NUMA-Aware Graph Mining Techniques
for Performance and Energy Efficiency. In: Proc. ACM/IEEE Int. Conf. High Performance
Computing, Networking, Storage and Analysis (SC 2012), pp. 1–11. IEEE
Computer Society (2012)
[6] Girvan, M., Newman, M.E.J.: Community structure in social and biological networks.
Proc. Natl. Acad. Sci. USA 99, 7821–7826 (2002)
[7] D. A. Bader and K. Madduri. Designing multithreaded algorithms for
breadth-first search and st-connectivity on the Cray MTA-2. In ICPP,
pp. 523–530, 2006.
[8] R.E. Korf and P. Schultze. Large-scale parallel breadth-first search.
In AAAI, pp. 1380–1385, 2005.
[9] A. Yoo, E. Chow, K. Henderson, W. McLendon, B. Hendrickson, and
U. Catalyurek. A scalable distributed parallel breadth-first search
algorithm on BlueGene/L. In SC ’05, p. 25, 2005.
[10] Y. Zhang and E.A. Hansen. Parallel breadth-first heuristic search on a shared-memory architecture. In AAAI Workshop on Heuristic
Search, Memory-Based Heuristics and Their Applications, 2006.
[11] Yasui Y., Fujisawa K., Sato Y. (2014) Fast and Energy-efficient Breadth-First Search on a Single NUMA System. In: Kunkel J.M., Ludwig T., Meuer H.W. (eds) Supercomputing. ISC 2014. Lecture Notes in Computer Science, vol 8488. Springer, Cham
[12] Hiragushi T., Takahashi D. (2013) Efficient Hybrid Breadth-First Search on GPUs. In: Aversa R., Kołodziej J., Zhang J., Amato F., Fortino G. (eds) Algorithms and Architectures for Parallel Processing. ICA3PP 2013. Lecture Notes in Computer Science, vol 8286. Springer, Cham
[13] Merrill, D., Garland, M., Grimshaw, A.: Scalable GPU graph traversal. In: Proc. 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 2012), pp. 117–128 (2012)
Программист высокопроизводительных вычислений
=====
Внедрение зависимостей в .Net Марка Симана 2 — Внедрение конструктора, время жизни
2017-12-11, 11:19
Инженер
=====
Как мы создавали менеджер паролей со стойкой криптографией и мастер-паролем. Опыт команды Яндекс.Браузера
2017-12-12, 12:33
Инженер, коммуникатор и просто гик
=====
Аутентификация и авторизация в Ember, часть 1: библиотека ember simple auth
2017-12-11, 12:53
Пользователь
=====
Формула-1 для дронов: команда Университета ИТМО заняла первое место на Robotex-2017
2017-12-11, 13:31
User
=====
ASO Monthly #19 Ноябрь 2017
2017-12-11, 13:31
Маркетинг, ASO
=====
Как проверить автомобиль перед покупкой: используем доступные в Интернете базы данных и логику
2017-12-12, 05:30
Корпоративный облачный провайдер
=====
Конвейер игр для магазинов от второкурсников с любовью
2017-12-11, 13:45
User
=====
Туториал по Unreal Engine. Часть 1: знакомство с движком
2017-12-12, 12:07
Переводчик-фрилансер
=====
Airflow Workshop: сложные DAG’и без костылей
2017-12-11, 17:52

Привет, Хабр! Меня зовут Дина, и я занимаюсь разработкой игрового хранилища данных для решения задач аналитики в Mail.Ru Group. Наша команда для разработки batch-процессов обработки данных использует Apache Airflow (далее Airflow), об этом yuryemeliyanov писал в недавней статье. Airflow — это opensource-библиотека для разработки ETL/ELT-процессов. Отдельные задачи объединяются в периодически выполняемые цепочки задач — даги (DAG — Directed Acyclic Graph).
Как правило, 80 % проекта на Airflow — это стандартные DAG’и. В моей статье речь пойдёт об оставшихся 20 %, которые требуют сложных ветвлений, коммуникации между задачами — словом, о DAG’ах, нуждающихся в нетривиальных алгоритмах.
Представьте, что перед нами стоит задача ежедневно забирать данные с нескольких шардов. Мы параллельно записываем их в стейджинговую область, а потом строим на них целевую таблицу в хранилище. Если в процессе работы по какой-то причине произошла ошибка — например, часть шардов оказалась недоступна, — DAG будет выглядеть так:

Для того чтобы перейти к выполнению следующей задачи, нужно обработать ошибки в предшествующих. За это отвечает один из параметров оператора — trigger_rule. Его значение по умолчанию — all_success — говорит о том, что задача запустится тогда и только тогда, когда успешно завершены все предыдущие.
Также trigger_rule может принимать следующие значения:
Для реализации логики if-then-else можно использовать оператор ветвления BranchPythonOperator. Вызываемая функция должна реализовывать алгоритм выбора задачи, который запустится следующим. Можно ничего не возвращать, тогда все последующие задачи будут помечены как не нуждающиеся в исполнении.
В нашем примере выяснилось, что недоступность шардов связана с периодическим отключением игровых серверов, соответственно, при их отключении никаких данных за нужный нам период мы не теряем. Правда, и витрины нужно строить с учётом количества включённых серверов.
Вот как выглядит этот же DAG со связкой из двух задач с параметром trigger_rule, принимающим значения one_success (хотя бы одна из предыдущих задач успешна) и all_done (все предыдущие задачи завершились), и оператором ветвления select_next_task вместо единого PythonOperator’а.

Документация по параметру оператора trigger_rule
Документация по оператору BranchPythonOperator
Операторы Airflow также поддерживают рендеринг передаваемых параметров с помощью Jinja. Это мощный шаблонизатор, подробно о нём можно почитать в документации, я же расскажу только о тех его аспектах, которые мы применяем в работе с Airflow.
Шаблонизатор обрабатывает:
Вот как мы применяем Jinja в Airflow:
Приведу несколько примеров рендеринга параметров в интерфейсе Airflow. В первом мы удаляем записи старше количества дней, передаваемого параметром cut_days. Так выглядит sql c использованием шаблонов jinja в Airflow:

В обработанном sql вместо выражения уже подставляется конкретная дата:

Второй пример посложнее. В нём используется преобразование даты в unixtime для упрощения фильтрации данных на источнике. Конструкция "{:.0f}" используется, чтобы избавиться от вывода знаков после запятой:

Jinja заменяет выражения между двойными фигурными скобками на unixtime, соответствующий дате исполнения DAG’а и следующей за ней дате:
 
Ну и в последнем примере мы используем функцию truncshift, переданную в виде параметра:

Вместо этого выражения шаблонизатор подставляет результат работы функции:

Документация по шаблонизатору jinja
В одном из наших источников интересная система хранения логов. Каждые пять дней источник создаёт новую таблицу такого вида: squads_02122017. В её названии присутствует дата, поэтому возник вопрос, как именно её высчитывать. Какое-то время мы использовали таблицы с названиями из всех пяти дней. Четыре запроса падали, но trigger_rule=’one_success’ спасал нас (как раз тот случай, когда выполнение всех пяти задач необязательно).
Спустя какое-то время мы стали использовать вместо trigger_rule встроенную в Airflow технологию для обмена сообщениями между задачами в одном DAG’е — XCom (сокращение от cross-communication). XCom’ы определяются парой ключ-значение и названием задачи, из которой его отправили.

XCom создаётся в PythonOperator’е на основании возвращаемого им значения. Можно создать XCom вручную с помощью функции xcom_push. После выполнения задачи значение сохраняется в контексте, и любая последующая задача может принять XCom функцией xcom_pull в другом PythonOperator’е или из шаблона jinja внутри любой предобработанной строки.
Вот как выглядит получение названия таблицы сейчас:
Ещё один пример использования технологии XCom — рассылка email-уведомлений с текстом, отправленным из PythonOperator’а:
А вот получение текста письма внутри оператора EmailOperator:
Документация по технологии XCom
Я рассказала о способах ветвления, коммуникации между задачами и шаблонах подстановки. С помощью встроенных механизмов Airflow можно решать самые разные задачи, не отходя от общей концепции реализации DAG’ов. На этом интересные нюансы Airflow не заканчиваются. У нас с коллегами есть идеи для следующих статей на эту тему. Если вас заинтересовал этот инструмент, пишите, о чём именно вам хотелось бы прочитать в следующий раз.
Пользователь
=====
Гаджет моего тела (Часть #2)
2017-12-12, 10:38
Источник
«…в результате имплантации была задета не только кора головного мозга, но и, так сказать, сама древесина»
(из протокола осмотра)
Гаджеты, они сегодня как вирусы. Они незаметны, они вездесущи, они легко проникают в организм и скоро cмогут делать там все, что захотят. Вернее все, что будет заложено в их ПО или что захочет тот, у кого в руках пульт управления...
Продолжим сегодня рассуждать о том, что же с развитием Интернета вещей (IoT) нам предлагает рынок современных гаджетов. Для тех, кто пропустил — с первой частью можно ознакомиться здесь.
Пожалуй, самое широко известное и распространенное применение имплантированной носимой электроники — электрокардиостимуляторы. С момента разработки первого образца в 1961 году десятки миллионов людей использовали этот тип носимой электроники. Современные электрокардиостимуляторы имеют более совершенные свойства и расширенные функции.
Источник
Искусственная электронная кожа (e-skin) может частично воплотить эту идею в жизнь. Ученые из токийского университета (University of Tokyo’s School of Engineering, Tokyo, Japan) разрабатывают гибкие электронные схемы, которые можно установить непосредственно на кожу человека. Они легко деформируются и растягиваются, превращая поверхность в сенсорный экран, который полезен не только в повседневной жизни, но и в случаях нарушения местной иннервации (к примеру, ожогах). Простейший вариант этой технологии — татуировка, над каждой разновидностью которой давно работают различные группы специалистов.
Исследователи Иллинойского университета разработали имплантируемую сетку из компьютерных волокон, которые тоньше человеческого волоса и могут осуществлять мониторинг внутренних процессов тела с поверхности кожи. Компания Dangerous Things разработала NFC-чип, который имплантируется в палец с помощью очень простого процесса, похожего на нанесение татуировки, и позволяет вам разблокировать устройства или вводить код, просто указывая на нужный гаджет пальцем. В мире уже давно есть фанаты, разместившие в себе подобные чипы.
Источник
Специалисты Северо-восточного университета в США разработали «татуировку» со встроенными наносенсорами, которая предназначена для контроля уровня кислорода в крови у пациентов с анемией. Эта же система может использоваться, например, велосипедистами при мониторинге уровня натрия для предотвращения обезвоживания. Метод заключается в инъекции под кожу раствора, содержащего специально подобранные наночастицы, которые флюоресцируют при взаимодействии с молекулами натрия или глюкозы. С помощью смартфона можно контролирует изменения уровня флуоресценции.
Источник
Ряд компаний занимается разработкой сенсоров в виде временной татуировки — приклеиваемой на кожу тонкой пленки. В частности, компания Electrozyme разработала сенсор метаболических веществ, выделяемых вместе с потом, который позволяет спортсменам оценить свой электролитный баланс, уровень гидратации, напряжение мышц и физическую работоспособность. Учёные из Калифорнийского университета используют аналогичную технологию для мониторинга диабета.
Источник
Вместо татуировки можно создать и новую кожу. Стандартная электронная кожа состоит из матрицы, которая включает в себя различные электронные компоненты — гибкие транзисторы, органические светодиоды и фотогальванические ячейки. Зачастую такие устройства построены из очень тонких слоев материала, распыленного и вновь конденсированного на базе большой (до нескольких десятков квадратных сантиметров) основы для e-skin. Со временем ученые надеются превратить саму кожу в чувствительный экран, используя тактильные центры и сенсоры вместо дисплея.
Иногда так случается, что кое-что в теле пациента приходится заменить на полностью искусственное изделие. И неспроста протезирование является сегодня одним из самых востребованных и прогрессирующих направлений развития носимой электроники. Конечно, до искусственной руки Люка Скайуокера наука еще не дошла, но успехи электроники уже налицо. 
Источник
В частности, современные бионические протезы позволили расширить возможности людей с ограниченными возможностями. Функционально они максимально восполняют, к примеру, движения человеческой ноги, обеспечивают безопасность во всех фазах шага, естественность и гармоничность движений, дают пациенту ощущение уверенности на любой поверхности. В основе практически любого современного протеза лежит микропроцессор, который связан с большим количеством датчиков, расположенных по всему изделию. Сенсоры собирают информацию о наклоне поверхности и рельефе дороги, нагрузке на протез. Благодаря им центральный микропроцессор получает и обрабатывает всю необходимую информацию, в соответствии с которой и работает коленный модуль. Интеллектуальное управление протеза позволяет пользователю передвигаться в собственном удобном темпе.
Источник
Уже разработано устройство, позволяющее слепым людям видеть. «Электронные глаза» представляют собой очки со встроенными КМОП-камерами, которые фиксируют изображение, обрабатывают и преобразовывают его в матрицу сигналов управления вживленными в мозг человека электродами. С помощью сигналов электрической стимуляции, подаваемых на электроды, отображается грубый рельеф с выделением контуров предметов ближнего плана для ориентации. Данное устройство не позволяет видеть так, как видят зрячие люди, но для человека становится доступной общая схема расположения окружающих его предметов. 
Источник
Немецкая компания Implandata Ophthalmic Products разработала имплант, позволяющий обеспечивать постоянный мониторинг глазного давления для контроля развития глаукомы. 
Но, конечно, самым важным этапом любого протезирования является возможность управления «инородным телом» с помощью собственной нервной системы.
В настоящее время решается задача совмещения всех этих устройств с телом человека таким образом, чтобы полноценно взаимодействовать с нервной системой, поскольку органические материалы на основе углерода практически не отторгаются организмом. Правда, частицы углерода хорошо диффундируют через клеточные мембраны, что может вызывать еще не очень понимаемый специалистами иммунный ответ организма и, по неподтвержденным пока теориям, даже служить причиной возникновения опухолей. Вероятно, в ближайшие годы мы увидим, как прототипы электронных устройств на основе искусственной кожи входят в повседневную жизнь в качестве нательных датчиков, экранов и даже батарей, аккумулирующих получаемую непосредственно из человеческого тела энергию. Потом наступит время встроенных смартфонов, и люди смогут стать киборгами (если им это нужно, конечно, или, к примеру, если это будет прописано в конституции государства).
Еще дальше идут ученые, работающие над созданием технологий, превращающих все наше тело в пользовательский интерфейс, с помощью которого человек сможет управлять самыми разнообразными устройствами. 
Источник
В американском агентстве DARPA (Defense Advanced Research Projects Agency) разрабатываются имплантаты в мозг, способные как записывать сигналы, приходящие из нервных узлов по массиву электродов, так и стимулировать другие нервные узлы в реальном времени для того, чтобы эффективно переподключить поврежденные секции мозга, что позволяет восстановить память. Около уха размещается внешнее устройство, которое может обмениваться данными с имплантатом и контролировать его работу.
Источник
Тактильные интерфейсы — лишь первый шаг по превращению нашего тела в центр управления армией устройств, облегчающих и улучшающих жизнь человека. Так, в Японии и Италии создаются устройства, позволяющие людям с ограниченными возможностями открывать дверь или отвечать на телефонный звонок буквально силой мысли. Кроме того, идут разработки микрочипов, которые будут не просто восстанавливать утраченные нейронные связи, но и создавать новые.
Уже сегодня есть пациенты, которые используют имплантированные устройства, работающие совместно с мобильным приложением для того, чтобы открывать кодовый замок или контролировать течение болезни, а то и лечить ее. В частности, бионическая поджелудочная железа, которая проходит тестирование в Бостонском университете США имеет микро-сенсор на имплантированной в тело иголке, который передает на смартфон данные об уровне сахара в крови. 
Источник
Компания Stimwave Technologies разработала миниатюрное устройство-нейростимулятор для снятия болей в спине и ногах. Оно представляет собой беспроводной имплантат со встроенным чипом и электродами и вводится в организм с помощью обычной иглы. 
Фонд Гейтса поддерживает проект Массачусетского технологического университета по созданию имплантируемого женского контрацептива, который можно контролировать снаружи. Это миниатюрный, встроенный в тело чип, который генерирует небольшие количества контрацептивного гормона внутри женского тела и может работать до 16 лет без перерыва. Имплантация не более болезненная, чем нанесение татуировки. Кроме того, по мнению разработчиков, «возможность включить или выключить устройство — это очень удобный инструмент для тех, кто планирует состав своей семьи». Тут главное не потерять пульт управления… 
Компания Boston Scientific разработала имплантируемый нейростимулятор мозга Vercise, который предназначен для лечения людей с тремором (хроническим дрожанием). Имплантируемое устройство содержит батарею, которое может работать в течение 25 лет без замены, а сам прибор может очень точно настраиваться в соответствии с анатомией и потребностями пациента.
Источник
В целом нейростимуляция активирует зоны мозга, ответственные за память, реакцию, концентрацию внимания, математические и лингвистические способности, и здесь можно ожидать еще много новостей.
Источник
Задачей европейского проекта SmartHand является создание сменной руки, которая будет настолько близка по функциям к утраченной, насколько это возможно. SmartHand — это сложный протез с четырьмя двигателями и 40 датчиками, который подключается непосредственно к нервной системе пациента. SmartHand создает ощущение призрачной руки, известное многим, потерявшим конечность. Первый пациент уже мог поднимать предметы и ощущать кончики пальцев протеза.
Smart Dust или «умная пыль» — это, возможно, самая последняя инновация в имплантологии. Представьте себе матрицу из настоящих компьютеров с антеннами, каждый из которых много меньше песчинки, которая может самоорганизовываться внутри тела в любую нужную сеть для того, чтобы обеспечить выполнение различных сложных внутренних процессов. Полчища этих микро-устройств могут атаковать ранние проявления рака или избавлять от боли в ране. Быть может, они смогут хранить важную информацию, которую трудно расшифровать или украсть. Используя «умную пыль», врачи смогут осуществлять различные действия без необходимости нарушения целостности тела — доставлять нужные лекарства в нужные места, проводить внутренние операции, осматривать внутренние органы и многое другое.
Источник
Какая бы электроника не использовалась, ей нужна батарея питания. В настоящее время для этого используются миниатюрные химические элементы питания, металл-гидридные и полимерные аккумуляторы, топливные элементы, термогенераторы, пьезопреобразователи и преобразователи кинетической энергии в электрическую. А, к примеру, одной из важнейших проблем технологий имплантации является доставка питания в устройство, которое находится в теле человека, и его нельзя извлекать или подключить к электророзетке. 
Исследователи Лаборатории Драпера в Кембриджском университете разработали биоразлагаемую батарею, которая способна генерировать энергию внутри тела, передавать ее беспроводным способом, если это необходимо, а затем растворяться и исчезать.
Источник
В ученом мире хватает скептиков, но хватает и тех, кто с энтузиазмом принимает новые возможности человека. 
Наверняка электроника в нашем теле сразу же будет использоваться и для развлечений. Возможности открываются безграничные — от создания реалистичной картинки прямо в глазу человека до реалистичных симуляций игровой реальности. Главное опять же — не утратить управление всеми этими чудо-приборами.
В скором будущем уже осуществятся и прогнозы фантастов об оружии, срабатывающем только в руках владельца. К авторизации по отпечатку пальца добавится и пароль, вшитый в ваш личный чип-идентификатор.
Подключение человеческого мозга напрямую к компьютеру — это мечта (или кошмар) любителей фантастики и чудесных изобретений. И эта мечта, похоже, близка к реализации. Исследователи из компании BrainGate при Университете Брауна в США занимаются именно этой задачей. Используя имплантированный в мозг набор электродов размером с таблетку аспирина, ученые смогли показать, что сигналы нейронов могут быть в реальном времени декодированы компьютером и использованы для управления различными устройствами. А команда исследователей из University of the Witwatersrand в Йоханнесбурге уже подключила человеческий мозг напрямую к сети Интернет в режиме реального времени. Проект называется Brainternet, что хорошо перекликается с приведенным выше. Собранные в ходе эксперимента данные должны помочь разработчикам при создании интерфейсов нового поколения.
Источник
По прогнозам компании Intel переход к практическому использованию интерфейса компьютер-мозг человека начнется еще до 2020 года. Представьте, что вы получили способность пользоваться Интернетом, используя свои мыслительные способности, что может показаться феноменальной возможностью. Остается только научиться избавляться от вирусов с хакерами и путаницы в мыслях. Возможно, кто-нибудь даже поможет вам разобраться. И не исключено, что этот «кто-то» может оказаться Большим Братом, которому очень пригодятся управляемые киборги. Таким образом, прогрессивное человечество стоит на пороге изменений, которые могут сделать мир и намного удобнее, комфортнее, и, как думают некоторые, политически грамотнее.
С другой стороны, научившись пользоваться мозгом как инструментом управления внешней, носимой и внедренной электроникой, человечество делает еще один шаг к тому, что давно создано природой. К примеру, разве не с помощью мозга мы управляем своим телом? И, может быть, не до конца поняли, что мы еще умеем?
Автор публикации:
Александр ГОЛЫШКО, системный аналитик ГК «Техносерв»
По материалам: kit-e.ru, zhenskiyblog.ru, 3dnews.ru, ferra.ru, 374.ru, kaspersky.com, hi-news.ru, popmech.ru, ixbt.com, orto-kosmos.ru.

User
=====
Статистика популярности операционных систем в IaaS: Ubuntu пока номер один, популярность CentOS растет
2017-12-11, 14:48
Сетевые технологии и оборудование, гаджеты
=====
Как пользователи воспринимают разные методы аутентификации
2017-12-11, 14:44
Пользователь
=====
Как написать UI-тесты с использованием Instagram-аккаунтов и не получить блок
2017-12-13, 13:10
LearnQA.ru — онлайн курсы для тестировщиков
=====
Создание децентрализованного музыкального плеера на IPFS
2017-12-11, 14:59

В этой статье описаны результаты двухмесячных экспериментов с IPFS. Главным итогом этих экспериментов стало создание proof-of-concept стримингового аудио плеера, способного формировать фонотеку исключительно на основе информации, публикуемой в распределённой сети IPFS, начиная с метаданных (название альбома, треклист, обложка), заканчивая непосредственно аудио-файлами.
Таким образом, будучи десктопным electron-приложением, плеер не зависит ни от одного централизованного ресурса.
На первый взгляд технология напоминает собой что-то среднее между BitTorrent, DC, git и blockchain. На второй выясняется, что IPFS может хранить и распространять объекты любого вышеупомянутого протокола.
Начнём с того, что IPFS — это одноранговая сеть компьютеров, которые обмениваются данными. Для её формирования используется комбинация различных сетевых технологий, развивающаяся в рамках отдельного проекта libp2p. На каждом компьютере имеется специальный кэш, где до поры до времени хранятся данные, которые пользователь публиковал либо качал. Информация в кэше IPFS хранится в виде блоков и по умолчанию не закреплена. Как только места станет не хватать, местный garbage collector подчистит блоки, к которым давно не обращались. Методы pin / unpin отвечают за то, чтобы закрепить / открепить информацию в кэше.
Для каждого блока рассчитывается уникальный криптографический отпечаток его содержимого, который потом выступает в качестве ссылки на это содержимое в IPFS-пространстве. Блоки могут объединяться в merkle-tree объекты, узлы которых обладают собственным отпечатком, рассчитанным на основе отпечатков нижних узлов / блоков. Тут и начинается самое любопытное.
IPFS состоит из большого количества более мелких проектов Protocol Labs, один из которых носит название IPLD (InterPlanetary Linked Data). Не уверен, что смогу грамотно объяснить суть, так что любопытным оставлю ссылку на спецификацию.
Если коротко, то описанная в предыдущем параграфе архитектура настолько гибкая, что на её основе можно воссоздавать более сложные структуры, такие как блокчейн, unix-подобная файловая система или репозиторий git. Для этого используется соответствующие интерфейсы / плагины.
Я пока работал исключительно с dag-интерфейсом, позволяющим публиковать в IPFS обычные JSON-объекты:
В случае успеха возвращается объект контент-идентификатора (CID), внутри которого хранится уникальный отпечаток опубликованной информации, а также методы его конвертации в различные форматы. Вот таким образом можно получить base-строку.
Эта строка может использоваться для получения данных обратно при помощи метода get. При этом никаких дополнительных параметров уже не требуется, так как в строке содержится информация о формате данных и использованном алгоритме шифрования:
Дальше — больше. Можно опубликовать второй объект, одно из полей которого будет ссылаться на первый.
При попытке получить obj2 обратно будут разрешены все внутренние ссылки, если специальным параметром не будет указано обратное.
Можно затребовать значение отдельного поля, для чего нужно к CID-строке добавить постфикс вида /название_поля.
Таким образом можно "ходить" по всему объекту, в том числе по его ссылкам на другие объекты как если бы они были одним большим объектом. Так как ссылки являются криптографическими хешами содержимого, объект не может содержать ссылку на самого себя.
Основополагающую роль в реализации задуманного функционала сыграл интерфейс pubsub. С его помощью можно подписываться на p2p-комнаты, объединённые специальной строкой-ключом, рассылать свои сообщения и принимать чужие.
Одним из первых приложений, основанных на pubsub, был irc-подобный p2p-чат orbit (до сих пор работает, если что). Подписываешься на канал, получаешь сообщения, история хранится между участниками и постепенно "забывается". Такой Vypress для всего Интернета.
В какой-то момент своей аудиофильской жизни я заметил, что уже давно слушаю музыку исключительно при помощи Google Music. Причём не только на телефоне, но уже и на десктопе. Причин несколько, но главная — ощутимо меньшее количество кликов между "хочу послушать" и "слушаю" в большинстве повседневных ситуаций в сравнении с теми же торрентами. Вторая по важности причина — размер доступной фонотеки. Очень редко я не нахожу то, что ищу (гораздо чаще я встречаю региональные ограничения). Но это уже преимущество не перед торрентами, а другими стриминговыми сервисами.

Стало любопытно, можно ли создать сравнимое по юзабилити приложение, основанное исключительно на распределённых источниках данных. Со временем были сформулированы два главных условия, при которых эта задача становится выполнимой:
В BitTorrent-экосистеме когда кому-то захочется что-то скачать ему в большинстве случаев придётся сначала обратиться к торрент-каталогу — обычному сайту, расположенному на обычном сервере за обычным ip под обычным доменом. Первое условие необходимо для того, чтобы приложение могло формировать локальную базу данных автоматически и без участия централизованных ресурсов. 
Второе условие наиболее важное. Для его выполнения необходимо, чтобы публикуемая участниками распределённой сети информация оформлялась в соответствии с заранее известной всем остальным моделью / схемой. Это делает распределённое пространство данных доступным для систем поиска и фильтрации, повышает их результативность.
Как оказалось, IPFS даёт возможность обеспечить оба эти условия, причём довольно эффективно.
На сегодня "Патефон" представляет собой примитивную плеер-фонотеку. Вот практически вся его функциональность:
Начнём с публикации. В предыдущей главе я говорил о том, что информация в распределённой сети должна быть каким-то образом стандартизирована. В "Патефоне" эта задача достигается за счёт json-схемы альбома. 
В соответствии с этой схемой конкретный экземпляр проверяется, а затем и публикуется. За это отвечает обычная форма ввода.

Сперва предполагалось, что публикация будет происходить напрямую с файловой системы. Стандартизация данных при этом достигалась бы при помощи специального .meta файла, где были бы описаны все необходимые свойства и связи экземпляра. Но немного поэкспериментировав пришёл к выводу, что делать это через интерфейс самого приложения будет удобнее, нагляднее и эффективнее.

Когда форма заполнена и пользователь жмёт "ADD ALBUM" итоговые JSON-данные сперва проверяются на соответствие схеме, затем публикуются при помощи DAG-интерфейса, после чего полученный CID в виде буфера рассылается в специальной pubsub-комнате.
Очевидным решением было бы использовать в качестве ключа комнаты что-то вроде "albums", "music-albums" или, в крайнем случае, "pathephone", если хочется изолировать сеть в рамках своего приложения. Но потом в голову пришла более умная мысль — использовать в качестве ключа CID-строку json-схемы альбома. В конце концов именно cхема данных по-настоящему объединяет все копии приложения. Более того, именно от этой комнаты можно ожидать, что все принимаемые сообщения будут содержать проверенные экземпляры этой схемы.
Как видите, каждый "прилетающий" объект всё равно проверяется на соответствие схеме, так как никто не мешает любому участнику сети, зная ключ, целенаправленно начать флудить в комнате. В то же время уникальность ключа сводит к минимуму вероятность случайного вмешательства. С другой стороны, если какое-нибудь другое приложение начнёт пользоваться той же схемой, что и "Патефон", то они вместе начнут формировать и использовать одно и то же пространство данных, что может пойти в плюс.
За скобками остались некоторые специфичные для приложения вещи, вроде сохранения объекта в базе данных. 
Всё описанное выше я оформил в виде npm-модуля под названием @metabin/gate. Он прячет детали, оставляя разработчику лишь передать ipfs-ноду и json-схему:
IPFS — одна из самых продвинутых технологий распределённого обмена информацией. Несмотря на то, что обе (и go и javascript) версии находятся в стадии альфы, ими уже пользуются некоторые относительно популярные приложения, вроде OpenBazaar или d.tube. Другое дело, что используют его в основном только в качестве файлового хранилища.
Это можно понять, учитывая что IPFS в основном и рассматривается как своеобразная альтернатива BitTorrent. Концепция IPLD и возможности соответствующих интерфейсов редко оказываются в центре внимания. Хотя, на мой взгляд, это самая перспективная разработка Protocol Labs, дающая возможность распространять через IPFS обычные объекты данных.
А совместив нехитрым образом files, dag и pubsub-интерфейсы разработчик получает бесплатный, самоорганизующийся источник данных для своего приложения. Добавьте Electron и получите довольно заманчивый стек технологий:
Само собой, не всем приложениям такая автономность вообще нужна, учитывая что она ведёт к потере контроля над контентом. Не говоря уже о том, что всё это довольно экспериментальная область. Как такая сеть будет масштабироваться, как это скажется на производительности, можно ли на этот процесс повлиять — вопросов много, ещё больше не сформулировано.
Тем не менее возможности и перспективы открываются любопытные.
User
=====
Митап BugBusters: Все о Selenium, эффективных автоматизаторах и буднях нагрузочных тестировщиков
2017-12-11, 15:16

Буквально на днях в нашем офисе прошел митап по тестированию. Поговорили о тестах, которые сами попросят выполнить их ещё раз, о суперпроизводительных командах автоматизаторов, а также окунулись в будни нагрузочного тестировщика. 
Для всех, кто не смог присоединиться, предлагаю записи докладов.
С первым докладом выступил Кирилл Меркушев из Яндекса. Он рассказал, как без остановки гонять WebDriver-тесты и какая для этого нужна инфраструктура. 
Основное внимание Кирилл уделил созданию такой инфраструктуры на базе контейнеров, Selenoid и других инструментов Aerokube.
Антон Шапин из Epam поделился своим видением dream-team автоматизаторов. В докладе он рассказал, как им удалось собрать команду и организовать её работу. 
Отдельно Антон остановился на проблемах, возникавших при формировании команды, и вопросах разумного использования возможностей каждого участника.
Завершился митап бодрым докладом Анатолия Пласковского из Яндекс.Денег, на котором слушатели ближе познакомились с деятельностью нагрузочника.
Ключевые эпизоды: приёмка релизных сборок, боевые стрельбы и переговоры с командами.
Если хочется уточнить наболевшее или непонятное – ждем вас в комментариях.
Реактивист-разработчик Android
=====
Как IaaS помогает франчайзи «1С»: опыт 1cloud
2017-12-11, 16:01
Пользователь
=====
Дебаты об отличном сервере приложений Java c Tomcat, Jboss, GlassFish, Jetty и Liberty Profile. Часть вторая
2017-12-11, 15:53
Редактор
=====
Как правильно выбрать базу данных для вашей организации
2017-12-11, 15:53
Пользователь
=====
Утечка данных в Uber – учимся на ошибках?
2017-12-14, 10:30
Эксперт по безопасности
=====
Нацеленное скликивание рекламного бюджета в Я.Директ
2017-12-11, 20:13
Web-разработчик
=====
Использование Google FireBase для создания простого чата на Android
2017-12-11, 17:21
Developer
=====
Обзор зарплат тестировщиков или стоит ли работать за еду?
2017-12-11, 17:49
Head of QA
=====
Интеллектуальная собственность – нематериальный актив
2017-12-11, 18:04
Intellectual Property Manager at Parallels
=====
Оптимизация Android-приложения для работы с док-станцией Samsung DeX
2018-04-19, 12:20
Год назад появился смартфон Samsung Galaxy S8/S8+, а вместе с ним и док-станция DeX, позволяющая пользователю превратить телефон в полноценный компьютер. Для этого достаточно поставить телефон на станцию, и на экране подключенного монитора отобразится рабочий стол, похожий на Windows 8+, а также будут доступны другие периферийные устройства (мышь, клавиатура, принтер). Инновационная архитектура приложения МойОфис Документы позволяет адаптировать и поддержать технологические новинки (такие как DeX) без существенных затрат. Особенно приятно, что стоимость внедрения такого решения невелика, а количество пользователей, которые могут задействовать приложения на большом экране, достаточно высоко. В этой статье мы поделимся с вами опытом, как реализовать поддержку док-станции DeX в вашем Android-приложении. 

Прежде чем мы углубимся в технические особенности, стоит отметить, что многое из описанного ниже можно использовать не только в связке с Samsung DeX, достаточно подключить любую Bluetooth-клавиатуру и/или Bluetooth-мышь. Очевидно, что для удобства использования вашего приложения с подключенной периферией необходимо будет произвести ряд манипуляций с самим приложением. И хотя далеко не каждому приложению нужна поддержка десктопного режима, гибкость в использовании никогда не бывает лишней.

Источник: Samsung
Итак, поехали! Прежде всего следует кратко рассмотреть, что именно нужно изменить в приложении, чтобы оно хорошо смотрелось на Samsung DeX и с ним было комфортно работать пользователю.
Начнем с того, что Samsung DeX — это по сути доработка многооконного режима, появившегося в Android 7 (Nougat). То есть если ваше приложение уже приспособлено к работе в этом режиме, то никаких проблем при запуске на док-станции не будет. Однако для полноценного мимикрирования под десктоп необходимо учесть ряд особенностей и добавить дополнительную функциональность, представленную в таблице ниже и условно разделенную на основную и предпочтительную:
Запускаемые на Samsung DeX приложения условно можно разделить на три группы в зависимости от типа окна:
На деле все будет выглядеть следующим образом:
МойОфис Документы
Теперь же углубимся в детали.
Как уже было отмечено выше, ваши приложения, запущенные непосредственно на мобильном телефоне (речь идет о Samsung Galaxy S8/S8+) и транслируемые посредством док-станции, будут иметь разные значения dpi: 640 dpi (xxhdpi) и 160 dpi (mdpi) соответственно. Так что если вы еще не используете векторную графику, то стоит подготовить соответствующие ресурсы для изображений. Также нужно учесть, что может поплыть верстка, — следует подготовить альтернативные лэйауты как один из вариантов решения проблемы. Более подробно о работе с экранами с различной плотностью пикселей можно почитать в официальной документации для разработчиков Android.
Если вы хотите, чтобы окошко вашего приложения ресайзилось, достаточно добавить в манифесте приложения следующий флажок для каждой activity, размеры которой должны изменяться:
Поддержка многооконности — «фишка» не новая, наверняка с выходом Android Nougat многие успели ее прикрутить или хотя бы посмотреть в ее сторону. Все неплохо расписано в официальной документации. Реализовать данную функциональность достаточно просто: ставим флажок в манифесте, и готово:
Тут стоит учесть, что при переходе в многооконный режим и наоборот activity будет пересоздаваться каждый раз, так как в этом случае система должна выбрать необходимые ресурсы. В условиях нашего приложения МойОфис Документы activity пересоздаваться нельзя, поэтому мы используем следующий набор флагов в манифесте в «непересоздаваемых» activity:
В этом случае все необходимые замены лэйаутов необходимо обрабатывать ручками, например в методе onConfigurationChanged(Configuration newConfig) activity.
Нам может потребоваться узнать, находимся ли мы сейчас в desktop-режиме посредством DeX-станции (как правило, для изменения UI приложения). Делается это следующим образом:
А следующий флаг в манифесте необходим для установки поведения приложения при выходе из desktop-режима:
В приведенной ниже таблице показано, как будет вести себя приложение при установленном флаге или без него:
 
Прокрутка контента в десктопном режиме осуществляется колесиком мыши, а сам принцип скроллинга работает автоматически при использовании ListView, RecyclerView, GridView или ScrollView/HorizontalScrollView компонентов. Дополнительно рекомендуется реализовать поддержку полос прокрутки, чтобы пользователь мог совершить прокрутку, потянув за соответствующий компонент. Также можно добавить скроллирование посредством колесика мыши, если вы реализуете свой компонент, как это сделано в нашем случае, но об этом будет написано немного ниже (а добавление полос прокрутки осуществляется достаточно просто).
Поддержка мыши может быть полезна для множества приложений (карты и навигация; игры; в нашем случае документы и т. п.), которые запускаются на DeX док-станции.
У объекта MotionEvent есть метод getSource(), при помощи которого можно определить источник данного события. В коде это выглядит так:
Понять, что по элементу произошел тач правой кнопкой мыши, можно при помощи метода getButtonState() у объекта MotionEvent:
Теперь по клику правой кнопкой мыши можно открыть контекстное меню.
Однако в ряде моделей телефонов (не Sumsung Galaxy S8/S8+) реализована иная логика обработки нажатия на правую кнопку мыши. Поэтому на этих устройствах реализация этих функций будет недоступна.
Работа с мышью не была бы полноценной, если бы при помощи ее колесика нельзя было скроллить и зумить. Следующий участок кода демонстрирует работу с колесиком мыши:
Поведенческие сценарии могут различаться, но общий шаблон, думаю, понятен.
Когда мы наводим мышь на кликабельный элемент, принято, чтобы курсор изменялся. Для реализации этой возможности необходимо установить следующий флаг на кликабельной вью:
Далее нужно непосредственно в коде установить для вью кастомный PointerIcon при помощи метода setPointerIcon() (см. также view.onResolvePointerIcon(...)).
Поддержка хардварной клавиатуры производится проще: достаточно ее просто подключить к устройству. Единственное, на что стоит обратить внимание, — это обработка хоткеев:
Практически все, что описано в статье, в том или ином виде можно найти в официальной документации. Samsung предлагает разработчикам произвести ряд дополнительных улучшений:
Ниже приведен пример имплементации OnDragListener:
Как все это тестировать? Самый простой способ — это приобрести док-станцию Samsung DeX, подключить к ней всю периферию и баловаться. Если док-станции по каким-либо причинам в наличии нет, то, как уже было отмечено выше, можно просто подключить к смартфону, а еще лучше планшету, желательно, с Android 7 и выше, Bluetooth-клавиатуру и мышь. Также можно воспользоваться способом, предлагаемым непосредственно Samsung, если вы предпочитаете работать с эмулятором.
Определенно, как железка, Samsung DeX — устройство интересное и точно будет востребовано у тех пользователей, которым важны мобильность и необходимость заменить несколько рабочих девайсов смартфоном. В соответствии с нашим опытом практически любое приложение можно с легкостью адаптировать для работы в десктопном режиме на этой док-станции или при присоединении хардварных контроллов. Надеемся, что эта статья будет вам полезна.
Официальный аккаунт компании
=====
Основы информационной безопасности. Цена ошибки
2017-12-11, 19:10
Пользователь
=====
Как обсуждать биткойн и не выглядеть дураком
2017-12-11, 19:45
В этом праздничном сезоне будет трудно обойтись без обсуждения биткойнов. Поскольку стоимость биткойнов заигрывает с 20 000 долларов, а институциональные инвесторы начинают вкладывать большие деньги во фьючерсы на биткойн, этот вопрос стал горячей темой для обсуждения за кофе, бизнес-ланчами, праздничными столами и т. д.
Краткие советы помогут вам не выглядеть глупо при обсуждении биткойна. Независимо от того, считаете ли вы, что биткойн-мания — это тульпаномания или анархистская революция, вы подвергнете сомнению свой авторитет в сообществе криптопредпринимателей, если будете как попугай повторять неправильные мейнстримные представления о биткойне. Эти советы помогут вам обрести ясность и закрепить понимание этой потенциально революционной технологии.
Возможно, вы являетесь экспертом по криптовалютам — в этом случае вам, вероятно, не нужна эта статья. Отправьте эту статью приятелю, которому она может пригодиться! (Примечание: я использую «Биткойн» для описания сети и протокола и «биткойн(ы)» для описания единиц криптовалюты.)
Если вы не шутите или троллите, попробуйте не говорить следующего:
«Генеральный директор Биткойна». Биткойн не контролируется и не выдается какой-либо центральной организацией. Таким образом, нет ни компании, ни генерального директора, ни сотрудников, которые могли бы контролировать производство, передачу, хранение и использование всех биткойнов. Так, не стоит рекомендовать друзьям «обращаться в службу поддержки Биткоин», если они потеряют доступ к личным ключам. Если они не держат свои биткойны в кошельке или на бирже наподобие Coinbase или Kraken, то несут полную ответственность за сохранность своих ключей. Потеря доступа к закрытым ключам по существу приводит к потере самих биткойнов.
«Никто не знает количество существующих биткойнов!» или «Существует неограниченное количество биткойнов!». Количество биткойнов прозрачно и график уровня инфляции неизменно повторяется. Более того, общее количество биткойнов ограничено: в мире будет только 21 миллион биткойнов.
«Никто не понимает, как работает биткойн». В официальном издании объясняется, как работает биткойн: https://bitcoin.org/bitcoin.pdf. В документе подробно описываются криптографические и экономические основы, которые позволяют биткойну функционировать как децентрализованные в P2P денежные средства. Кроме того, программное обеспечение, лежащее в основе протокола и сети, полностью открыто и проверено: https://bitcoincore.org/.
«В биткойне нет прозрачности». Все транзакции биткойнов записываются в блокчейне и общедоступны для всех, кто подключен к Интернету. Записи транзакций постоянны — то есть они не могут быть изменены. Когда люди жалуются на то, что биткойн непрозрачен, то они хотят сказать, что идентификационная информация сторон транзакций скрыта за псевдонимами: у сторон есть сгенерированный компьютером адрес и требуется приложить усилия, чтобы узнать идентификационные данные тех, кто стоит за этим адресом. Однако, как только личные идентификаторы будут обнаружены и привязаны к конкретным адресам, вы сможете отслеживать все транзакции с самого начала сети. Несмотря на то, что существуют методы, позволяющие избежать такого рода трассировки для проблем, связанных с конфиденциальностью, факт остается фактом: блокчейн фактически облегчает прозрачность, если присутствуют транзакции.
«Биткойн не является законным, потому что он не поддерживается ничьим правительством!». В этом весь смысл — основная ценность биткойнов исходит из того факта, что он не поддерживается правительством или каким-либо другим центральным аппаратом. Сеть полагается на обширную и децентрализованную группу людей по всему миру для размещения записей транзакций и их проверки, чтобы не допустить ситуации, когда пользователи дважды тратят один и тот же биткойн.
«Биткойн — это мошенничество». Это заявление не имеет большого смысла, если не дать соответствующих пояснений. Во-первых, кто мошенничает? Не существует центрального подразделения по выпуску и продаже биткойнов. Нет ООО «Биткойн», которое платит за сенсационалистские объявления в Google, чтобы ввести вас в заблуждение при покупке биткойна. История и технология Биткойна прозрачны и доступны всем — вы можете сами судить о достоинствах технологий и экономики. Во-вторых, в каком контексте Биткойн является аферой? Биткойн реально применяется во многих юридических сделках, включая покупку недвижимости и трансграничные платежи. В гиперинфляционных странах, таких как Венесуэла и Зимбабве, биткойны рассматриваются как алтернативный резерв денежному объему, который может защитить граждан от быстрой девальвации валюты.
«Я не могу купить биткойн, потому что он слишком дорогой!». Вы не обязаны покупать целый биткойн! Вы можете купить долю. Сатоши — это самая маленькая единица биткойна, которую можно купить. Сто сатоши в настоящее время стоят всего около 1,5 цента в долларах США. Вы можете быстро и легко приобрести биткойн через Coinbase.
Надеюсь, что эта статья повысила ваш личный рейтинг и интеллектуальные возможности, или, по крайней мере, помогла вам их не потерять. Поделитесь статьей с другом, который не в состоянии разобраться в этой теме!
Статья является моим переводом материала Lindsay X. Lin «How to Discuss Bitcoin Without Sounding Like an Idiot».
IT-специалист
=====
Scrum внедрили, agile — забыли
2017-12-16, 10:46
Пользователь
=====
Туториал по Unreal Engine. Часть 2: Blueprints
2017-12-14, 11:18
Переводчик-фрилансер
=====
Знаки качества — дизайн и история
2017-12-11, 21:03
Графический дизайнер
=====
QueryDSL: Предикаты
2018-01-15, 09:34
backend.developer { java, kotlin }
=====
ISTQBить или не ISTQBить? Подготовка к сертификации Test Manager Advanced Level
2017-12-11, 23:04
Software Test Lead
=====
Дизайн–система Acronis. Часть вторая. Иконки, SVG шрифты, Gulp
2017-12-12, 10:39


Меня зовут Сергей, и я все еще работаю старшим дизайнером в компании Acronis. Мы продолжаем оптимизировать работу отдела дизайна продуктов для бизнеса и все больше интегрируемся в процессы на стороне front-end разработки.
В прошлой статье я рассказывал о создании библиотеки компонентов, выборе инструментов и взаимодействии с разработчиками. В этой статье я сделаю акцент на мелочах. Расскажу про SVG шрифты, работу с иконками и приправлю рассказ толикой автоматизации с использованием Gulp.

До вмешательства ситуация с иконками выглядела следующим образом:


В первом случае использовались PNG спрайты, во втором — один из двух шрифтов с примерно одинаковым набором иконок, а в третьем случае SVG хранились в JS и инлайнились в проект по мере необходимости. Складывалась абсурдная ситуация, когда добавление новой иконки в несколько продуктов превращалось в утомительный квест с привлечением разработчиков из разных команд.
Дизайнеры не отставали и старательно поддерживали похожий бардак у себя. Часть иконок в виде отдельных SVG была разложена по папкам, другая часть хранилась в AI файлах, а остальные иконки были доступны только в Sketch и Zeplin.
Забегая вперед скажу, что львиную долю проблем мы решили. Актуальные иконки собрали в одном месте, систематизировали и разбили по продуктам. В качестве универсального решения выбрали SVG шрифт, а процесс обновления и сборки шрифта автоматизировали и забрали на сторону дизайна.
Дизайнеры работают с терминалом, коммитят и пушат все изменения в git.




На первом шаге мы собрали все иконки в одном Sketch файле, разбили по продуктам и добавили в Zeplin.


С помощью такого подхода мы выявили недостающие, лишние и дублирующие друг друга иконки, а разработчики получили наглядные документы по каждому продукту.
Вместо отсутствующих иконок и в макетах, и на верстке мы используем заглушки.


Как только недостающая иконка появляется в шрифте и новая сборка публикуется в npm, заглушка автоматически меняется на корректное изображение.
На втором шаге собранные иконки мы добавили в Lingo и, для удобства поиска, протегировали. Теперь иконки ищутся по размерам, типам, синонимам, именам или фильтруются по продуктам.


Добавление иконок в новый документ или обновление уже существующего документа происходит с помощью Sketch плагина, который идет в комплекте с Lingo. К слову, в Lingo хранятся не только иконки, но и вся библиотека компонентов. Чуть подробнее об этом приложении я рассказывал в прошлой статье.
Любую иконку из библиотеки можно покрасить в другой цвет через overrides без необходимости разгруппировывать символы или держать в наборе дубликаты с другим цветом. Достигается это за счет наложения квадратной маски на шейп внутри символа.


Несмотря на очевидные плюсы, у этого способа оказался один существенный минус. При использовании масок код после экспорта SVG выглядел так:


Корректно добавить такую иконку в шрифт нельзя, из-за маски на месте иконки будет цветной прямоугольник. Идею держать дополнительный набор "чистых" иконок для разработчиков мы отбросили сразу как трудозатратную и стали искать решение. Первым делом мы посмотрели в сторону SVGO и провели несколько тестов:


SVGO удалил много лишнего, оптимизировал и минифицировал код, но основную проблему решить не смог. Немного погуглив мы поняли, что подходящих решений в открытом доступе нет и встали перед выбором. Либо отказываемся от масок на стороне дизайна, либо разрабатываем свое решение.
Конечно же, мы выбрали второй вариант и решили собрать свой велосипед с катафотами и брызговиками. На всякий случай мы проверили несколько десятков иконок и убедились, что Sketch отдает примерно одинаковый код для любого экспортированного SVG из библиотеки.


В таком коде легко найти закономерности, а вырезать из него лишнее и заново пересобрать SVG — дело техники. Итогом непродолжительной работы стала gulp-задача которая делает следующее:


Для запуска задачи достаточно написать в консоли одну команду, а работа с набором из 200 иконок занимает меньше секунды и дает именно тот результат, который нужен. 
Да, на дворе конец 2017 года, а мы используем SVG шрифт. Почему? Во-первых, так исторически сложилось, а во-вторых, это решение оказалось наиболее универсальным для всех команд разработки. К тому же, поддержка старыми браузерами для нас гораздо важнее тех возможностей, которые открываются при использовании современных техник работы с SVG.
До настоящего момента на стороне разработки использовались два способа сборки шрифтов. 
Icomoon мы сразу отбросили из-за большого количества ручной работы. FontCustom, после детальных тестов, заменили на более гибкий и предсказуемый gulp-iconfont который при установке не требует ничего лишнего и достаточно прост в настройке. Для каждой новой версии шрифта iconfont создает html превью со всеми иконками. Превью дает возможность контролировать качество иконок перед тем, как изменения уйдут в репозиторий.


Дальше дизайнер пушит обновленную версию в git, где срабатывает триггер, который дергает задачу в Jenkins на сборку и публикацию в npm.
Давайте еще раз посмотрим на процесс до вмешательства:


А теперь после:


Нам удалось не только оптимизировать, упростить и автоматизировать процесс работы с иконками по обе стороны баррикад, но и оздоровить коммуникацию между отделами. Разработчикам больше не нужно бегать за дизайнерами и выпрашивать иконки, а дизайнерам следить за качеством иконок в новых билдах. Мы получили прозрачную схему взаимодействия, внутренний контроль качества и значительную экономию времени для всех участников процесса.
Отдельное спасибо Сергею Сабурову и Дмитрию Козлову за помощь, терпение и поддержку. 
Кстати, мы всегда рады опытным дизайнерам. Если вы такой, напишите мне на почту: sergey.nikishkin@acronis.com
Если вы опытный, но не дизайнер, посмотрите открытые вакансии на HH.
Design Lead in Acronis
=====
Продвижение ПО на Запад: ожидание vs.реальность на примере одного видеоредактора
2017-12-12, 12:18
Пользователь
=====
Security Week 49: новые приключения Coinhive, дурацкая ошибка фишеров, прикрыта лавочка по торговле паролями
2017-12-12, 02:10
Пользователь
=====
Пускаю слюни, пишу код
2017-12-12, 13:02
Строю реактивный ранец и конкурента Википедии
=====
Интеграция — байки
2017-12-12, 09:56
EA
=====
Советы по чистому коду новичкам в Java/Android
2017-12-13, 13:55
Теме чистого кода на одном только habrahabr посвящено тысячи статей. Никогда бы не подумал, что захочу написать свою. Но после проведения компанией курсов для желающих связать карьеру с разработкой ПО, в частности разрабатывать под Android, мое мнение поменялось. 
За основу статьи взяты советы из классики “Роберт К. Мартин: Чистый код”. Отобрал из них те, которые наиболее часто встречались в виде проблем у студентов. Приведенные советы написаны с учетом моего опыта разработки коммерческих Android приложений. Поэтому не ко всем Android-проектам приведенные ниже советы подойдут, я уже не говорю про другие системы.
Советы в основном приводил с примерами кода как НЕ НУЖНО делать. Как ни странно, у большинства студентов были одни и те же ошибки. Все примеры кода были придуманы, любые совпадения с реально существующим кодом случайны.
1. Код должен быть легко читаемым, понятным и очевидным
Программисты большую часть времени тратят на чтение и анализ написанного кода, а не на написание нового. Важно чтобы Ваш код был легко читаемым, понятным и с прогнозируемым поведением. Это позволит коллегам и Вам по прошествии времени затратить минимальное время на понимание того, что делает каждый кусок кода. Понятный код с прогнозируемым поведением позволит уменьшить вероятность ошибки при внесении изменений не автором кода.
2. При написании кода нужно придерживаться Java Code Conventions либо других спецификаций, принятых на проекте командой
Спецификацию можно сравнить с правилами оформления текста книги. Думаю, немногие захотят читать книгу, где каждый абзац отличается шрифтом, размером и цветом текста. Так и с кодом, куда легче читать оформленный в едином стиле код.
1. Имена классов, функций, переменных и параметров должны передавать закладываемый в них смысл.
Довольно очевидный совет, но не всегда его придерживаются. Тут, в качестве примера, можно привести оглавление книги: если все главы названы просто "Главами", то понять в чем суть нельзя. Наименования классов, функций должны сообщать об их предназначении без погружения в детали реализации.
Наименования классов MyActivity, CustomView, MyAdapter говорит только об одном, что это не часть Android SDK и все. SecondActivity говорит, что это не часть Android SDK и о наличии в проекте еще одного Activity, но не факт =)
Правильно именовать классы: MainActivity, FoodsActivity, FoodCartActivity, PhoneInputView. MainActivity — очевидно, что основной разводящий экран приложения. FoodsActivity — экран с перечнем продуктов. PhoneInputView — компонент ввода номера телефона.

Следующие названия переменных бесполезны и нарушают правила наименования:
как и параметры конструктора:
2. Название публичного класса и файла должно совпадать.
Есть следующий класс
Название файла при этом оказывается “products_Activity.java”.
Это не правильно. Название публичного класса и java-файла должны совпадать.
3. В наименованиях нужно использовать только буквы латинского алфавита, никаких цифр, символов подчеркивания и дефисов. Исключения составляют наименования из стандартов (ГОСТ, ISO), символы подчеркивания для разделения слов в наименованиях констант. 
Примеры выше: m_textview_1. Часто вместо lastName пишут userName2, что не правильно.
4. Не нужно использовать строчные “L” и “O” в качестве имен локальных переменных, т.к. их трудно отличить от “1” и “0”.
Надуманный пример, но что-то подобное я встречал в своей практике
В этой функции пузырьковой сортировки есть одна ошибка, сможете за секунды ее найти=)?
Такой код труден для чтения и при написании легко сделать ошибку, которую можно очень долго искать.
5. Не нужно указывать тип в суффиксе имен.
Вместо accountList нужно писать просто accounts. Это позволит в любое время изменить тип переменной без переименования самой переменной.
А еще ужаснее выглядит nameString, ageFloat. 
Исключение составляют наследники классов Android SDK: Activity, Fragment, View, Uri и т.д. По названию NewsSynsService сразу понятно, что класс является "сервисом" и ответственен за синхронизацию новостей. Использование суффикса view в nameView, photoView позволяет легко отличить переменные, относящиеся к верстки, от остальных. Имена view обычно начинают с существительного. Но имена кнопок лучше начинать с глагола: buyButton
6. Имена могут и должны содержать термины из математики, названия алгоритмов, паттернов проектирования и т.д. 
Увидев имя BitmapFactory, не автор кода сразу поймет смысл этого класса.
7. Не нужно указывать никакие префиксы при именовании.
Вместо m_user, mUser просто пишется user. Указывать префикс s для статических полей в современных IDE излишне. 
Исходники Android SDK не являются здесь показателем в силу давности создания первых версий и наследования кодовой базы до наших дней.
s_ ни к чему в начале названия статического поля. К тому же название констант должно писаться прописными буквами: 
8. В наименование классов нужно использовать существительные. 
Классы это как объекты реального мира. Поэтому нужно использовать существительные для их названия: AccountsFragment, User, Car, CarModel.
Не нужно называть классы Manager, Processor, Data, Info, т.к. они имеют слишком общее значение. Лучше название класса длиной в два-четыре слова, чем просто Data.
9. Названия классов должны начинаться с прописной буквы. 
Слова НЕ должны отделяться символом подчеркивания. Нужно следовать нотации CamelCase: GoodsFragment, BaseFragment
10. Используйте одно слово для каждой концепции. 
Использование fetch, retrieve, get в одном классе сбивает с толку. Если класс назвали Customer, то имена переменных класса и параметров функций этого типа лучше называть customer, а не user.
1. Функция должна выполнять только одну “операцию”. Она должна выполнять ее хорошо. И ничего другого она делать не должна. 
Под “операцией” следует понимать не одну математическую операцию или вызов другой функции, а действия на одном уровне абстракции. Чтобы определить, что функция выполняет более одной операции, следует попробовать извлечь из нее другую функцию. Если извлечение функции не дает ничего кроме простой перестановки кода, то значит разбивать дальше функцию не имеет смысла
К примеру есть функция setProducts класса ProductsAdapter:
Внутри функции происходит три основных операции: 1) фильтрация newProducts, 2) очистка и вставка новых значений в products, 3) обновление адаптера notifyDataSetChanged.
Фильтрация элементов newProducts должна происходить в другом методе и даже в другом в класса (например в presenter-е). В ProductsAdapter должны прийти уже отфильтрованные данные.
Лучше переделать код следующим образом:
Параметр newProducts содержит уже отфильтрованный список продуктов.
Можно еще строчки java products.clear(); products.addAll(newProducts);
вынести в отдельную функцию, но особого смысла я в этом не вижу. Лучше заменить notifyDataSetChanged на DiffUtil, это позволит обновлять ячейки в списке эффективнее
2. Размер функции должен быть 8-10 строк. 
Функция размером в 3 экрана это не функция, это *****. У меня тоже не всегда получается ограничить размер функции 8-10 строками кода, но нужно стремится к этому. Пример по понятным причинам приводить не буду.
Функции большого размера трудно читать, модифицировать и тестировать. Разбив большую функцию на малые, легче будет понять смысл основной функции, так как мелкие детали будут скрыты. Выделяя функции можно увидеть и избавиться от дублирования кода в основной функции. 
3. В теле функции все должно быть на одном уровне абстракции.

Вычисление значения локальной переменной errorMessage имеет более низкий уровень абстракции, чем остальной код внутри функции. Поэтому код java "Article " + article.getName() + " is incorrect"  лучше вынести в отдельную функцию.
4. Наименовании функции и передаваемых параметров должно сообщать о том, что делает функция 
Причиной выделения блока кода в отдельную функцию может являться желание в пояснении, что делает код. Для этого нужно дать подходящее название функции и параметрам функции.
Непонятно по какому полю будет происходить поиск, что передается на вход функции.
Лучше переделать в следующий вид:
Название функции говорит, что происходит поиск Product по полю id. На вход функция принимает не “null” значение. Если Product не найдется, то вернется “null”.
Роберт К. Мартин советует использовать параметры в качестве части названия функции:
Вызов функции может выглядеть так: 
На проектах я такой способ не встречал. Лучше данный способ не использовать и писать полностью название: 
5. Имена функций должны начинаться с глагола. Лучше длинное имя, чем не содержательное короткое.

Название start не сообщает, что именно стартует функция. Только заглянув в тело становится понятно, что функция открывает Activity. 
А должно быть понятно предназначение функции уже по названию.
6. Вместо передачи в аргументы функции флага (boolean) лучше разбить функцию на две функции
Часто этот флаг является причиной увеличение размера функции при ветвлении логики выполнения в зависимости от значения флага. В таких случаях следует подумать о разбиении данной функции на две. Разное поведение функции в зависимости от переданного флага не всегда очевидно.
7. Дублирующий код следует выносить в отдельную функцию.

Код внутри setOnClickListener отличается только стилем. Этот код стоит вынести в отдельный метод:
8. Не передавайте и не возвращайте из функции “null” когда можно вернуть пустой объект.

Это позволит избежать ошибок NullPointerexception и не нужно будет в местах вызова писать проверки на null.
Kotlin поможет отучиться от вредной привычки передавать и возвращать null.)
1. Код в классе должен читаться сверху-вниз как газетная статья в порядке убывания уровня абстракции. Вначале идут публичные функции, затем приватные.
Основная идея совета в том, что при открытии файла программист начинает просматривать его сверху. Если вначале разместить все публичные функции, то легче будет понять основные операции с объектами класса, ответственность класса и где может использоваться. Данный совет подходит, когда проект строится на интерфейсах.
2. Связанные концепции/функции следует размещать рядом, чтобы не было необходимости постоянно перемещаться по файлу вверх-вниз. Если одна функция вызывает другую, то вызываемая функция должна располагаться под вызывающей функцией (если это возможно) и разделяться пустой строкой.
Данный совет может противоречить предыдущему. Выбрав приоритетный совет для команды, стоит придерживаться его на всем проекте.
3. Каждая последовательная группа строк кода должна представлять одну законченную мысль. Мысли отделяются друг от друга в коде с помощью пустых строк, только слишком злоупотреблять пустыми строчками не стоит
Связанная группа строк это как абзацы в статье. Для разделения абзацев используется красная строка или пустая строка. Так и в коде следует разделять разные по смыслу группы строк, используя пустые строки.
4. Переменные экземпляра класса, статические константы должны быть все в начале класса в одном месте.
В начале класса объявляются публичные константы, затем приватные константы и после них идут приватные переменные. 
Правильное объявление:
5. Используйте пробелы для повышения читаемости кода

6. "Магические цифры, строки" должны быть вынесены в константы !!!
Пожалуй, самый популярный совет, но все равно продолжают его нарушать.
Лишь прочитав постановку задачи я понял, для чего в коде использовалось число “10”. Лучше это число вынести в константу и дать осмысленное имя.
Строки тоже стоит выносить в константы, особенно если используются в более чем одном месте.
1. Класс должен иметь одну “ответственность”, одну причину для изменения. 
К примеру, наследники класса RecyclerView.Adapter должны отвечать за создание и связывание View с данным. В нем не должен находится код сортировки/фильтрации списка элементов.
Часто в файл с Activity добавляют класс RecyclerView.Adapter, что является не правильным.
2. Не нужны пустые методы или просто вызывающий метод из родительского класса
3. Если переопределяете какой-то метод без вызова метода родительского, то проверьте, что так можно делать.
Загляните в исходники родительских классов, документации. Переопределяемые методы жизненного цикла Activity, Fragment, View должны обязательно должны вызывать методы родительского класса.
Есть аннотация @CallSuper, предупреждающая о необходимости вызывать родительский метод при переопределении.
1. Используйте средства Android Studio для улучшение качества Вашего кода.
Если она подчеркивает или выделяет код, значит что-то может быть не так. В Android Studio есть средства Rearrange, Reformat, Extract Method, Inline Method, анализаторы кода, горячие клавиши и т.д.
2. Не нужно комментировать каждый метод, код должен быть самодокументированным.
Следует отметить, что комментарии должны пояснять намерения и причины, а не поведение кода. Создавая комментарий, необходимо брать на себя ответственность о поддержании комментария в актуальном состоянии. 
3. Строки, цвета, размеры должны быть в ресурсах (xml)
Не нужно писать:
Исключения составляют файлы из папок test, mock. Такие файлы не должны попадать в релизную версию
4. Не нужно передавать context в RecyclerView.Adapter. context можно получить из любой view. Элемент по клику в RecyclerView.Adapter должен меняться через notifyItemChanged, а не непосредственным изменением вьюшек в методе обработки нажатия

В методе onClick непосредственно происходит вызов bindViewHolder, что НЕ правильно.
5. В RecyclerView.ViewHolder вместо определения позиции объекта в списке нужно вызывать getAdapterPosition, а не передавать position из метода onBindViewHolder

Правильно писать:
Либо можно вместо позиции передавать ссылку на исходный объект с данными
6. Используйте Quantity Strings (Plurals)  для множественного числа. Вместо написания в коде логики по подбору правильного окончания слов.

7. Не сохраняйте большие данные Bitmap в bundle. Размер bundle ограничен

8. Если прописать “id” в xml, то Android сам восстановит состояние стандартных View при повороте.

Пользователь
=====
Создание приложения на Ionic с использованием API
2017-12-12, 22:39
User
=====
Упущенные возможности неинтегрированных коммуникаций
2017-12-12, 10:45
коммуникационные проекты
=====
Еще одна реализация регистронезависимого поиска по кириллическим символам в SQLite
2017-12-12, 09:52
Инженер по метрологии
=====
Идеальный мавен. Часть 2: структура проекта
2017-12-12, 10:01
Это вторая статья, посвященная мавену и его использованию для организации моих проектов. Целью этой статьи будет структура проекта под управлением мавен. Особых откровений вы не найдёте, скорее набор общих правил для моих проектов. Первую статью можно прочитать здесь.
Как я уже сказал, я не буду описывать здесь типовую структуру проекта на мавен – вы её знаете или легко можете найти по ссылке: Standard Directory Layout. В этой части я остановлюсь на особенностях, которые я применяю в своих проектах, итак:
Практически любой мой проект имеет несколько модулей. Практика показывает, что, когда необходимо добавить еще один модуль в проект это гораздо проще сделать в случае изначально модульного проекта. В моих проектах существует 3 типа «модулей» – POM (всегда один), несколько девелоперских модулей с кодом и BOM – если дев. модулей больше одного. В принципе POM это не модуль в понимании мавен, но я всегда оформляю его почти как «модуль» (покажу ниже). В итоге получается, что-то вроде такого:

Начнём с проектного POM‘а. Я практически всегда убираю его в отдельный каталог с именем pom. Делаю я так по нескольким причинам:
Проектный POM содержит ссылку на супер POM, список модулей, версии проектов от которых он зависит (не библиотек третьих стран, а именно проектов, которые находятся параллельно в разработке в этой компании) и определение версии для модулей и проектов (dependencyManagement). Вот типичный POM для маленького проекта:
В этом примере:
версия проекта, от которого он зависит
и определение версии для модулей и проектов
Зачем это нужно? Что бы полностью исключить использование версий в девелоперских модулях. С такой конфигурацией все версии фиксируются в супер POM‘е (для внешних библиотек) и в проектных POM‘ах (для самого проекта и его зависимостей на внутренние проекты). Это не только делает POM‘ы в модулях чище, но и необходимо для того релиз процесса, который я использую. 
Это «bill of materials» — список артефактов, которые экспортирует проект. В моих проектах он появляется если количество девелоперских модулей больше одного. Это позволяет управлять версиями зависимостей между внутренними проектами с помощью только одной записи в секции dependencyManagement.
Пример BOM:
Самый примитивный POM. Включает в себя ссылку на проектный POM, artefactId, список зависимостей без версий. При необходимости секцию build c ссылками на плагины. Версия и groupId наследуются из родительского POM’а.
Пример:
groupId это имя пакета в Java для этого проекта – в последнее время это стало практически стандартом. С artifactId – немного чудесатей, для своих проектов я всегда беру имя группы плюс имя модуля, что-то вроде этого:
Почему? Имя у итогового артефакта должно быть уникальным (слишком часто все сваливают в один каталог), саму идею я вынес и мира eclipse — так там именуются плагины. Поначалу непривычно, но оно достаточно уникально, придумать его очень просто, увидев в каталоге по имени очень быстро можно найти артефакт в репозитории и source control‘е.
Почему не использовать finalName в супер POM‘е (e.g. <finalName>${ groupId}.${artifactId}-${version}</finalName>)? Так сложилось исторически, на заре мавенизации очень много плагинов игнорировало правила, и проще было написать это руками. Сейчас, наверное, это не так.
Следовать этой конвенции с именами не обязательно. Главное, это уникальность имени итогового артефакта.
По структуре проектов это, наверное, все.
Человек
=====
Intel AI Academy — новогодний подарок для всех разработчиков AI
2017-12-25, 09:31
Пользователь
=====
Клик, трейд, депозит. Наши онлайн-инструменты для корпоративных клиентов
2017-12-13, 12:19
Организация
=====
Parcel — очень быстрый бандлер, не требующий настройки
2017-12-12, 13:57

Parcel — маленький и быстрый бандлер, позиционируется как решение для маленьких проектов. С момента первого релиза (7 дней назад) уже собрал 8725 звездочек на гитхабе. Согласно официальной документации имеет следующие плюсы:
Быстрая сборка
Parcel использует worker process для многопоточной сборки, а так же имеет свой файловый кэш для быстрой пересборки при последующих изменениях.
Собирает все ваши ассеты
Из коробки имеется поддержка ES6, TypeScript, CoffeeScript, HTML, SCSS, Stylus, raw-файлов. Плагины не требуются.
Автоматические преобразования
Весь код автоматически проходит через Babel, PostCSS, PostHTML — подхватываются при необходимости из node_modules.
️ Разделение кода без лишней конфигурации
Используя динамический import(), Parcel разделяет бандл для возможности быстрой начальной загрузки точки входа в приложение
Горячая перезагрузка
Типичный хот-релоад без конфигурации — сохраняете изменения и они автоматически применяются в браузере.
Дружелюбный вывод ошибок
При ошибке подсвечивается кусок кода, в котором она произошла.
Так же на главной странице приводится бенчмарк:
Подход Parcel схож с оным у Webpack (тут сложно придумать что-то новое).
У нас есть сущность — Asset. Ассет — это любой файл. Механика работы такова: реализуется интерфейс, который предоставляет логику для превращения файла в AST, разрешения всех зависимостей, применения нужных трансформаций и генерирования итогового кода. Если вас не устраивает работа какого-то ассета из коробки или вы хотите добавить свой — нет ничего сложного.
Дальше в дело вступает Packager. Упаковщик склеивает ассеты в итоговый бандл. Это происходит после обработки и успешного построения дерева. Упаковщики регистрируются на основе типа файлов. Хотите написать свой упаковщик? Вам сюда.
Так же мы можем писать свои плагины, которые Parcel будет подхватывать из package.json. Для этого у названия пакета плагина должен быть префикс parcel-plugin-. Но это уже совсем частный случай, который скорее всего уже ведет к тому, что надо переключаться на webpack или другой удобный инструмент.
Ставим пакет, инициализируем приложение через любой пакетный менеджер:
Для примера напишем hello world на Preact. Создадим следующую структуру:
А так же установим необходимые пакеты:
Для того, чтобы сконфигурировать Babel создадим .babelrc со следующим содержанием:
Для PostCSS:
Для autoprefixer:
index.html
App.jsx
Clock.jsx
Clock.css

И это все. Как можно заметить, мы не потратили ни минуты на написание конфигурационных файлов, за исключением .babelrc и .postcssrc
Перед нами эдакий "Webpack на минималках", предоставляющий возможность быстрого развертывания рабочего окружения для небольшого проекта. Стек технологий по сути ограничен лишь стандартным набором ассетов, но в любой момент его можно расширить и своими собственными. С учетом полной поддержки Babel мы легко можем использовать практически любой другой фреймворк или библиотеку (разве что с Angular будут сложности, ведь писать с его помощью на ES6 и без родного инструментария — задача на любителя), а поддержка PostCSS из коробки является еще одним приятным дополнением.
Из неудобств я пока что могу отметить только одно — при работе с TypeScript бандлер не учитывает пользовательские пути и базовый каталог (секции baseUrl и paths), указанные в файле tsconfig, и, соответственно, не может нормально разрешать пути импортируемых модулей. На гитхабе идет обсуждение решения этой проблемы.
Frontend-разработчик
=====
Мониторинг серверов Trassir
2017-12-12, 11:41
User
=====
Загоним мамонта в яму: как провести презентацию, чтобы вас услышали и запомнили
2017-12-12, 13:57
Пользователь
=====
Zabbix 3.4: Макросы в интервалах времени
2017-12-12, 12:48
Привет. Продолжаем освещать нововведения Zabbix 3.4. Сегодня поговорим об использовании макросов в интервалах обновления и других временных периодах.

Пользовательские макросы – давно зарекомендовавший себя механизм, используемый в Zabbix повсеместно и дающий системе мониторинга необходимую ей гибкость. По сути это переменные, которые вы можете назначать с глобальным уровнем видимости, шаблона или узла сети. Использование макросов всячески приветствуется и рекомендуется, например в шаблонах, что делает их настраиваемыми в других окружениях и другими пользователями.
Выглядят пользовательские макросы следующим образом, вы их наверняка уже встречали:
Zabbix позволяет гибко настраивать время опроса метрик: у каждой метрики может быть свой собственный интервал.

Обновления каждой метрики также могут быть "гибкими"(см. Пользовательские интервалы), а значит происходить по определенному расписанию ("раз в сутки ночью" или "в 9:00 утра в будни").
Аналогичным образом мы можем определить время хранения истории и трендов для каждого элемента данных отдельно.
Подобная тонкость настройки нужна далеко не всегда, поэтому использование макросов дает пару новых идей по настройке этих параметров.
Во-первых, интервалы обновления метрик (как обычные, так и пользовательские интервалы), о которых сказано выше, теперь поддерживают пользовательские макросы. Во-вторых, использовать макросы можно и в интервалах хранения истории и трендов. В итоге это выглядит вот так:

Просто задайте значения этих макросов глобально, а потом переназначайте на уровне шаблона или узла сети, если требуется:

В общем, для интервалов обновлений можно создать небольшой глобальный набор макросов, которые затем использовать по умолчанию для всех новых элементов данных, в зависимости от их типа и важности. Например:

Это позволит не тратить каждый раз время на обдумывание "я хочу собирать эту метрику раз в 60 или раз в 61 секунду? или может раз в 5 минут будет достаточно?", а просто использовать принятые на вашем сервере и проекте правила по сбору и хранению элементов данных, зафиксированные в глобальных макросах. Хотя, возможно, такой вариант подойдет не всем :)
Поддерживается и контекст макросов, что может быть очень полезно, например, при LLD.
Представьте, что мы собираем трафик сетевых интерфейсов на множестве устройств. Чтобы не нагружать Zabbix, мы бы хотели сделать следующим образом:
Для начала определим глобальные макросы {$DELAY_IF}, {$HISTORY_IF}, {$TREND_IF}:

Затем используем их в прототипе элемента данных интерфейса, но уже с контекстом (в данном случае это будет имя интерфейса ifName):

Уже на уровне узла сети укажем новое значение макроса с контекстом для ключевого интерфейса (для примера возьмем Gi0/0.114):

Теперь посмотрим частоту обновления и время хранения для различных интерфейсов в "Последних данных". Как видно, у нашего очень важного Gi0/0.114 теперь свои правила хранения и сбора:

Если же мы захотим изменить общий интервал или увеличить частоту опроса или времени хранения еще одного интерфейса — нам нужно будет просто переназначить макросы на уровне хоста. Изменять шаблон, прототип и ждать обнаружения не потребуется — все применится сразу. На самом деле, даже доступ на запись к шаблону не требуется.
А еще макросы теперь можно применять в других ситуациях, где нужно было указывать время или период. Например, в действиях:

или указать через макрос время доступности инженера для автоматических уведомлений:

С точным списком мест, в которых возможно применение макросов, можно ознакомиться здесь.
Новые возможности макросов в 3.4 открывают парочку неплохих возможностей: с одной стороны — для более тонкой настройки (для LLD), а с другой стороны — для централизации и управления временем опроса и хранения. И кстати, в интервалах времени появилась поддержка суффиксов s,m,h,d,w — мелочь, а удобно :)
До встречи!
P.S. Статья также доступна в нашем блоге на английском языке.
User
=====
Не поддавайтесь хайпу, или почему цена биткоина не отражает его реальной ценности
2017-12-12, 13:57
Пользователь
=====
Новая спайварь StrongPity2 сменила FinFisher в кампании кибершпионажа с возможным участием интернет-провайдера
2017-12-12, 14:07
User
=====
Управление зависимостями в PHP
2017-12-12, 16:42
При создании PHP-приложения или библиотеки обычно у вас есть три вида зависимостей:
Как управлять этими зависимостями?
Жёсткие зависимости:
Опциональные зависимости:
Зависимости опциональные и связанные с разработкой:
И так далее. Что может случиться плохого? Всё дело в ограничениях, присущих require-dev.
Зависимости с менеджером пакетов — это прекрасно. Это замечательный механизм для повторного использования кода и лёгкого обновления. Но вы отвечаете за то, какие зависимости и как вы включаете. Вы вносите код, который может содержать ошибки или уязвимости. Вы начинаете зависеть от того, что написано кем-то другим и чем вы можете даже не управлять. Не говоря уж о том, что вы рискуете стать жертвой сторонних проблем. Packagist и GitHub позволяют очень сильно снизить такие риски, но не избавляют от них совсем. Фиаско с left-pad в JavaScript-сообществе — хороший пример ситуации, когда всё может пойти наперекосяк, так что добавление пакетов иногда приводит к неприятным последствиям.
Второй недостаток зависимостей заключается в том, что они должны быть совместимы. Это задача для Composer. Но как бы ни был хорош Composer, встречаются зависимости, которые нельзя использовать совместно, и чем больше вы добавляете зависимостей, тем вероятнее возникновение конфликта.
Резюме
Выбирайте зависимости с умом и старайтесь ограничить их количество.
Рассмотрим пример:
Эти два пакета — инструменты статичного анализа, при совместной установке они иногда конфликтуют, поскольку могут зависеть от разных и несовместимых версий PHP-Parser.
Это вариант «глупого» конфликта: он возникает лишь тогда, когда пытаешься включить зависимость, несовместимую с твоим приложением. Пакеты не обязаны быть совместимыми, ваше приложение не использует их напрямую, к тому же они не исполняют ваш код.
Вот другой пример, когда библиотеке предоставляются мосты для работы с Symfony и Laravel. Возможно, чтобы протестировать мосты, вы захотите включить в качестве зависимостей и Symfony, и Laravel:
В каких-то случаях это может прекрасно работать, но, скорее всего, будет ломаться. Пример несколько притянутый за уши, потому что очень маловероятно, что какой-то пользователь станет использовать оба пакета одновременно, и ещё менее вероятно, что вам понадобится предусмотреть этот сценарий.
Посмотрите на этот composer.json:
Здесь кое-что происходит… Можно будет установить компонент Symfony YAML (пакет symfony/yaml) только версий [3.0.0, 4.0.0[.
В приложении вам наверняка не будет до этого дела. А вот в библиотеке это может привести к проблеме, потому что у вас никогда не получится протестировать свою библиотеку с symfony/yaml [2.8.0, 3.0.0[.
Станет ли это настоящей проблемой — во многом зависит от конкретной ситуации. Нужно иметь в виду, что подобное ограничение может встать поперёк, и выявить это будет не так просто. Показан простой пример, но если требование symfony/yaml: ^3.0 спрятать поглубже в дерево зависимостей, например:
вы об этом никак не узнаете, по крайней мере сейчас.
KISS. Всё нормально, на самом деле вам этот пакет не нужен!
PHAR’ы (PHP-архивы) — способ упаковки приложения в один файл. Подробнее можно почитать об этом в официальной PHP-документации.
Пример использования с PhpMetrics, инструментом статичного анализа:
Внимание: упакованный в PHAR код не изолируется, в отличие от, например, JAR’ов в Java.
Наглядно проиллюстрируем проблему. Вы сделали консольное приложение myapp.phar, полагающееся на Symfony YAML 2.8.0, который исполняет PHP-скрипт:
Ваш скрипт myscript.php применяет Composer для использования Symfony YAML 4.0.0.
Что может случиться, если PHAR загружает класс Symfony YAML, например Symfony\Yaml\Yaml, а потом исполняет ваш скрипт? Он тоже использует Symfony\Yaml\Yaml, но ведь класс уже загружен! Причём загружен из пакета symfony/yaml 2.8.0, а не из 4.0.0, как нужно вашему скрипту. И если API различаются, всё ломается напрочь.
Резюме
PHAR’ы замечательно подходят для инструментов статичного анализа вроде PhpStan или PhpMetrics, но ненадёжны (как минимум сейчас), поскольку исполняют код в зависимости от коллизий зависимостей (на данный момент!).
Нужно помнить о PHAR’ах ещё кое-что:
Одна из самых популярных методик. Вместо того чтобы требовать все зависимости мостов в одном файле composer.json, мы делим пакет по нескольким репозиториям.
Возьмём предыдущий пример с библиотекой. Назовём её acme/foo, затем создадим пакеты acme/foo-bundle для Symfony и acme/foo-provider для Laravel.
Обратите внимание, мы пока ещё можем всё поместить в один репозиторий, а для других пакетов вроде Symfony использовать репозитории только для чтения.
Главное преимущество этого подхода в том, что он относительно прост и не требует дополнительных инструментов, за исключением разделителя по репозиториям вроде splitsh, используемого для Symfony, Laravel и PhpBB. А недостаток в том, что теперь вместо одного пакета вам нужно поддерживать несколько.
Можно пойти другим путём и выбрать более продвинутый скрипт установки и тестирования. Для нашего предыдущего примера можно использовать подобное:
Работать будет, но, по моему опыту, тестовые скрипты получатся раздутыми и относительно медленными, трудными в поддержке и не слишком простыми в понимании сторонними программистами.
Этот подход довольно свежий (в PHP), в основном потому, что раньше не было нужных инструментов, так что я расскажу чуть подробнее.
Идея проста. Вместо
мы установим phpstan/phpstan и phpmetrics/phpmetrics в разные файлы composer.json. Но тут возникает первая сложность: куда их класть? Какую создавать структуру?
Здесь поможет composer-bin-plugin. Это очень простой плагин для Composer, позволяющий взаимодействовать с composer.json в разных папках. Допустим, есть корневой файл composer.json:
Установим плагин:
После этого, если выполнить composer bin acme smth, то команда composer smth будет выполнена в поддиректории vendor-bin/acme. Теперь установим PhpStan и PhpMetrics:
Будет создана такая структура директорий:
Здесь vendor-bin/phpstan/composer.json выглядит так:
А vendor-bin/phpmetrics/composer.json выглядит так:
Теперь можно использовать PhpStan и PhpMetrics, просто вызвав vendor-bin/phpstan/vendor/bin/phpstan и vendor-bin/phpmetrics/vendor/bin/phpstan.
Пойдём дальше. Возьмём пример с библиотекой с мостами для разных фреймворков:
Применим тот же подход и получим файл vendor-bin/symfony/composer.json для моста Symfony:
И файл vendor-bin/laravel/composer.json для моста Laravel:
Наш корневой файл composer.json будет выглядеть так:
Для тестирования основной библиотеки и мостов теперь нужно создать три разных PHPUnit-файла, каждый с соответствующим файлом автозагрузки (например, vendor-bin/symfony/vendor/autoload.php для моста Symfony).
Если вы попробуете сами, то заметите главный недостаток подхода: избыточность конфигурирования. Вам придётся дублировать конфигурацию корневого composer.json в другие два vendor-bin/{symfony,laravel/composer.json, настраивать разделы autoload, поскольку пути к файлам могут измениться, и когда вам потребуется новая зависимость, то придётся прописать её и в других файлах composer.json. Получается неудобно, но на помощь приходит плагин composer-inheritance-plugin.
Это маленькая обёртка вокруг composer-merge-plugin, позволяющая объединять контент vendor-bin/symfony/composer.json с корневым composer.json. Вместо
получится
Сюда будет включена оставшаяся часть конфигурации, автозагрузки и зависимостей корневого composer.json. Ничего конфигурировать не нужно, composer-inheritance-plugin — лишь тонкая обёртка вокруг composer-merge-plugin для предварительного конфигурирования, чтобы можно было использовать с composer-bin-plugin.
Если хотите, можете изучить установленные зависимости с помощью
Я применял этот подход в разных проектах, например в alice, для разных инструментов вроде PhpStan или PHP-CS-Fixer и мостов для фреймворков. Другой пример — alice-data-fixtures, где используется много разных ORM-мостов для уровня хранения данных (Doctrine ORM, Doctrine ODM, Eloquent ORM и т. д.) и интеграций фреймворков.
Также я применял этот подход в нескольких частных проектах как альтернативу PHAR’ам в приложениях с разными инструментами, и он прекрасно себя зарекомендовал.
Уверен, кто-то сочтёт некоторые методики странными или нерекомендуемыми. Я не собирался давать оценку или советовать что-то конкретное, а хотел лишь описать возможные способы управления зависимостями, их достоинства и недостатки. Выберите, что вам больше подходит, ориентируясь на свои задачи и личные предпочтения. Как кто-то сказал, не существует решений, есть только компромиссы.
¯\_(ツ)_/¯
=====
Основы TypeScript, необходимые для разработки Angular-приложений
2017-12-12, 14:08
Пользователь
=====
Avito iOS Winter Edition — видео, фото, слайды, отзывы
2017-12-12, 14:13
Прошёл четвёртый по счёту традиционный iOS Meetup в Avito. Мы обсуждали Data Driven подход к разработке, практическое применение Mach-O, lldb и dSYM, возможности расширения lldb, методологию Type Driven и концептуальные различия архитектур. Если же вы не успели к нам на доклады и глинтвейн, или просто были не в Москве, то под катом мы выложили видеозаписи выступлений, слайды от докладчиков и немного фидбэка от участников встречи.

Доклад о Data-driven подходе к разработке. Какие метрики можно собирать? Как они помогут быть эффективным? И как следить за качеством разрабатываемой функциональности? Посмотрите видео и узнаете, как замерять время компиляции отдельных фреймворков, размер приложения, время запуска приложения, CrashFree, OOM. Метрики — не только для менеджеров и аналитиков!
Отзывы 
Пишем код без багов, быстро отлаживаем приложения, пользуясь совершенными инструментами. Посмотрите доклад о том, как прокачать lldb при помощи расширений на Python и сделать отладку приятнее и быстрее.
Отзывы 
Полезный практический доклад, в котором речь идёт о бинарном формате исполняемых файлов Mach-O, об отладочной информации и объектных файлах. 
Отзывы 
Ещё один рубеж обороны надежного приложения от ошибок разработчика! Рассматриваем строгую типизацию: на примерах показано, как дополнительная информация, переданная на этапе компиляции, поможет отловить ряд ошибок, не доводя систему до падения в runtime. Кроме этого, Валерий рассказал, что мобильный девелопер может почерпнуть из языков, которые ставят типы во главе процесса разработки.
Отзывы 
Разбираемся, чем концептуально отличаются архитектуры. Отвечаем на вопрос, почему появляется по архитектуре в неделю и почему в них нет ничего нового. И узнаём, на что нужно будет обратить внимание при выборе архитектуры следующего приложения.
Отзывы 
Спасибо всем, кто пришёл на митап!
Смотрите фотоотчёт, а чтобы раньше всех узнавать о мероприятиях для разработчиков в Avito, подписывайтесь на наш Timepad. И обязательно расскажите в комментариях, на какие темы вам бы хотелось послушать доклады!
User
=====
Синглтон, локатор сервисов и тесты в iOS
2017-12-14, 15:20
iOS Разработчик
=====
Внедрение зависимости и реализация единицы работы с помощью Castle Windsor и NHibernate
2017-12-12, 14:35
Разработчик
=====
Как работает «МЕГА Белая Дача»: открываем ТЦ с другой стороны
2017-12-12, 16:17
Пользователь
=====
Приглашаем на итоги конкурса по анализу данных
2017-12-12, 14:44
Пользователь
=====
Запускаем django-приложение в Docker на Vagrant под Windows
2017-12-13, 06:34
User
=====
Extended Validation не работает
2017-12-12, 15:09
Информационная безопасность
=====
Приглашаем на Рождественский Agile MeetUp
2017-12-12, 16:35
Пользователь
=====
Туториал по Unreal Engine. Часть 3: материалы
2017-12-15, 11:51
Переводчик-фрилансер
=====
Разбор кейса о верификации резервных копий (SureBackup) и использовании vSAN
2017-12-13, 12:47
Пользователь
=====
Разработка приложений на языках C/C++ с использованием Tcl/Tk
2017-12-12, 20:06
Программист
=====
Как сегментировать пользователей для разных вертикалей
2017-12-12, 17:12
Пользователь
=====
cBackup — резервное копирование конфигураций сетевого оборудования
2018-01-22, 12:20
Администратор, программист
=====
Ускорение сборки C и C++ проектов
2017-12-12, 17:44








Программист
=====
Конструирование сайта, защищенного от блокировок
2017-12-12, 19:07
Привет всем! В связи с ростом блокировок, в том числе необоснованных, сайтов со стороны государства, вашему вниманию предлагается описание идеи, а также прототип настроек сайта, защищенного от блокировок по конкретному пути и доменному имени. Идеи по защите от блокировок:
будут изложены в других постах. Кому интересна тема, заходите под кат.
https://github.com/http-abs/http-abs/
Принцип защиты заключается в том, что каждый пользователь получает уникальную пару из индивидуального (под)домена и префикса пути для просмотра сайта. Назовем эту пару идентификатором агента.
Если по каким-то причинам, вы можете управлять только ограниченным набором поддоменов, пользователь будет конечно, разделять свой поддомен с некоторыми другими пользователями.
Конечно, в случае ограничения, указанного выше, жизнь станет сложнее у тех пользователей, которым не повезло разделить поддомен с оператором блокировок. Но не сильно.
При условии предыдущего примечания.
Легко. Идентификатор агента, выделенный пользователю для просмотра сайта, сохраняется у него в куках. Пользователь просто делится ссылкой из адресной строки.
Чтобы изолировать оператора блокировок. Если повезет, после нескольких блокировок, его можно будет даже идентифицировать.
Ничуть, от него не требуется в буквальном смысле ничего. Он заходит на сайт обычным образом, читает материалы и делится ссылками. В его жизни меняется только вид адресной строки.
Возможно. Ему не придется учитывать префикс пути и поддомен, входящие в идентификатор агента: их будет обрезать обработчик на первой стадии обработки входящего URL. Однако, чтобы сделать процесс совершенно прозрачным, возможно придется еще потрудиться.
Конечно. Но ему будет гораздо труднее доказывать необходимость блокировки бесконечного количества ссылок на материал, сформированных по замысловатому правилу, при том, что правила могут и измениться на еще более замысловатые.
Такое развитие событий не исключено, но эти случаи я хочу рассмотреть отдельно в другой статье. Возможно, мы с вами вместе придумаем что-то дополнительно в комментариях.
Для того, чтобы сделать процесс максимально прозрачным, код сосредоточен в сервере фронтенда nginx. Это позволяет ставить под защиту самые разнообразные прикладные серверы практически без ограничений.
Поскольку обработка запроса будет весьма нетривиальной, использован дополнительный пакет libnginx-mod-http-lua, внедряющий язык lua в процесс обработки запросов под nginx.
В идеале, обработка должна производиться так, чтобы бакенд (аплинк, прикладной сервер) — вовсе не был озабочен тем, поставили ли его под защиту. Ему приходят запросы по URL, из которого убраны все элементы идентификатора агента (назовем такие чистыми URL). Чтобы не переделывать возвращаемые страницы, переход по чистому URL с установленной кукой идентификатора агента, вызывает редирект на индивидуальный URL.
Фронтенд не хранит состояние нигде, кроме куки идентификатора агента.
На браузере не задействовано ни одной строки кода javascript. Используется только протокол HTTP.
Фактически, реализован лишь proof-of-concept, позволяющий наблюдать реальную работу алгоритма. Не решено множество деталей, связанных с упаковкой продукта: модульность, выделение и проверка параметров и так далее.
Для поддоменов, выбрана схема с фиксированным набором поддоменов, пригодная для использования в паре с файлом hosts, без инсталляции дополнительного сервера DNS.
Формат префикса пути предопределен и представляет из себя 32 цифры по основанию 16.
Параметры запуска установлены пока прямо в коде.
Набор поддоменов (a, b, c) установлен в переменной и может быть расширен.
Домен установлен, как example.com.
Бакенд ожидается на точке http://127.0.0.1:8000
Растущая угроза внезапных блокировок заставляет готовиться заранее и изобретать методы защиты от них владельцев даже вполне лояльных сайтов. Такая защита вполне возможна, не требует никаких телодвижений со стороны пользователя и может быть реализована вполне скромным объемом усилий со стороны администратора сайта.
User
=====
Как я осознал, что такое распределенные системы
2017-12-12, 17:49
User
=====
Вы все еще кодите на Java? Пора измениться
2017-12-12, 21:20
Андроид разработчик / Фрилансер
=====
Человеческий фактор в информационной безопасности
2017-12-12, 19:50
Пользователь
=====
Чем страдает бизнес?
2017-12-13, 10:11
Разработчик
=====
Zabbix: LLD-мониторинг дисков без UserParameter и скриптов на агентах
2017-12-14, 11:32
Даю советы как не надо было делать
=====
История победы на международном соревновании по распознаванию документов команды компании SmartEngines
2017-12-14, 11:32
Привет, Хабр! Сегодня мы расскажем о том, как нашей команде из Smart Engines удалось победить на международном конкурсе по бинаризации документов DIBCO17, проводимом в рамках конференции ICDAR. Данный конкурс проводится регулярно и уже имеет солидную историю (он проводится 9 лет), за время которой было предложено множество невероятно интересных и безумных (в хорошем смысле) алгоритмов бинаризации. Несмотря на то, что в своих проектах по распознаванию документов при помощи мобильных устройств мы по возможности не используем подобные алгоритмы, команде показалось, что нам есть что предложить мировому сообществу, и в этом году мы впервые приняли решение участвовать в конкурсе.
Для начала расскажем вкратце в чем состоит его суть: дано подготовленное организаторами множество цветных изображений документов S (пример одного из таких изображений приведен на рисунке слева) и множество соответствующих им идеальных (с точки зрения все тех же организаторов) бинарных образов I (ground truth, ожидаемый результат для примера приведен на рисунке справа). Требуется построить алгоритм A, переводящий исходные изображения из S в двухуровневые черно-белые a(A) (т.е. решить задачу классификации каждого пикселя как принадлежащего объекту или фону), таким образом, чтобы эти результирующие изображения оказались как можно “ближе” к соответствующим идеальным из I. Множество метрик, по которым оценивается эта близость, конечно же, зафиксировано в условиях конкурса. Особенностью данного конкурса является то, что ни одно тестовое изображение конкурсантам заранее не предоставляется, для настройки и подготовки доступны данные с прошедших контестов. При этом, новые данные каждый раз содержат свою собственную “изюминку”, отличающую их от предыдущих конкурсов (например, наличие тонких “акварельных” текстовых начертаний или просвечивающиеся с противоположной стороны символы) и предъявляющие новые вызовы для участников. Конкурс регулярно собирает порядка двух-трех десятков участников со всего мира. Далее приводится описание нашего конкурсного решения.

Первым делом были собраны данные со всех предыдущих конкурсов. Всего удалось загрузить 65 изображений рукописных документов и 21 изображение печатных. Очевидно, что для достижения высоких результатов необходимо было смотреть на проблему более широким взглядом, поэтому помимо анализа изображений от организаторов, своими силами был осуществлен поиск открытых датасетов с архивными печатными и рукописными документами. Организаторами не запрещалось использование сторонних датасетов. Было успешно найдено несколько тысяч изображений, по своей природе подходящих под условия конкурса (данные с различных тематических конкурсов, организованных ICDAR, проект READ и другие). После изучения и классификации этих документов стало понятно с какими классами проблем мы можем столкнуться в принципе, и какие из них оставались до сих пор проигнорированы организаторами конкурса. Например, ни в одном из предшествующих конкурсов документы не содержали никаких элементов разграфки, хотя таблицы нередко встречаются в архивах.
При подготовке к конкурсу мы шли параллельно несколькими путями. Помимо классических алгоритмических подходов, хорошо исследованных нами ранее, было решено опробовать методы машинного обучения для пиксельной классификации объект-фон, несмотря на столь незначительное множество исходно предоставленных данных. Поскольку в конце концов именно этот подход оказался наиболее эффективным, о нем мы и расскажем.
В качестве первоначального варианта была выбрана нейронная сеть архитектуры U-net. Подобная архитектура хорошо зарекомендовала себя при решении задач сегментации в различных соревнованиях (например 1, 2, 3). Важным соображением было и то, что большой класс известных алгоритмов бинаризации явно выразим в такой архитектуре или подобной архитектуре (в качестве примера можно взять модификацию алгоритма Ниблэка с заменой среднеквадратичного отклонения на средний модуль отклонения, в этом случае сеть строится особенно просто).
Пример нейронной сети U-net архитектуры
Преимущество же такой архитектуры состоит в том, что для обучения сети можно создать достаточное количество обучающих данных из небольшого числа исходных изображений. При этом сеть имеет сравнительно малое число весов за счет своей сверточной архитектуры. Но есть и свои нюансы. В частности, используемая искусственная нейронная сеть, строго говоря, не решает задачу бинаризации: каждому пикселю исходного изображения она ставит в соответствие некоторое число от 0 до 1, которое характеризует степень принадлежности данного пикселя к одному из классов (содержательное заполнение или фон) и которое необходимо еще преобразовать в финальный бинарный ответ.
В качестве обучающей выборки было взято 80% исходных изображений. Остальные 20% изображений были выделены на валидацию и тестирование. Изображения из цветных были сконвертированы в градации серого, для избежания переобучения, после чего все они были нарезаны на неперекрывающиеся окна размером 128x128 пикселей. Оптимальный размер окна подбирался эмпирически (пробовались окна от 16x16 до 512x512). Первоначально не применялось никаких методов аугментации и таким образом из сотни первоначальных изображений было получено порядка 70 тысяч окон, которые подавались на вход нейронной сети. Каждому такому окну изображения была поставлена в соответствие бинарная маска, вырезанная из разметки.
Примеры окон
Настройка параметров нейронной сети, процесса обучения и аугментации данных происходили в ручном режиме, поскольку время каждого эксперимента (аугментация данных, обучение / дообучение сети, валидация и тестирование решения) составляло несколько часов, да и принцип работы “внимательно вглядываться и понимать, что происходит” по нашему мнению более предпочтителен, чем запускать hyperopt на неделю. В качестве метода стохастической оптимизации был выбран Adam. Кросс-энтропия была использована в качестве метрики для функции потерь.
Уже первые эксперименты показали, что подобный подход позволяет достичь качества выше, чем простейшие необучаемые методы (типа Отцу или Ниблэка). Нейронная сеть хорошо обучалась и процесс обучения быстро сходился к приемлемому минимуму. Далее приведено несколько примеров анимации процесса обучения сети. Первые два изображения взяты из оригинальных датасетов, третье найдено в одном из архивов.
Каждая из анимаций была получена следующим образом: в процессе обучения нейронной сети, по мере улучшения качества, через сеть прогоняется одно и то же исходное изображение. Полученные результаты работы сети склеиваются в одну gif анимацию.
Исходное изображение рукописного текста со сложным фоном
Результат бинаризации по мере обучения сети
Сложность бинаризации вышеприведённого примера в том, чтобы отличить неоднородный фон от витиеватого почерка. Часть букв размыта, с оборота проступает текст с другой страницы, кляксы. Тот, кто писал эту рукопись, явно не самый аккуратный человек своего времени =). 
Исходное изображение печатного текста с проступившим текстом предыдущей страницы
Результат бинаризации по мере обучения сети
На этом примере, помимо неоднородного фона также присутствует текст, проступивший с предыдущей страницы. Отличие, по которому можно определить, что проступивший текст нужно классифицировать как "фон" — неправильное "зеркальное" начертание символов.

Изображение с таблицей и результат его бинаризации в процессе обучения сети
После каждого эксперимента мы дополнительно оценивали релевантность полученной модели на множестве отобранных данных из открытых архивов и на различных типах печатных документов и анкет. Было замечено, что при применении сети к примерам из этих данных результат работы алгоритма являлся зачастую неудовлетворительным. Было принято решение добавить такие изображения в обучающую выборку. Наиболее проблемными случаями являлись края страниц документов и линии их разметки. Всего было выбрано 5 дополнительных изображений, содержащих интересующие объекты. Первичная бинаризация была проведена с помощью имеющейся версии сети, после чего пиксели были верифицированы экспертом, а получившиеся изображения были добавлены в обучающую выборку.

На примере выше можно заметить, что сеть выделяет края страниц, что является ошибкой

Здесь, помимо ошибок сети на краях страниц, есть ещё очень "неуверенное" выделение линий таблицы и текста внутри неё
В процессе обучения сети, и анализа ошибок для повышения качества использовались методы аугментации данных. Применялись следующие виды искажений: отражения изображений относительно осей, яркостные, проективные, шумы (гауссов шум, соль-перец), пробовались эластичные трансформации, наподобие вот таких таких, вариация масштаба изображения. Применение каждого из них обусловлено спецификой задачи, наблюдаемыми ошибками в работе сети, а также распространенными практиками.
Пример комбинации нескольких методов аугментации, применяющихся “на лету” в процессе обучения сети
Поскольку при генерации данных со всеми различными искажениями и их комбинациями число примеров растёт стремительно, применяется аугментация на лету, при которой раздутие выборки происходят непосредственно перед подачей примеров на обучение. Схематично это можно изобразить так:
Схема, отображающая процесс раздутия данных на лету и выдачу на обучение
Подобный подход позволяет оптимизировать процесс раздутия данных и обучения сети, поскольку:
В целом, хотелось бы отметить важность правильной аугментации — благодаря данной технике можно обучить более сложную и умную сеть с одной стороны, а с другой — избежать переобучения и быть уверенным, что на тестовой выборке качество работы сети будет не хуже чем при обучении и валидации. Некоторые специалисты, по непонятным причинам, пренебрегают работой с данными и ищут способ улучшить работу системы только за счёт изменения архитектуры сетей. С нашей точки зрения, работа с данными не менее, а чаще — более важна, чем вдумчивый подбор параметров архитектуры сети.
Процесс повышения качества решения происходил итеративно. В основном работа шла по трём направлениям:
Работа с данными происходила следующими циклами:
Процесс анализа ошибок системы их устранение
Подробно работу с данными можно описать так: после каждого значительного этапа изменения системы (с улучшением качества, как мы каждый раз надеялись) подсчитывались различные метрики и статистики путём кросс-валидации. Отписывалось порядка 2000 “окон” (но не больше 50 окон с одного изображения), на которых ошибка достигала максимальной величины согласно квадратичной метрике и изображения из которых эти окна были вырезаны. Затем проводился анализ этих изображений и их классификация по типу ошибки. Результат выглядел примерно вот так:
Пример диаграммы распределения ошибок
Далее выбирается самый распространённый тип ошибок. Создаётся искажение, которое имитирует изображения с этим типом ошибок. Проверяется, что текущая версия системы действительно ошибается на искаженном изображении и ошибка возникает вследствии добавленного искажения. Затем, созданная процедура аугментации добавляется к набору уже существующих и применяется в процессе обучения. После обучения новой сети и обновления подстроечных параметров системы, происходит новый анализ и классификация ошибок. Как финальный этап цикла — проверяем, что качество растёт и число ошибок конкретного типа сильно уменьшилось. Естественно, здесь описан весьма “идеалистичный” ход развития событий =). Например для некоторых типов ошибок крайне сложно создать подходящие искажения или при добавлении искаженных изображений один тип ошибок может исчезать, а три других проявляться. Тем не менее подобная методология позволяет нивелировать 80% ошибок, возникающих на разных этапах построения системы.
Пример: на некоторых изображениях присутствует шум от неоднородного фона, клякс и, в частности, зернистость пергамента. Ошибки, проявляющиеся на подобных примерах, можно подавить с помощью дополнительного зашумления исходного изображения.
Пример имитации “зернистого пергамента” с помощью искажения
Процесс оптимизации архитектуры нейронной сети и гиперпараметров слоёв осуществлялся в нескольких направлениях:
В целом нужно сказать, что оба подхода дали некоторые результаты, но по качеству и надёжности уступали подходу обучения U-net сети “с нуля”.
Следующий этап создания финального решения — построение ансамбля из нескольких решений. Для построения ансамбля мы использовали 3 U-net сети, различной архитектуры, обученных на разных наборах данных и один необучаемый метод бинаризации, который применялся только на краях изображения (для отсечения краёв страниц).
Мы пробовали строить ансамбль двумя различными способами:
В процессе работы над ансамблем нам удалось достичь улучшения качества по сравнению c одной U-net сетью. Однако, улучшение было очень незначительным, а время работы ансамбля, состоящего из нескольких сетей было крайне велико. Даже несмотря на то, что в данном соревновании не было ограничения по времени работы алгоритма, совесть не позволила нам коммитить подобное решение. 
По мере продвижения к финальной версии алгоритма, каждый из этапов (добавление доразмеченных данных, вариация структуры, раздутия и т.д.) подвергался процессу кросс-валидации, для понимания того всё ли мы делаем правильно.
Финальное решение было выбрано основываясь на этой статистике. Им стало ровно одна U-net сеть, хорошо обученная с применение всего описанного выше + отсечение по порогу.
Одним из возможных способов предоставления решения организаторам, было создание докер контейнера с образом решения. К сожалению не было возможности использовать контейнер с поддержкой gpu (требование организаторов), и финальный расчёт шёл только на cpu. В связи с этим так же были убраны некоторые хитрые трюки, позволяющее немного повысить качество. Например первоначально, каждое изображение мы прогоняли через сетку несколько раз:
Затем полученные результаты усреднялись.
Как показали последующие результаты, даже без подобных ухищрений качества работы сети хватило с запасом, чтобы занять первое место на обоих тестовых датасетах =)
В текущем году в соревновании приняли участие 18 коллективов со всего мира. Были участники из США, Китая, Индии, Европы, стран Ближнего Востока и даже из Австралии. Было предложено множество решений, использующих нейросетевые модели, модификации классических адаптивных методов, теорию игр и комбинации различных подходов. При этом наблюдалась большая вариативность в типах архитектур используемых нейросетей. Использовались как полносвязные, свёрточные, так и реккурентные варианты с LSTM слоями. В качестве этапа предобработки использовась например фильтрация и морфологию. В оригинальной статье, кратко описаны все методы, которые применяли участники — часто они настолько разные, что остаётся только удивляться, как они показывают похожий результат.
Нашему решению удалось занять первое место как на рукописных документах, так и на печатных. Далее приведены финальные результаты замеров решений. С описанием метрик можно ознакомиться в работах организаторов конкурса опубликованных в предыдущие года. Мы кратко опишем только топ 5 результатов, остальные можно будет прочитать в оригинальной статье (ссылка на неё должна скоро появиться на официальном сайте соревнования). В качестве ориентира организаторами приводятся замеры для классического ноль-параметрического глобального метода Отсу и локального малопараметрического авторства Сауволы (к сожалению, точные значения настроечных коэффициентов неизвестны).
Лучший метод, не использующий никаким образом нейронные сетки занял 7-ое место.
Ниже демонстрируется работа нашего алгоритма на ряде тестовых изображений.





Ну и свидетельство, которое нам вручали на конференции ICDAR 2017 в Японии:
Пользователь
=====
OSU! Relax (основы)
2017-12-12, 22:31
Привет, Хабр! Представляю вашему вниманию перевод статьи Adventures in osu! game hacking.
Не так давно я начал играть в OSU! и она мне понравилась. Со временем захотелось немного поковыряться во внутренностях этой игры.
Итак, как мы будем разбирать beatmap? Мы можем разобрать все, начиная от названия песни, заканчивая настройками сложности. (Мы будем держать вещи простыми и анализируем только моменты времени, объекты попадания и некоторые значения, относящиеся к слайдеру.)
В стандартном режиме игры мы имеем дело с тремя типами объектов: кругом попадание, ползунком и счетчиком. В документации для формата файла .osu указано, что все объекты имеют такие составляющие: X, Y, время, тип. Все они будут включены в нашу структуру.
Я не хочу останавливаться на этом разделе слишком долго, так как это просто чтение каждой строки, ее разделение и сохранение результатов.
Существует несколько различных способов сделать это, но самый простой — с помощью Cheat Engine. Если вы параноик, как я, вы можете сделать эту часть в автономном режиме, в конце концов, было много известных случаев автоматических запретов, связанных с использованием Cheat Engine. По крайней мере, убедитесь, что вы вышли из своего OSU!, прежде чем продолжить.
Начните с открытия Cheat Engine. Если OSU! пока не запущена, запустите её сейчас. Нажмите на значок в верхнем левом углу, чтобы открыть список процессов, отсюда выберите OSU!.exe и нажмите „Attach debugger to process”. Вернитесь к OSU!.. Теперь убедитесь, что никакая музыка не играет. Вы можете сделать это в главном меню, щелкнув на значок остановки в правом верхнем углу.
Теперь вернитесь к Cheat Engine, введите 0 в поле «Значение» и выполните первое сканирование. Как только оно будет закончено, вы увидите больше миллиона результатов. Мы сократим это до нескольких. Вернитесь к OSU! и снова начните воспроизведение музыки. Теперь вернитесь к Cheat Engine, установите для типа сканирования значение «Увеличенное значение» и нажмите «Следующее сканирование». Это значительно уменьшит количество результатов. Продолжайте нажимать кнопку «Следующее сканирование», пока не останется с несколько результатов.
Мы почти получили его. Все, что осталось сейчас, — это динамически получать это значение. Вот почему мы использовали отладчик Cheat Engine раньше. Щелкните правой кнопкой мыши на каждый адрес и выберите <> в раскрывающемся меню. Некоторые из них нам не подходят, но вы должны найти тот, который при разборке выглядит аналогичным.

Я загрузил базовый внешний сигнатурный сканер, который мы будем использовать позже в нашей реализации.
Обратите внимание, что указанная выше подпись относится только к каналу Stable (Latest) release. Подписи, вероятно, будут отличаться по каналам Stable (Fallback), Beta и Cutting Edge (Experimental), но процесс его поиска будет таким же, как и выше.
Теперь нам нужно найти идентификатор процесса OSU! и обработать его. Существует много разных способов сделать это, но вероятно проще всего использовать CreateToolhelp32Snapshot, а также Process32Next для перебора списка процессов.
Теперь у нас есть идентификатор процесса, мы можем открыть дескриптор процесса.
Поскольку нам нужно только считать его память, мы будем использовать PROCESS_VM_READ, как желаемый флаг доступа.
Это была большая часть скучного материала. Теперь нам нужны только адрес времени игры и способ отправки ключевых входов, прежде чем мы сможем продолжить. Для первого из них понадобиться подписи, которые мы сделали ранее.
Для последней из этих вспомогательных функций нам понадобится что-то, что будет нажимать клавишу, когда мы ее вызываем. Опять же, есть несколько способов реализовать это, но я нашел keybd_event, но SendInput будет самым легким. Поскольку keybd_event устарел, мы будем использовать SendInput,
Все, что осталось, это перебрать удаленные объекты и отправить входы по мере продвижения. Изначально мы находимся в начале beatmap. Теперь мы можем прочитать время, чтобы узнать, где мы находимся на самом деле.
Обязательно добавьте проверку для карт с AudioLeadIn time.
Вот где начинается настоящая забава. Возможно, вы ожидали, что эта часть будет сложной, но логика здесь на самом деле довольно прямолинейна. Мы ждем 'start time' текущего объекта, удерживаем ключ, ждем 'end time’, а затем освобождаем его. После того, как мы выпустили ключ, мы переходим к следующему объекту и продолжаем, пока не достигнем конца beatmap.
Обратите внимание, что я вычитал пять миллисекунд со времени начала, это своего рода волшебное число, и ваш пробег может отличаться от него. Он не мог нажать на все кнопки и слайдеры идеально без этого. Я также добавляю две миллисекунды к концу окончания круга в классе beatmap. Поскольку круги не нужно удерживать, мы хотим как можно скорее отпустить их. Если мы отпустим их слишком быстро, нажатие может быть проигнорировано, поэтому нужны дополнительные 2 мс.
Ну, теперь мы готовы скомпилировать и протестировать OSU!Relax!
User
=====
[DotNetBook] Структура экземпляров типов и VMT
2017-12-13, 01:20
Семинары по платформе .NET CLRium
=====
Алгоритм генерации гирлянды для новогодней головоломки
2017-12-15, 08:49
User
=====
Корреляция IQ с нашей жизнью (Feature ranking)
2017-12-13, 02:28
Пользователь
=====
Язык Lua и Corona SDK (3/3 часть)
2017-12-13, 13:01
User
=====
Встраиваем In-App purchase в своё приложение
2017-12-16, 17:45
User
=====
Виртуальные ядерные установки: полный цикл разработки для Oculus Rift в одном лице (UPD 18.12.2017)
2017-12-13, 10:04
Пользователь
=====
Уязвимость из 1998 года снова в строю – встречайте ROBOT
2017-12-13, 10:17
User
=====
Батхёрт Разработческий: как победить?
2017-12-13, 11:57
Метод Лаб — ускорение сайтов
=====
Опубликованы результаты опроса по использованию javascript-технологий «The state of JavaScript 2017»
2017-12-13, 12:52
 
Всем нам время от времени становится интересно, не устарела ли та или иная технология, и что сейчас в тренде. Особенно это актуально в мире frontend. 
Сегодня у нас появились ответы на некоторые из этих вопросов. Сегодня на сайте https://stateofjs.com опубликован новый отчет с результатами опроса двадцати тысяч разработчиков. Лучше всего сразу смотреть оригинал, но если времени мало, то в этой статье будут освещены ключевые моменты.
Красивую подробную диаграмму можно посмотреть здесь, а если кратко по местам, то десятка лидеров выглядит так:
(Кто-нибудь знает, как убрать width:100% для таблицы на Хабре?)
В левой колонке — количество людей, которые использовали фреймворк и остались довольны.
Как можно видеть, React по прежнему абсолютный лидер. Vue.js, о котором столько говорили и писали, хоть и нарастил свою долю за год, но не более того. Убийцей Реакта так и не стал. Тем не менее, как можно видеть из диаграммы по ссылке, интерес к vue попрежнему большой, много людей планируют его попробовать.
В общем, пока что стоит писать на js с классами (ES6) или, если вы любите строгую типизацию, то typescript. 
Имхо flow уже не получит такого распространения, как typescript, потому что facebook похоже начал активно развивать Reason. Который, правда, тоже еще далек от распространенности.
Тут какая-то жесть. В одну кучу смешали все технологии. Как можно сравнивать Rest API, redux и MySQL? Что больше, метр или килограм?
Тем не менее, посмотреть отчет по state management стоит, потому что какие-то выводы можно сделать. 
Например, что redux в 10 раз обгоняет mobx и другие аналоги, т.е. он похоже стал абсолютным лидером в этой области, стандартом де факто. А также, что GraphQL, несмотря на весь хайп, пока что не так распространен, как RestAPI.
Из этого отчета ясно, что express — абсолютный лидер, остальные отстают на порядок.
Хотя опрос называется "The State of JavaScript", все мы понимаем, что без знания css ничего не получится. Вот технологии, связанные с css:
→ Полная диаграмма здесь
Похоже, мы дожили до того момента, когда голый css проигрывает расширенным вариантам (sass/scss/less)
Тут без сюрпризов. В топе Npm и webpack. Ну еще gulp, но судя по диаграмме многие от него будут отказываться.
Судя по диаграмме, нативные приложения по прежнему в топе:
Есть также отчеты "прочее", которые авторы не смогли причислить ни к какой категории, поэтому они все свалены на одной странице.
Из любопытного: 
1) yarn-ом пользуются больше, чем npm.
2) vscode минимум в два раза популярнее, чем остальные редакторы, включая webstorm.
Также в отчете есть распределения зарплат по технологиям, распространение технологий по странам и интерактивная диаграмма о связи технологий друг с другом (например, сколько раз используется react+redux)
Тимлид, ведущий подкаста «Цинковый прод»
=====
Эксперименты с контрактами Solidity в тестовой сети Rinkeby блокчейна Ethereum
2017-12-14, 17:07
Управляющий директор
=====
Почтовые ящики, которые и не ящики вовсе…
2017-12-13, 13:38
Проджект-лид
=====
Новогодний датасет: открытая семантика русского языка
2017-12-20, 15:04
Компьютерная лингвистика
=====
Когда биткоин перестанет расти: токены — настоящая альтернатива коинам
2017-12-13, 13:20
Корпоративный аккаунт
=====
Две крупнейших CRM — Salesforce и Zoho — сравнение телефонных интеграций
2017-12-13, 15:40
Пользователь
=====
Разработка через приемочные тесты (ATDD). Что это такое, и с чем его едят
2017-12-13, 13:26
Разработка через тестирование (TDD) – отличный способ повысить качество и надежность кода. Этот же подход может быть распространен и на разработку требований. Он называется "Разработка через приемочные тесты" – acceptance test driven development (ATDD). Сначала я присматривался к этому подходу, потом пробовал применить, потом долго тюнинговал, чтобы приспособить его под мои нужды, и теперь хочу поделиться мыслями. И для себя еще раз разложить все по полочкам.
В этой статье я расскажу небольшое введение в тему. Пример будет совсем простой и скорее для иллюстрации. А в следующей статье постараюсь поделиться историей, как я применял ATDD на практике при разработке настоящей фичи в реальном продукте.
Когда я работал программистом в аутсорс компании на один банк, то мне приходилось изучать спецификации требований и оценивать трудоемкость задач. Оценивать нужно было как можно точнее, мы работали по модели оплаты за проект (Fixed Price), и все промахи в сроках были на нашей стороне и не оплачивались. Каждый раз, когда я читал спецификации, мне было все понятно, я не замечал в них нелогичные моменты, упущения, странности. Но как только начиналась разработка, то все косяки требований вылезали наружу, и было удивительно, как я их пропустил в начале. Несмотря на все усилия, я так и не мог придумать способ, как читать спецификации и находить в них проблемы до реализации.
Потом я перешел на работу в крупную компанию, которая занималась разработкой большого и сложного коробочного продукта, над которым работала огромная команда. Аналитики общались с партнерами и клиентами и записывали их пожелания. Потом эти спецификации, прежде чем быть взятыми в работу, проходили процедуру ревью, в которой участвовали и разработчики. Чтобы не тратить время на самой встрече, надо было сначала прочитать требования и подготовить вопросы. Как и в предыдущем проекте, большинство вопросов к содержимому документов возникали позднее – во время разработки, а не тогда, когда должны были возникнуть, то есть на этапе ревью.
Затем я ушел в свой стартап. Естественно, там не было никаких аналитиков, спецификаций и ревью. Обратную связь мы получали от пользователей в виде имейлов или звонков, тут же превращали это в фичи и включали в план разработки. Несмотря на отсутствие задокументированных требований, все равно приходилось оценивать трудоемкость задач. То, что на первый взгляд казалось очень простым, на деле становилось головной болью и наоборот. При быстром переключении контекста с одной проблемы на другую, уже реализованные решения вылетали из головы и становилось все труднее и труднее совмещать их в одном продукте. Нам было нужно какое-то подобие технической документации, тестпланов и требований. И чтобы стоило недорого.
Acceptance test driven development (ATDD) является развитием идеи test driven development (TDD). Общий смысл в том, что прежде чем что-то делать, надо придумать критерий выполненной работы и критерий того, что работа сделана правильно. Почему это важно? Потому что эти критерии позволяют на самом раннем этапе понять, что именно требуется сделать, как это сделать, что именно считать хорошим результатом. Т.е. не выяснять детали по ходу дела, строя прототипы, а сразу приближаться к цели, так как цель уже определена, причем вполне формально.
Эти критерии описываются на понятном заказчику языке в виде готовых сценариев. Сценарии моделируют то, как проектируемая фича будет использоваться в дальнейшем. Если сценарий реализован и ожидаемый в нем результат может быт получен на практике, значит задача решена корректно и работу можно считать выполненной. Набор таких сценариев и называется приемочными тестами. Приемочные тесты фокусируются на поведении системы с точки зрения человека, а не на внутреннем устройстве и на технических деталях реализации.
Несмотря на общее название, этот подход относится ко вполне определенной части процесса – той, где происходит разработка требований и их формализация в спецификации. В данном процессе часто участвуют люди как со стороны бизнеса, так и с технической стороны, т.е. люди, обладающие разными компетенциями и взглядами на мир. Заказчики на интуитивном уровне понимают, что именно они хотят видеть в продукте, но сформулировать и перечислить требования кратко (и полно) могут далеко не все. Разработчики (представители исполнителя), в свою очередь, часто не знают, что именно забыл рассказать заказчик, и как это выяснить.
Для решения этих задач используется фреймворк Given – When – Then.
Смысл приемочного теста — показать, что произойдет с системой в конкретном сценарии. Для этого в сценарии должно быть описано, как выглядит система в начале теста, затем описывается какое-то действие, которое эту систему меняет. Это может быть воздействие снаружи, а может быть и какой-то внутренний триггер. В результате система немного меняет свое состояние, что и является критерием успешности. Важный момент: система рассматривается как черный ящик. Другими словами, формулируя тест, мы не знаем, как система устроена внутри и с чем она взаимодействует снаружи. Тут есть одна особенность. Иногда изменение системы недоступно для непосредственного наблюдения. Это означает, что саму приемку провести не получится. Выхода тут два — либо попытаться определить состояние косвенно, через какие-то соседние признаки, либо просто не использовать такой тест. Примером могут быть изменение полей в таблицах БД, отложенные изменения в каких-то недоступных файлах и т.д.
В юнит тестах используется шаблон Arrange – Act – Assert (AAA). Это означает, что в тестах должны быть явные части, отвечающие за подготовку данных — arrange, само действие, результат которого надо проверить – act, и собственно проверка, что реальность совпала с ожиданиями – assert. Для приемочных тестов используется подход Given – When – Then (GWT). Суть та же, только с другого ракурса.
Given часть может содержать в себе как одно, так и набор состояний. В случае, когда их несколько, эти состояния должны читаться через "И". Объединять какие-то состояния через "ИЛИ" можно, но тогда это будут два разных теста. Такое возможно для упрощения записи. Я рекомендую избегать этого до того момента, как все возможные комбинации состояний не будут описаны. Тогда можно быть уверенным, что ничего не забыто и слить несколько тестов в один для упрощения чтения и понимания. Это же справедливо и для Then — исходов, которые надо проверить может быть несколько. When должен быть один. Взаимовлияния триггеров лучше избегать.
GWT тесты вполне можно читать вслух: "Пусть (given) A и B, и C. Когда (when) случается D, то (then) получается E и F.". Их вполне можно использовать для документации или формулирования требований. Когда я говорю "читать", я не имею ввиду, что именно так они и должны быть записаны. В реальности такие тесты получаются очень масштабными. Если их записать простым текстом, то потом взглянуть на них системно очень тяжело. А без системы легко пропустить какие-нибудь важные сценарии.
Очень важный момент: формат записи нужно выбирать тот, который наиболее подходит к вашей задаче, с которым удобнее работать. Никаких ограничений тут нет. Given, when, then — это общая структура записи, то есть то, что обязательно должно быть в тесте, а непосредственное представление может быть любым – хоть предложения, хоть таблицы, хоть диаграммы.
ATDD не диктует правила, а предоставляет фреймворк для того, чтобы составить свою спецификацию через примеры. Есть модель черного ящика, GWT и их комбинирование. Все остальное является применением этих механизмов на практике, часть из которых можно считать устоявшимися.
Для примера возьмем что-нибудь простое и понятное, например, светофор. Как можно описать требования к разработке светофора с помощью GWT нотации? Для начала нужно понять, что именно в светофоре можно назвать Given, что является When, а что Then.
За состояние светофора можно принять информацию о том, какая секция сейчас горит. Светофор переключается (меняет состояние) через какие-то промежутки времени. Значит триггером является таймер, точнее, срабатывание таймера. Результатом срабатывания триггера является переход в одно из состояний. Т.е. можно считать, что в примере со светофором Given и Then – один и тот же набор:
Опишем поведение светофора в нотации GWT:
Вот 5 сценариев, прочитав которые, можно понять, как работает светофор. Естественно, у светофора есть еще куча режимов, например, режим желтого мигающего (когда он неисправен), или ручной режим управления (например, в случае ДТП) и т.д. Но не будем усложнять иллюстрацию.
Описывать тесты словами мне кажется избыточным. Тем более, что меняется в них только название цвета. Тут лучше подойдет диаграмма состояний или простая таблица:
Пример показывает один из основных преимуществ приемочных тестов: они позволяют общаться с бизнес пользователями практически на их языке. Приятным бонусом идет готовый набор сценариев для тестирования и последующей автоматизации.
Нотация Given — When — Then структурирует процесс составления тестов и дает уверенность в том, что тесты описывают все аспекты поведения системы. Не нужно сидеть и постоянно спрашивать себя: "А какой сценарий я еще не описал?".
Итак, алгоритм такой:
На каждом из этих этапов требуется участие заказчика или человека, который играет его роль, потому что именно он лучше всех представляет, что и как в итоге должно работать.
Как уже было сказано, подобный подход, несмотря на свою избыточность, дает уверенность в том, что ни один из сценариев не будет пропущен. Это, пожалуй, главное преимущество такой формализации. Зачастую бизнес-пользователь видит процесс только в общих чертах и ему не видны детали. Я уверен, что вы постоянно слышите от заказчика или даже аналитика фразы типа: "Нам нужна такая-то фича, я все придумал, вот, смотри картинку", или "Тут нам нужна такая-то кнопка, у нас уже есть похожая функциональность в другом месте, сделай как там". Если до того, как начать разработку, сесть и прикинуть возможные варианты развития событий, то сразу всплывет очень много деталей, в которых, как известно, и кроется дьявол.
Подобный подход так же полезен и в случае, когда от аналитика приходит спека и ее нужно прочитать, дать свои оценки сложности и трудозатрат. При прочтении все детали ускользают, но если по ходу чтения вести конспект по форме GWT, то сразу становится понятно, какие сценарии плохо или неточно покрыты в требованиях и требуют уточнений.
Помимо анализа требований с целью разработки решения, GWT сценарии можно применять и для сбора требований. Предположим, что есть какая-то функциональная область и человек, который в ней разбирается, но время на общение с ним очень ограничено. Если подготовиться заранее и разобрать сценарии с помощью GWT фреймворка, то на самом интервью нужно будет узнать только то, что мы ничего не забыли из раздела Given, When и уточнить, что именно должно быть в разделе Then.
Есть специальные инструменты для автоматизации GWT сценариев, записанных в том числе и на естественных языках. Пример — cucumber. Я с ними не работал, поэтому ничего кроме факта их существования рассказать не могу.
Обратная сторона мощности GWT — избыточность. Предположим, что вы определили N штук given, M штук when и K штук then. В худшем случае количество тестов будет огромным – N  M  K. И с этим надо как-то жить. Это верхняя оценка сложности; в реальности далеко не все эти сценарии будут осуществимы, а часть из них будет дублировать друг друга, а еще часть можно пропустить ввиду низкого приоритета или очевидности.
Вторым недостатком можно указать формат. По моему опыту могу сказать, что GWT записи всегда стремятся к минимализму. Во время их разработки не хочется тратить времени на детальные описания, потому что зачастую сценарии похожи друг на друга. В результате получается тяжело читаемая структура. После некоторого перерыва для ее понимания приходится восстанавливать контекст, и заново вспоминать условные сокращения и записи. Это также затрудняет задачу передать кому-то документ для ознакомления, так как, скорей всего, для его прочтения потребуется сам автор.
Следующий недостаток объясняет, почему ATDD скорее относится к области формализации требований с бесплатным бонусом в виде тестовых сценариев, а не собственно тестирования. Такие сценарии не могут описать композитные (большие и сложные) сценарии. Тестирование идеального черного ящика в первую очередь основано на аксиоме его идеальности. В реальности ящики черными не бывают, они всегда взаимодействуют с чем-то снаружи себя, являясь при этом частью более сложной системы — продукта. Легко можно переусложнить требования, если попытаться включить в один документ сразу все связи внутри продукта. Такой набор приемочных тестов будет настолько огромным и сложным, что мало чем сможет помочь. Поэтому, в реальной жизни сквозные сценарии в качестве приемочных тестов не применяются.
Если верить Википедии, то идея формулировать спецификации через конкретные сценарии была впервые описана Ward Cunningham в 1996 году, а сам термин specification by example ввел Martin Fowler в 2004 году. Дальнейшее развитие идеи формулируется в книге "Bridging the Communication Gap: Specification by Example and Agile Acceptance Testing" от Gojko Adzic 2009 года. В 2011 он же выпустил еще одну книгу на эту тему: "Specification by Example: How Successful Teams Deliver the Right Software". Рекомендую эти книги для обращения к первоисточнику.
User
=====
Системы ИИ в 2018: шесть прогнозов
2017-12-13, 15:48
Пользователь
=====
Две правды, одна ложь: популярные концепции о «выдающихся программистах»
2017-12-22, 10:07
Пользователь
=====
Апдейт Veeam Availability Suite 9.5: что нужно знать
2017-12-21, 14:30
Пользователь
=====
Hadoop 3.0: краткий обзор новых возможностей
2017-12-24, 11:55
Пользователь
=====
Незадача коммивояжера и жёлтый октябрь
2018-02-13, 14:00
Главнокоммивояжер Аристарх поглядывал на Пророка, покручивая дубинкой от снежных троллей — ходовым сезонным товаром — 11% отклонение прогноза продаж на 10 дней в среднем (MAPE) впечатлило и, как у нас в чате говорят, зашло в роли baseline. Если он так же хорош, как и их Цукерберг, то сразу в прод — таков был первый порыв. Пророк поглядывал на главнокоммивояжера, прищурив правый глаз. Такой серьезный, в костюме, и верит в то, что инновации апплодисментами встретят и сразу же примут — мысль в голове вертелась, постепенно обретая форму. А Вы в курсе, юноша, скольким коллегам и контрагентам со своими нововведениями немилы станете? Они же Вас невзлюбят сразу, к гадалке не ходи! В общем, порыв жил обычным циклом инноваций.

В дисциплине управления проектами стейкхолдерами называют всех, кого проект коснется (а также тех, кто может оказать на него влияние). Люди они разные, со своими интересами, ожиданиями, и чаяниями. Закрыть глаза в надежде, что и проекта не заметят — весьма опрометчиво (вспоминается неприглашенная колдунья). Boston Consulting Group оценивает долю IT проектов, почивших по не-техническим причинам, в 75%. Последние две редакции свода знаний по управлению проектами (PMBOK) выделяют процессы по управлению стейкхолдерами в отдельную область знаний под счастливым номером 13 и настоятельно рекомендуют учитывать связи между ними, центры влияния, а также культуру общения для повышения шансов на успех.
Мы покажем, как оценить стейкхолдеров с помощью машинного обучения. Выделим группы похожих между собой людей и решим задачу кластеризации — сегментации клиентов в терминах маркетинга — в социальных структурах, которые построим из: 1) потоков сообщений и 2) эмоциональной окрашенности текста. Для этого заглянем в переписку, любезно предоставленную г-жей Клинтон, способом, предложенным в журнале Биоинформатика.
Главнокоммивояжеру Аристарху предстоит внедрить современные информационные технологии в организации, частью которой он руководит непосредственно. Изменение существующих процессов — неизбежное следствие цифровой трансформации, в которую он ввязался. Давным давно в далекой галактике новинкам были рады, в честь инноваторов всякий раз устраивали вечеринки по поводу очередного рацпредложения, всем было весело и перемены заходили на ура.
На Земле — все иначе. Новшествам здесь не рады. И чаще всего противятся. Скопом.
Современный взгляд на управление изменениями базируется на работе Эверетта Роджерса, его книга "Diffusion of Innovations" (цитата заголовка содержания из гугл книг — сама по себе арт-объект) пережила пять редакций с 1962.
Наблюдение за процессом принятия инноваций в сельском хозяйстве — фермеры годами медлили с переходом к современным методам хозяйства и их поведение, странное с экономической точки зрения, озадачивало и даже досаждало исследователю. Изучать внедрение новшеств в реальном мире Роджерс начал в середине ХХ века, а до этого — служил в Корее в авиации, где изучал практические вопросы социологии. Все началось с гибридной кукурузы — распространение новых семян в двух популяциях фермеров из Айовы стало отправной точкой к условному разделению общества на инноваторов, ранних пользователей, раннего большинства, позднего большенства, и тормозов, а также — появлению роли агента изменений — самой важной из возможных ролей стейкхолдера (заинтересованного лица).

Модель распространения перемен в социальной сети практически не претерпела изменений и мы покажем, как применить классические и современные методы на практике для анализа стейкхолдеров.
Из-за того, что все стейкхолдеры — люди разные, на разных должностях, и с разными привычками, масштаб перемен будет различаться для индивидуумов. Лет пять назад на каком-то тренинге по управлению изменениями мне попала на глаза занятная формула, которая задает условия для достижения успеха:
A + B + C > D, 
где:
A — недовольство текущим положением дел;
B — общее видение проблемы;
C — безопасное действие;
D — усилие, необходимое для перехода к новому положению дел.
Когда это неравенство выполняется для каждого стейкхолдера по-отдельности, перемены распространяются быстро и устойчиво. В противном случае — жди сопротивления.
Рецепт успеха прост: убедить каждого.
Да вот, незадача, Аристарх — один, а стейкхолдеров — уйма. Убеждать каждого по-отдельности — замаешься, а всех скопом собрать — толку мало, подход к директорам отличается от такового к работникам склада. С даром убеждения у главнокоммивояжеров все в порядке в силу должностных обязанностей, подход к каждому найдут, а вот времени — в обрез, продажи нужно двигать. Поэтому мы будем разделять популяцию на группы, участники которых максимально похожи между собой. Каждой группе Аристарх подготовит свое сообщение, сократив общие усилия на то, чтобы достучаться до сердца всякого.
Ограничим незадачу коммивояжера вопросом: "Как сегментировать стейкхолдеров?"

Не все кажущиеся очевидными вещи общеприняты и распространены везде.
Поначалу никто не информирован. Это важно помнить, потому что каждого стейкхолдера впереди эмоциональное путешествие. Вовлеченность, или готовность к сотрудничеству, меняется во времени.

Вот та модель (позаимствовано у SAP), которая в общем виде описывает вовлеченность (нужный нам качественный результат всех переживаний) отдельно взятого индивидуума, которого ваш проект коснется. Вначале о вас не знают ничего и спокойно делают свою работу, наслаждаясь текущим положением дел. Затем появляетесь вы, вместе с инновациями, будь-то кукуруза нового сорта, или машинное обучение, реакция одна — раздражение. 98% популяции невзлюбят вас сразу. Ненадолго, как это бывает — провели себе совещание, время потратили, да и забыли. Пообедали вкусно, да и досада улетучилась. На инноватора с его нововведениями примерно так же пофиг, как и до совещания.
Дальше — лучше.
Неизбежность перемен становится очевидной и грусть нарастает. Печальный стейкхолдер постепенно теряет интерес к происходящему (некоторые — лихорадочно пытаются выкрутиться, но это не помогает) и скатывается в состояние, которое в проектах внедрения SAP называют "долиной слёз" — паника, неверие, оцепенение, эмоциональные срывы — вот краткий перечень симптомов, которые я повидал лично.
Все там будут.
Наконец, до личности доходят выгоды от проекта, начинается сотрудничество. Вовлеченность растет. Ура, начинается продуктивная стадия взаимодействия! Построить позитивное отношение стейкхолдеров к проекту можно действуя в четырех направлениях: 1) установив эффективную структуру коммуникации информации о проекте; 2) поощряя нужное поведение; 3) управляя организационной структурой; и 4) навыками людей. Мне неизвестны примеры производителей информационных систем, уделяющие хоть сколько-нибудь сравнимое с таковым у SAP внимание организационным изменениям.
Мы рассмотрели динамику эмоций индивидуумов. С организацией — веселее.

Ситуация, когда всецело увлеченный проектом топ-менеджер радостно топает в столовку и сталкивается с хмурым руководителем среднего звена, только-только позавчера проникнувшимся оттенками светлого будущего, ведущим за собой угрюмых работников, находящихся на самом дне энтузиазма — закономерный результат пренебрежения законами природы и экономии на проведении организационных изменений.
В 2008 году ЦРУ опубликовало 32-страничную брошюру 1944 года. Переведенный на множество языков, документ распространялся от Греции до Норвегии. Как парализовать работу режима максимально снизив эффективность всего? Советы актуальны и сегодня и в процессе внедрения инноваций многие практики проявляются в современных организациях. Бестселлер, что сказать.

Распространение инноваций и препоны, возникающие на их пути, интересует исследователей достаточно давно. В социальных науках принято размышлять о том, почему люди действуют определенным способом и есть даже отдельное направление исследовательской деятельности, основанное на традиционной статистике и социологических исследованиях. На мой взгляд, эта область деятельности ищет ответы на вопрос "Как люди рационализируют свой выбор?". Временами поставленные исследователями вопросы бывают толковыми и их действительно можно применять в реальном мире.
(O'Tool, J. (1996). Leading change: The argument for values-based leadership)
Все те, чьи интересы затронет проект оптимизации, против него подружатся.

Сплочение коллектива перед лицом внешней угрозы — характерное свойство рода человеческого. История знает много примеров начала охоты на ведьм в трудные времена. Главное, что нужно знать — это то, что 1) существующие социальные связи будут влиять на то, как именно подружатся стейкхолдеры и 2) незадачу можно описать и измерить средствами теории графов.
Теория графов — раздел дискретной математики, изучающий свойства графов. В общем смысле граф представляется как множество вершин (узлов), соединённых рёбрами. В строгом определении графом называется такая пара множеств G=(V,E), где V есть подмножество любого счётного множества, а E — подмножество V х V. Ребра могут быть ориентированными (направление сообщения) и неориентированными (участие в комитете), иметь вес (количество сообщений), или знак (дружба/вражда). Как узлы, так и ребра могут иметь дополнительные свойства, которые ограничиваются только фантазией и размером оперативной памяти.
В нашей текущей незадаче узлами будут те самые стейкхолдеры, которых нужно сделать довольными, а рёбрами — связи между ними.
Забавный факт: руководители проектов регулярно пользуются элементами теории графов, но в большинстве своем не догадываются об этом. Например, сетевые диаграммы — один из способов отображения плана проекта, в них узлами выступают задачи, а рёбрами — взаимосвязи между работой, которых бывает четыре вида: 
Finish-Start — последующая задача начинается после завершения предыдущей;
Start-Start — последующая задача начинается одновременно с предыдущей;
Start-Finish — последующая задача может завершиться только после начала предыдущей;
Finish-Finish — последующая задача может завершиться только после завершения предыдущей;
Практическая польза представления плана проекта в виде графа выражается в реализации метода критического пути и нахождения последовательности задач, задержки в которых приведут к неизбежному сдвигу конечной даты проекта, т.к. запаса времени на критическом пути не предполагается по определению. Задача решается, например, алгоритмом Дейкстры.
Исследование социальных сетей достаточно давно используется для описания отношений между людьми и до сих пор полученные графы анализировались по-отдельности.
Например, прекрасное во всех отношениях исследование альтруизма среди "ведьм" в китайских деревнях в сравнении с простыми крестьянами, показавшее, что разницы-то и нет, оперирует четырьмя графами связей, соответствующими различным видам взаимодействия:
Отношения между "ведьмами" (квадраты) и обычными крестьянами (круги) в одном из поселений: a) помощь в сельскохозяйственных работах, b) подарки, c) половые связи, d) биологическое родство.
Как это видно из рисунка, отношения между людьми формируются неоднородно — некоторым больше помогают, кого-то задаривают, а кто и популярен у противоположного пола более, чем прочие. 
Традиционный подход к классификации, рекомендуемый PMBOK, состоит в ранжировании по нескольким параметрам: сила, влияние проекта, заинтересованность — все эти измерения довольно субъективны и вопросом: "А кто кого заборет: главнокоммивояжер главносчетовода, или же завскладом всех сильнее?" я лично задаюсь уж несколько лет кряду.
Анализ социальных сетей (тех самы графов связей) позволяет оценить роль и важность отдельных личностей. Для этого используются метрики центральности, оценивающие узлы графа количественно.
Пример распределения метрик центральности для одного графа: A) Betweenness, B) Closeness, C) Eigenvector, D) Degree centrality.
Degree centrality — количество связей для каждого узла, для ориентированного графа выделяют входящие и исходящие связи, соответственно in-degree/out-degree. Метрика позволяет оценить, насколько "общителен" одельно взятый стейкхолдер.
Closeness centrality — оценивает среднее расстояние от каждого узла до всех прочих, здесь мерой расстояния является минимальное количество рёбер, по которым можно пройти между двумя узлами (кратчайший путь на графе). Метрика позволяет оценить, насколько быстро будет распространяться сообщение от отдельно взятого стейкхолдера.
Betweenness centrality — оценивает количество кратчайших путей, проходящих через узел. Метрика позволяет определить, через кого проходят потоки информации, например, и выявить, через кого из стейкхолдеров "договариваются" отделы.
Eigenvector centrality — оценивает, насколько узел связан с наиболее связанными узлами графа. Метрика позволяет выявить наиболее влиятельных стейкхолдеров.
Page Rank — оценивает вероятность нахождения случайного бродяги, гуляющего по рёбрам между узлами. Метрика является одной из самых важных в алгоритме ранжирования результатов поиска Google.
Приведенные метрики встроены в большинство пакетов для работы с графами (SAP BO, SNAP, Gephi, Pajek, NodeXL и прочие), и алгоритмы их вычисления достаточно дёшевы в плане вычислительной нагрузки. Можно держать все связи в уме, как я это делал в бытность руководителем проектов (голова от этого пухнет и болит), а можно взять и посчитать.
Практическая польза оценки центральности стейкхолдеров в том, что самые популярные узлы в сети — всегда на виду, и как только вам удасться договориться с ними о трансляции нужного сообщения (это может быть и смена поведения), то у прочих участников возникнет иллюзия того, что новые практики уже использует большинство. Возникнет сетевой эффект.
Главное, что нужно помнить — это Free Lunch Theorem (ни один оптимизационный алгоритм не является универсальным), поэтому на практике обыкновенно используют несколько метрик одновременно.
Возвращаясь к вопросу сегментации, разделим его на два подвопроса: "Как разделить стейкхолдеров на группы?" и "Как повлиять на группы?" — первый важен потому что со всей массой общаться неэффективно, второй же нацелен на то, чтобы повысить шансы на доставку сообщения (и проведение необходимых перемен) за счет использования правильных людей на правильном месте в нужное время.
Разделение графа на кластеры, или задача нахождения оптимального сечения, основано на предположении о том, что в социальной сети есть группы, более плотно связанные внутри — у каждого узла в кластере большая часть связей должна быть внутри. Задача — не нова и алгоритмов для ее решения есть немало, один другого краше.
Самый простой способ оценки качества разделения — модулярность — мера сравнения количества ребер в полученных кластерах со случайным графом (модель Эрдоша-Рейни: узлы соединяются между собой случайным образом, так, чтобы средняя степень узла — количество связей — осталась неизменной).
Пример удачного разделения графа на кластеры, максимизирующий модулярность.
В полученных разделах можно выделить центральные узлы одним или несколькими обозначенными ранее методами — так мы получим список кандидатов на роль агента изменений, а можно пойти несколько дальше и попробовать проанализировать характер связей кандидатов и предсказать их шансы на успех.
Одно из первых масштабных исследований (Battilana, Julie, and Tiziana Casciaro. "Overcoming Resistance to Organizational Change: Strong Ties and Affective Cooptation." Management Science 59, no. 4 (April 2013): 819–836.), обобщившее 68 кейсов масштабных перемен в английских госпиталях, выделяет несколько правил успеха для агентов изменения:
Шансов у зеленого с синими больше, чем у красного с серыми.
До сих пор мы рассматривали методы и эвристики, позволяющие делать выводы на основании структуры связей между стейкхолдерами. Обычный подход к решению таких задач, как: 1) прогнозирование связей в будущем (Link Prediction Problem), 2) определение роли и популярности стейкхолдера, или 3) сегментации, заключался в создании признаков для узлов и рёбер вручную и последующем использовании классического алгоритма машинного обучения.
Времена меняются, вычислительная математика развивается. State of the art в области автоматического создания признаков из графов является работа node2vec, на базе которой и построено исследование, которое мы воспроизведём сегодня. Встречайте!
Теории графов в этом году исполняется 300 лет без 18, и до сих пор предметом анализа были отдельные графы. Наш мир несколько сложнее и связи между узлами могут быть разного рода, да и узлы — разнотипными. Мне приходится по роду деятельности анализировать организации и одним родом связей в этом случае ну никак не обойтись.

Вопрос: "А как моделировать реальную организационную структуру?", заданный преподавателю в первый год моей второй магистратуры в далёком уже 2014, нашел ответ лишь летом 2017, за неделю до защиты диплома.
Статья в журнале Биоинформатика (Predicting multicellular function through multi-layer tissue networks. Marinka Zitnik and Jure Leskovec. Bioinformatics, 33, 14:i190-i198, 2017) предлагает новый подход к моделированию сложных систем. Идея в том, чтобы представить отношения между сущностями (в оригинальной работе это взаимодействия между протеинами в различных отделах мозга), а также иерархию отношений (структура мозга), и спроецировать их в многомерное пространство таким образом, чтобы связанные сущности размещались компактно.
Механику работы метода (а внутри — всё тот же случайный бродяга) отлично рассказывает Юре Лесковек в предпоследней лекции курса CS224W образца 2017 года. 
Для нас важнее то, что теория графов — это универсальный уровень абстракции, оперирующий сущностями и их связями. Именно это свойство позволяет применить алгоритм из области биоинформатики к управлению проектами, достаточно лишь описать все те сети связей между стейкхолдерами.
Подходов к описанию социальных структур несколько и первый — социологические исследования. Графы, описывающие сотрудничество, доверие, уважение, можно построить, задавая вопросы, вроде:
Не все связи одинаково полезны, и существующие конфликты можно выявить и нанести на карту организации, расспрашивая:
Негативные связи, в целом, случаются гораздо реже, но их эффект — весьма разрушителен.
Можно задавать вопросы, а можно и без этого обойтись и выудить отношения из вороха данных, который создается в любой организации ежечасно. Например, информационные потоки, которые описывают наиболее привычный способ общения между людьми, можно добыть из:
Важная информация о структуре существующих отношений может быть почерпнута из:
У читателя может сложится мнение, что рассмотренные методы воплощают опасения Оруэлла и я предлагаю бизнесам воплощать подобие Большого Брата всякий раз, когда возникает необходимость трансформации.
Так и есть.
В современном мире темп изменений нарастает, цифровая трансформация убивает бизнес-модели, и все это приводит к тому, что способность управлять переменами и гибкость организации становятся важными качествами для выживания.
Роль руководителя трансформируется.
Задачи планирования и контроля — традиционная область ответственности менеджера среднего звена — всё чаще реализуются в ERP-системах. Работа с людьми, наверное, — последний бастион. И в этом смысле я полагаю уместным сравнение значимости методов анализа социальных структур в управлении организацией с ролью рентгеноскопии в травматологии — можно ведь изменять организации, руководствуясь интуицией и богатым жизненным опытом, а можно и померять реальное состояние дел.
Привет, воспроизводитель!
Ты можешь повторить ход исследования и измерить незадачу коммивояжера самостоятельно. Этот навык пригодится при встрече с главнокоммивояжером, и ты блеснешь пониманием состояния дел. Померяешь незадачу – поделись результатом и личными впечатлениями.
Кроме того, по-прежнему разыскивается самый главнокоммивояжер.
А если ты, читающий эти строки, дослужился до высоких рангов и руководишь большими проектами, то ищи датасаентиста и будет тебе счастие.
Дерзай!

Анонс: сцена четвертая, в которой мы вместе с Аристархом заглянем в /dev/null
(вот тот код, который делает всё описанное)
data artist
=====
Туториал по Unreal Engine. Часть 4: UI
2017-12-17, 10:24
Переводчик-фрилансер
=====
Блокчейн 101: книги, исследования и статьи по теме
2017-12-13, 13:44
Делаем хабрапосты и подкасты для компаний
=====
Интеллектуальные чат-боты на ChatScript: основы
2017-12-13, 15:15
Пользователь
=====
Зависимости наших зависимостей или несколько слов об уязвимости наших проектов
2017-12-14, 12:38

Эта история началась 30 ноября, утром. Когда вполне обычный билд на Test environment внезапно упал. Наверное, какой-то линтер отвалился, не проснувшись подумал я и был не прав.
Кому интересно чем закончилась эта история и на какие мысли навела – прошу под кат.
Открыв билд-лог, я увидел, что упал npm install. Странно подумал, я. Еще вчера вечером все работало отлично. После некоторого изучения логов, была найдена подозрительная строка:
Здесь я опять удивился. Опять же, еще вчера мы coffee-script на проекте не использовали и за ночь вряд ли что-то сильно изменилось. Быстрый просмотр package.json подтвердил, что никакой супостат ничего нового туда не добавлял. Значит, наверное, у нас обновилась какая-то зависимость, которая использует coffee-script. Но против этой идеи говорило то, что уже довольно давно я выставил строгие версии для всех зависимостей проекта и, как мне казалось, такого случиться не могло. Бесплодно поискав похожую проблему в интернете, я опять вернулся к мысли об обновившейся зависимости. Поэтому на коленке был набросан скрипт который обошел все package.json-ы в node_modules в поисках coffee-script. Таких зависимостей оказалось около 5-6 штук. Это еще больше укрепило мои подозрения, и не сильно долго размышляя я снес весь node_modules, а заодно и все dependencies кроме одной в локальном репозитории и запустил npm install снова. Процесс прошел успешно. Дальше, шаг за шагом была найдена та зависимость, которая и валила install.
Это оказалась karma-typescript у которой в транзитивной зависимости оказался "pad", который в свою очередь зависел от coffee-script. И тут я снова приуныл. Вариантов было немного. Или временно отключать тесты, или ждать фикса, или делать форк и чинить самому (причем не очень понятно, что же конкретно нужно чинить). Без особой надежды я отправился на Github создавать issue. Каково же было мое удивление, когда мне ответили буквально через 20 минут. Оказалось, что некоторый товарищ, решил обновить coffee-script пакет в npm и вместо того, чтобы объявить старый пакет устаревшим он просто сделал ему unpublish. 
На мое счастье, пользователь @ondrejbase уже предложил костыль временное решение которое мне помогло. И вся эта история закончилась относительно дешево. Но могло быть совсем не так.
Это была присказка. А теперь я предлагаю поговорить о зависимостях наших проектов и проблемах, которые они нам приносят.
Давайте начнём с простого.
Недавно я в очередной раз наткнулся на статью, для новичков которая начиналась со строки
npm install -g typescript
И не выдержал. По моему мнению это один из самых плохих советов который вы можете дать начинающему разработчику. Я серьезно. Вот проблемы, к которым приводит этот совет:
И все это происходит потому, большинство пособий начинается с npm install abc -g. Хотя можно поставить все локально и подключить в package.json как ./node_modules/.bin/tsc
Если вы соберетесь писать свой гайд, пожалуйста, вспомните эту статью и спасите мои нервы, а также нервы всех тех, кто будет ее использовать на практике.
N.B. Глобально устанавливать генераторы кода (create-react-app, create-angular-app) это нормально. Они один раз отработают и все. Кроме того, вам не понадобится ставить их заново, когда вы решите создать следующий репозиторий.
Давайте идти дальше. Установим create-react-app, и создадим базовое приложение. Заходим в package.json и что мы там видим?
"react": "^16.2.0"
Все (или почти все) знают, что значит символ ^. Он значит, что npm может установить любую версию старше или равной указанной, в пределах мажорного релиза. И что в этом плохого? Не хочется быть категоричным, но, как по мне этот подход тоже "не очень", и вот почему:
Давайте начнем с того, почему у нас сломался билд. Все зависимости были заданы жестко и тем не менее мы свалились. Несмотря на то, что версии наших основных зависимостей оставались прежними (напомню, никаких ^, ~ в package.json) их зависимости были не такими строгими. Мы не контролировали зависимости наших зависимостей, хотя и пытались. И, что самое неприятное такое поведение поощряется по умолчанию. Я не знаю кто и зачем это сделал, но он подложил большую свинью всем нам, а особенно тем, кто практикует continuous-integration.
Конечно, конкретно эту проблему исправить легко. Достаточно создать lock-file (например, с помощью команды npm shrinkwrap) или использовать yarn — пакетный менеджер который по умолчанию фиксирует все зависимости вашего проекта.
Однако это решает только часть проблемы. Остается еще одна, гораздо более опасная ее часть и имя ей unpublish. Если вы не сталкивались с этой проблемой до этого, то вот тут прекрасная статья которая показывает всю уязвимость современной веб разработки. В любой момент, умышленно или по неосторожности, ваш проект может перестать собираться только потому, что кто-то удалил свой пакет из npm. И сделать это отнюдь не сложно. Достаточно просто ввести команду unpublish. С этой бедой можно бороться. Но давайте будем честны перед собой? У кого из нас стоит свой локальный npm? А кто хотя бы задумывался об этом? Боюсь, что не так много. И я лишь надеюсь, что теперь вы предупреждены.
Кстати, если вы зайдете в мой репозиторий (не делайте этого, прошу), то увидите, что почти все мои проекты нарушают все, что было сказано выше. И это еще одно доказательство того, насколько проблема распространена. 
На этом у меня все, надеюсь было интересно и полезно. По странной традиции здесь должны быть какая-то реклама, но ее у меня нет, поэтому я просто передам спасибо коллегам, которые вместе со мной тушили этот пожарчик.
И приношу свои извинения за английские слова, но в русском эквиваленте они сильно режут глаз.
Full-Stack developer JS + .NET
=====
Итальянская забастовка роботов
2017-12-13, 16:39
User
=====
Иннополис глазами жителя Москвы
2017-12-14, 10:36
Пользователь
=====
Добавление специальных предложений для клиентов в почтовых сообщениях средствами Zentyal + Postfix + alterMIME
2017-12-13, 16:43
Системное администрирование
=====
Критика 1С
2017-12-13, 16:44
Просто дайте 1 раз в год 1 стабильный релиз. И чтобы конфигурации могли просить только стабильный релиз, но не более. Один раз в год все переустановить можно, это как раз идеальный баланс. Вышел на январские праздники, и не останавливая бизнеса, все быстренько переустановил. Да много фирм-разработчиков так делают — выпускают стабильный релиз и кучу релизов на свой страх и риск.

Удивительно, но факт!

Вполне нормально брать деньги за подписку ИТС каждый месяц, а релизы каждый месяц не выпускать. (Не, ну вдруг вы не знаете.)

Просто на заметку
Когда о серьезной проблеме рассказали в одном предложении, то наверняка, на нее даже внимания не обратят. По идее о серьезной проблеме нужно говорить много, чтобы оппонент понял, что она серьезная.


Пользователь
=====
«Я всегда должен быть на виду» — Интервью с Олегом Шелаевым из ZeroTurnaround (часть 1)
2017-12-14, 18:37
Организатор конференций для программистов
=====
Как перенести данные с VMware на OpenStack: DRaaS и миграция
2017-12-13, 17:53
Пользователь
=====
Vivaldi Sync — первое знакомство
2017-12-13, 17:18
Зрю в корень, жгу глаголом
=====
HP оставляет случайный кейлогер в драйвере клавиатуры ноутбука
2017-12-13, 17:31
User
=====
Понимаем и работаем с gulp
2017-12-13, 17:40
User
=====
Veritas Access 7.3: плюсы, минусы, подводные камни
2017-12-13, 17:57
Пользователь
=====
Туториал по созданию трекера криптовалют под андроид на Kotlin
2017-12-14, 15:07
Андроид разработчик / Фрилансер
=====
SOC for beginners. Как организовать мониторинг инцидентов и реагирование на атаки в режиме 24х7
2017-12-14, 15:29
Пользователь
=====
Сервис онлайн-касс АТОЛ Онлайн: API и интеграция с CMS
2017-12-13, 19:00
User
=====
Corona SDK — игра давилка насекомых (Crush)
2017-12-13, 19:56
User
=====
Mail for Good: как сообщество программистов помогает НКО
2017-12-14, 12:20
Строю реактивный ранец и конкурента Википедии
=====
Что-то не так с IDS сигнатурой
2017-12-21, 17:37
Пользователь
=====
Динамический звук на разрушаемых уровнях Rainbow Six: Siege
2017-12-25, 09:52
Переводчик-фрилансер
=====
Рожденный перехватывать трафик
2017-12-13, 23:44
Storycaster
=====
Это Спарта
2017-12-14, 00:23
Биоробот
=====
Как правильно чистить лук, или Почему разработка ПО выходит из-под контроля
2017-12-14, 08:54
Пользователь
=====
Построение процесса бизнес-анализа в проектах по разработке BI-приложений с продвинутой визуализацией
2017-12-14, 08:57
Пользователь
=====
Представляя функции как сервис — OpenFaaS
2017-12-19, 12:26
Open Source geek
=====
Эликсир для джавистов. Часть первая
2017-12-14, 10:23

Не так давно я решил изменить образ мышления с помощью изучения нового языка программирования. С самого начала карьеры я работал с Джавой, и сейчас стал ощущать необходимость в совершенно другой парадигме. Так я повстречал невероятный язык под названием Эликсир.
Рубистам хорошо знакомо название этого языка, а также, возможно, имя создателя – Джозе Валима. Однако у пришедших из более многословных языков шансы быть знакомыми с Эликсиром довольно низки.
Так я решил написать несколько постов, чтобы помочь джавистам быстрее понять как устроен Эликсир. С помощью сравнения этих двух языков будет проще постичь новый для вас мир. Через эти посты я расскажу о синтаксисе, о работе языка и о ключевых особенностях, которые делают Эликсир по-настоящему потрясающим!
На минутку забудем об объектах, классах и интерфейсах. Эликсир – компилируемый функциональный язык программирования, в основе которого лежит принцип иммутабельности. Достаточно запомнить три простых правила, чтобы понять, как же в нём всё устроено.
Какой бы язык вы ни изучали, начало всегда одно – «Hello world!». Что ж, и мы не будем изобретать велосипед.
Типы переменных в Эликсире определяются динамически во время выполнения программы. То есть нет необходимости явно задавать тип переменной при её объявлении, но при этом существуют определённые правила для каждого типа:
А теперь создадим список и добавим в него новый элемент. Вот так это будет выглядеть на Джаве:
Вспоминаем про первое правило – правило иммутабельности. Чтобы изменить значение какой-либо переменной, нужно заново привязать его к другой переменной. В примере ниже только первые три строчки изменяют значение элементов списка, далее уже создаётся копия с новыми значениями:
Небольшая подсказка по спискам в Эликсире: они хранятся в памяти как связные списки. Так как имеется только ссылка на голову списка, то чтобы добавить в список новый элемент, нужно вставить его в начало, заменив голову списка. Это гарантирует выполнение вставки за время O(1). Если же требуется добавить элементы в конец, то нужно, как показано выше, произвести сцепление текущего списка с новым элементом, обёрнутым в список. Но с этим поаккуратнее, операция не из простых, поскольку требует большое количество памяти для копирования целого списка и создания нового.
Ещё одна загадка – определение длины списка. Как вы могли догадаться, единственный способ это сделать – пройти весь список и посчитать количество элементов (сложность O(n)). Опять же, будьте осторожны.
Не переживайте, просто ваш разум привык видеть всё в императивном ключе. Кроме этого способа подсчёта элементов есть и другие, которые будут рассмотрены позже. А пока… давайте прислушаемся к слогану небезысвестной компании:
В отличие от методов в Джаве, которые могут иметь один из четырёх модификаторов доступа (public, default, protected или private), функции в Эликсире могут быть только двух типов (public или private). Определение публичных и приватных функций начинается со слов def и defp соответственно. Например:
Итак, функцию можно задавать различными способами, при этом не нужно указывать ключевое слово return. А всё потому, что при обращении к функции будет всегда возвращён результат последней команды внутри неё.
Попробуем снова добавить в список новый элемент, используя вышеприведённые функции. Поскольку в Джаве каждый объект является указателем, то при передаче объекта методу в качестве аргумента внутри метода используется созданная копия этого указателя. Это означает, что до тех пор пока значения списка не будут переприсвоены в addElement, любые изменения, происходящие в списке, будут отображены за пределами метода. В Эликсире всё совершенно иначе.
Помните второе правило? Функции идемпотентны! Всё, что происходит внутри функций, за исключением возвращаемого результата, остаётся там же. Это одна из основных особенностей Эликсира, позволяющая запускать код изолированно и достигать тем самым абсолютно новый уровень параллелизма. И вам даже не придётся беспокоиться о побочных эффектах своего кода.
Нельзя не упомянуть пайп-оператор |>. С его помощью можно легко и просто объединять функции в цепочки, передавая результат предыдущей функции следующей в качестве аргумента. Приведу простейший пример его использования:
Чтобы реализовать что-то в Джаве, сначала нужно создать класс, содержащий конструкторы, методы и переменные. В Эликсире же код состоит из модулей, объединяющих функции в одно целое. Выглядит всё это примерно так:
Можно провести параллель между модулями и функциями и статическими методами внутри класса. Здесь нет понятия экземпляра, поэтому каждый модуль и каждая функция «статичны». Различия в их использовании иллюстрирует следующий пример:
Если вас заинтересовал язык, то подписывайтесь на рассылку, начиная с понедельника будет неделя всего самого интересного – от всеобъемлющей информационной поддержки новичков до вакансий и опенсорса для искушенных разработчиков.
Спасибо за внимание!
Переводчик
=====
Запускаем контейнеры в Azure
2017-12-21, 10:56
User
=====
Искусственный интеллект трансформирует информационную безопасность, но не стоит ждать мгновенных перемен
2017-12-14, 12:02
Пользователь
=====
Криптография русского крестьянина
2017-12-14, 15:39












Переводчик-фрилансер
=====
Как быть тимлидом, если работаешь с людьми из разных продуктовых команд
2017-12-15, 09:18
Инженер разработчик
=====
История о том, как П и Х игру «поделили»
2017-12-14, 16:45
Разработчик и автор
=====
Поиск проблем производительности NodeJs приложения (с примерами)
2017-12-14, 13:56
Из-за однопоточной архитектуры Node.js важно быть настороже высокой производительности вашего приложения и избегать узких мест в коде, которые могут привести к просадкам в производительности и отнимать ценные ресурсы CPU у серверного приложения.
В этой статье речь пойдет о том, как производить мониторинг загрузки CPU nodejs-приложения, обнаружить ресурсоемкие участки кода, решить возможные проблемы со 100% загрузкой ядра CPU. 
К счастью, у разработчиков есть удобные инструменты для обнаружения и визуализации “хот-спотов” загрузки CPU.
В первую очередь, это профайлер, встроенный в Chrome DevTools, который будет связываться с NodeJs приложением через WebSocket (стандартный порт 9229).
Запустите nodejs-приложение с флагом --inspect 
(по умолчанию будет использоваться стандартный порт 9229, который можно изменить через --inspect=<порт>). 
Если NodeJs сервер в докер-контейнере, нужно запускать ноду с --inspect=0.0.0.0:9229 и открыть порт в Dockerfile или docker-compose.yml
Откройте в браузере chrome://inspect

Откроется окно инспектора, схожее со стандартным “браузерным” Chrome DevTools.
Нас интересует вкладка “Profiler”, в которой можно записывать CPU профайл в любое время работы приложения:

Возьмем для экспериментов простое приложение (можно склонировать отсюда), эксплуатирующее узкое место в либе cycle (используемой в другой популярной либе winston v2.x) для эмуляции JS кода с высокой нагрузкой на CPU.
Будем сравнивать работу оригинальной либы cycle и моей исправленной версии.
Установите приложение, запустите через npm run inspect. Откройте инспектор, начните запись CPU профайла. В открывшейся странице http://localhost:5001/ нажмите "Run CPU intensive task", после завершения (алерта с текстом “ok”) завершите запись CPU профайла. В результате можно увидеть картину, которая укажет на наиболее прожорливые ф-ии (в данном случае — runOrigDecycle() и runFixedDecycle(), сравните их %):

Другой вариант — использование встроенного в NodeJs профайлера для создания отчетов о CPU производительности. В отличие от инспектора, он покажет данные за все время работы приложения. 
Запустите nodejs-приложение с флагом --prof
В папке приложения будет создан файл вида isolate-0xXXXXXXX-v8.log, в который будут записываться данные о “тиках”.
Данные в этом файле неудобны для анализа, но из него можно сгенерировать человеко-читаемый отчет с помощью команды
node --prof-process <файл isolate-*-v8.log>
Пример такого отчета для тестового приложения выше тут
(Чтобы сгенерировать самому, запустите npm run prof)
Существуют также некоторые npm-пакеты для профайлинга — v8-profiler
, предоставляющий JS-интерфейс к API V8 профайлера, а также node-inspector (устарел после выхода встроенного в Chrome DevTools-based профайлера).
Предположим, так случилось, что в коде закрался бесконечный цикл или другая ошибка, приводящая к полной блокировке выполнения JS-кода на сервере. В этом случае единственный поток NodeJs будет заблокирован, сервер перестанет отвечать на запросы, а загрузка ядра CPU достигнет 100%. Если инспектор еще не запущен, то его запуск вам не поможет выловить виновный кусок кода.
В этом случае на помощь может прийти дебаггер gdb.
Для докера нужно использовать
--cap-add=SYS_PTRACE
и установить пакеты
apt-get install libc6-dbg libc-dbg gdb valgrind
Итак, нужно подключиться к nodejs процессу (зная его pid):
sudo gdb -p <pid>
После подключения ввести команды:
Я не буду вдаваться в подробности, что делает каждая команда, скажу лишь, что тут используются некоторые внутренние ф-ии движка V8.
В результате этого выполнение текущего блокирующего JS-кода в текущем “тике” будет остановлено, приложение продолжит свою работу (если вы используете Express, сервер сможет обрабатывать поступающее запросы дальше), а в стандартный поток вывода NodeJs-приложения будет выведен stack trace. 
Он довольно длинный, но в нем можно найти полезную информацию — стек вызовов JS функций. 
Находите строки такого вида:
Они должны помочь определить “виноватый” код.
Для удобства написал скрипт для автоматизации этого процесса с записью стека вызовов в отдельный лог-файл: loop-terminator.sh
Также см. пример приложения с его наглядным использованием.
Иногда вы не виноваты :)
Наткнулся на забавный баг в nodejs < v8.5.0 (проверил на 8.4.0, 8.3.0), который при определенных обстоятельствах вызывает 100% загрузку 1 ядра CPU.
Код простого приложения для повторения этого бага находится тут.
Смысл в том, что приложение запускает WebSocket-сервер (на socket-io) и запускает один дочерний процесс через child_process.fork(). Следующая последовательность действий гарантированно вызывает 100% загрузку 1 ядра CPU:
Причем приложение все еще работает, Express сервер отвечает на запросы.
Вероятно, баг находится в libuv, а не в самой ноде. Истинную причину этого бага и исправляющий его коммит в changelog’ах я не нашел. Легкое “гугление” привело к подобным багам в старых версиях: 
https://github.com/joyent/libuv/issues/1099
https://github.com/nodejs/node-v0.x-archive/issues/6271
Решение простое — обновить ноду до v8.5.0+.
Если в вашем серверном приложении есть ресурсоемкий код, изрядно нагружающий CPU, хорошим решением может стать вынесение его в отдельный дочерний процесс. Например, это может быть серверный рендеринг React-приложения. 
Создайте отдельное NodeJs-приложение и запускайте его из главного через child_process.fork(). Для связи между процессами используйте IPC-канал. Разработать систему обмена сообщениями между процессами довольно легко, ведь ChildProcess — потомок EventEmitter.
Но помните, что создавать слишком большое количество дочерних NodeJs процессов не рекомендуется.
Говоря о производительности, другой не менее важной метрикой является потребление RAM. Существуют инструменты и техники для поиска утечек памяти, но это тема для отдельной статьи.
Разработчик
=====
Торговая стратегия для торговли коинтегрированными парами акций
2018-01-31, 12:26






Инженер-математик
=====
Один+Один — благотворительный маркетплейс на Blockchain
2017-12-14, 14:27
Пользователь
=====
Японская поэзия на службе изучения английского: приложение для запоминания произношения слов
2017-12-19, 10:59
Пользователь
=====
5-минутный гид по эзотерическим языкам программирования: попробуем их классифицировать
2017-12-14, 14:49
User
=====
Ошибка на сайте… Что делать?
2017-12-14, 15:12
Пользователь
=====
Учим машину разбираться в языках
2017-12-15, 09:29
Автор
=====
Танчики в консоли, статья вторая: «Настало время всё переделать!»
2017-12-14, 14:46
Начинающий программист
=====
Туториал по Unreal Engine. Часть 5: Как создать простую игру
2017-12-19, 13:51
Переводчик-фрилансер
=====
Паттерн передачи scala.concurrent.Promise в актор: особенности использования и альтернативы
2017-12-14, 15:14
В процессе поддержки различных проектов я несколько раз попадал в ситуацию, при которой по причине неправильной работы с Promise возникали проблемы на продакшне. Причём паттерн этой самой неправильной работы всегда был один и тот же, но скрывался он в разных обличьях. Более того, ошибочный код был написан различными людьми. К тому же, ни в одной статье по работе с Promise я толком не нашёл упоминание проблемы, которую хочу осветить. Так что предполагаю, что многие забывают про проблему, про которую я расскажу.
Интересно почитать много примеров асинхронного кода на Scala, с промисами, фьючами и акторами? Добро пожаловать под кат!
Для начала, немного теории про Future в качестве вступления. Знакомые с этим типом из стандартной библиотеки Scala могут смело пропускать эту часть.
В Scala для представления асинхронных вычислений используется тип Future[T]. Пусть нам нужно вытащить из СУБД значение по ключу. Сигнатура синхронной функции для такого запроса выглядела бы примерно так:
Использовать её тогда можно было бы так:
У такого подхода есть недостаток: метод get() может быть блокирующим, причём ввиду возможной передачи данных по сети на относительно длительный срок. Попробуем сделать его неблокирующим, тогда трейт будет выглядеть так:
Не исключено, что для корректного переписывания реализации нам понадобится воспользоваться асинхронным драйвером для используемой нами СУБД. Для примера же напишем in-memory реализацию на мапе:
Воспользоваться полученным значением мы можем, например, следующим образом:
У Future есть несколько полезных асинхронных методов обработки значения, опишем вкратце некоторые из них:
Также стоит уточнить, что все эти методы принимают также параметр implicit ec: ExecutionContext, содержащий информацию о контексте выполнения фьюч, как можно догадаться по названию.
Более подробно про фьючи можно почитать, например, здесь. 
Итак, что же такое Promise? Фактически, это типизированный write-once контейнер, содержащий в себе фьючу:
У него есть множество методов для того, чтобы фьюча завершилась, например:
Таким образом, промисы можно использовать для создания фьюч.
Представим себе, что мы хотим реализовать собственный асинхронный API поверх готового асинхронного Java API на коллбеках. Поскольку результат, который хотим вернуть во фьюче, доступен только в коллбеке, мы не сможем использовать напрямую метод Future.apply(). Здесь нам и поможет Promise. В этом ответе на SO есть, казалось бы, отличный пример использования Promise в реальном мире:
Что ж, используем эту функцию в своём новом веб-сервисе, например, на Akka-HTTP. Для начала подключим в SBT зависимость:
И напишем код сервиса:
Примечание: метод complete() из Akka-HTTP умеет принимать Future[T], для этого импортируется futureMarshaller. Он ответит на HTTP-запрос после завершения фьючи.
А ещё мы решили зашедулить таск, который будет отсылать значение по некоторому ключу всем email-ам из нашей базы через некоторое HTTP API. Причём делает это в цикле: после завершения рассылки всем клиентам начинает делать это заново.
Выложили всё это в продакшн. Однако через пару дней к нам приходят консьюмеры нашего API и жалуются на периодические отваливания сервиса по таймауту. А ещё через три дня мы обнаруживаем, что таск перестал рассылать почту! В чём же дело?
В логе видим такие стектрейсы:
Выходит, что метод makeResponse() кинул эксепшн. Глядя на исходники makeHTTPCall(), можно заметить, что в таком случае фьюча, которую он возвращает, никогда не завершается!
Именно поэтому наш API отваливался по таймауту, а цикл рассылки писем перестал работать. Увы, в Scala мы не можем программировать, не думая о том, что любая функция может вернуть эксепшн, как хотят многие...
Итак, вспоминаем, что метод Try.apply() умеет перехватывать эксепшны и возвращать Success со значением, либо Failure с брошенным исключением. Пофиксим тело лямбды наивным способом и отправим на код ревью:
Впрочем, на ревью нам подсказывают, что у промиса есть метод complete(), который сам делает то же, что мы написали руками:
Итак, что мы узнали о Promise:
Возможно, кто-то скажет, что это надуманный пример и в реальной жизни никто не забывает закрывать промисы? Что ж, для таких скептиков у меня есть ответ в виде нескольких реальных багов/PR-ов в Akka, связанных с некомплитящимися в определённых ситуациях промисами:
Кроме того, не всегда всё так просто и очевидно, как в этом примере.
Знакомые с акторами могут пропустить эту часть.
Нам понадобится немного знаний об акторах Akka в дальнейшем в этой статье. Подключим модуль akka-actor в наш проект, пример для SBT:
Актор в Akka — это объект с некоторым поведением, асинхронно получающий сообщения. По дефолту поведение определяется в методе receive:
После создания актор доступен не напрямую, а через прокси под названием ActorRef. Через этот прокси можно асинхронно отправлять сообщения с помощью метода ! (алиас для tell), и эти сообщения будут обработаны методом, определяющим поведение актора. Сообщения должны быть сериализуемыми, поэтому часто для сообщений создают case object (при отсутствии параметров у сообщения) или case class. Актор может одновременно обрабатывать не больше одного сообщения, поэтому его можно использовать в том числе как примитив синхронизации.
Есть ещё один важный момент: актор может менять свою функцию поведения, то есть фактически, обработки сообщений. Для этого в акторе нужно вызвать метод context.become(newReceive), где newReceive — параметр типа Receive. После этого, начиная со следующего сообщения, начнётся обработка функцией newReceive вместо дефолтного receive.
Итак, перейдём к следующему примеру.
Пусть нам нужно написать клиента для какого-нибудь сервиса. Например, для букинга. Допустим, мы хотим уметь получать информацию об отеле по id.
Теперь нужно определить метод, который обратится к API букинга и обработает ответ. Для этого мы воспользуемся асинхронным HTTP-клиентом библиотеки Akka-HTTP. Подключим его зависимостью:
Наш метод хотят запускать с довольно большим RPS в течение недолгого времени, а время ответа не очень важно. У клиента Akka-HTTP есть особенность: он не даёт запускать в параллель больше, чем akka.http.host-connection-pool.max-connections запросов. Воспользуемся крайне простым решением: сделаем так, чтобы все запросы шли через актор, то есть в один поток (фактическое решение было немного сложнее, но это неважно для нашего примера). Поскольку мы хотим возвращать фьючу, создадим промис и передадим его в актор, а уже в акторе закомплитим его.
И снова после выкладывания либы всё поначалу шло хорошо, но затем нам прилетел баг-репорт с заголовком "Метод getHotel() возвращает незавершающиеся фьючи". Почему же это произошло? Выглядит, что мы всё предусмотрели, использовали метод completeWith() на всё тело лямбды… Тем не менее, при некоторых условиях фьюча всё же залипает.
Всё дело в том, что лямбда, переданная методу foreach(), запустится только при успешном завершении фьючи eventual. Таким образом, если эта фьюча сфейлилась (например, отвалилась сетка), промис никогда не закомплитится!
Можно предположить, что фикс относительно тривиален: вместо foreach() нужно использовать onComplete(), и в переданной ему лямбде обработать ошибку. Примерно так:
Это решит проблему с залипанием фьючи конкретно по причине сфейлившегося eventual, но не решает всех возможных проблем с залипанием фьючи, переданной таким образом.
Для простоты дальнейших рассуждений реализуем более простой и вместе с тем более общий пример с передачей промиса в актор для завершения фьючи:
Кстати говоря, конструкция вида тела funcion() нередко встречается, например, в исходном коде Akka и других библиотек. В той же Akka можно найти несколько десятков использований Promise, написанных по такому паттерну:
Пара примеров для наглядности:
При использовании этого паттерна есть по меньшей мере пара проблем.
В обоих случаях промис, очевидно, не будет завершён.
С одной стороны, можно сказать, что эти примеры излишне синтетические. С другой, компилятор никак не защищает нас от таких ошибок. Кроме того, про эту проблему нужно помнить не только для акторов, но и для передачи промисов в обычные асинхронные функции.
У нас всё хорошо, наши сервисы, использующие этот актор, отлично работают. Впрочем, мы решили в начале запустить сервис на одном сервере.
Но вот, компания растёт, число пользователей тоже, а вместе с ним и количество запросов к нашему актору. На пиках нагрузки актор перестал успевать за необходимое время справляться с потоком сообщений и мы решили заняться горизонтальным масштабированием: запускать AlwaysCompletesRequest на разных нодах в кластере. Для организации кластера нужно использовать akka-cluster, однако в статье для простоты не будем организовывать кластер, а просто обратимся к одному удалённому актору AlwaysCompletesRequest.
Нам нужно создать ActorSystem с поддержкой akka-remote на обеих JVM: обращающейся к актору и хостящей его. Для этого добавим в application.conf обоих сервисов такую конфигурацию:
Также необходимо добавить зависимость от akka-remote для обоих сервисов, пример для SBT:
Теперь создадим актор на сервере:
Затем получим его на клиенте:
И запустили мы сервер вместе с клиентом… И сразу же получили залипшую фьючу!
В логах видим следующее исключение:
Таким образом, при попытке отправить промис удалённому актору мы вполне предсказуемо получили ошибку сериализации промиса. На самом деле, даже если бы мы и смогли сериализовать и передать промис, закомплитился бы он только на удалённой JVM, а в нашей JVM так и остался бы залипшим. Таким образом, передача промиса в актор работает только при локальной передаче сообщений, то есть данный паттерн отправки промиса в актор плохо масштабируется.
Как самый очевидный вариант решения проблемы — фейлить промис по таймауту. Посредством Akka можно сделать это, например, следующим образом:
Методы Promise, которые мы рассматривали ранее, завершаются успешно, только если Promise не был завершён ранее. Если же эти методы вызываются уже после завершения промиса, они кидают IllegalStateException. Для случаев, когда необходимо попытаться завершить промис, когда он, вероятно, уже был завершён, есть методы, аналогичные рассмотренным, но с префиксом try в названии. Они возвращают true, если завершили промис самостоятельно; false, если промис был завершён раньше вызова этого метода.
Также, как вариант, можно фейлить фьючу по таймауту прямо внутри актора:
Разумеется, этот вариант никак не решает проблему масштабируемости.
Можно было, конечно, сделать и иначе. Например, воспользоваться ask-паттерном, который требует передачу таймаута:
В таком случае и реализация актора должна быть несколько иной: вместо завершения промиса нужно ответить на сообщение:
Впрочем, лёгкость реализации таит за собой опасности. 
Как было сказано ранее, ask-паттерн требует передачу таймаута, благодаря чему фьюча не зависнет, даже если актор никогда не ответит на сообщение. Однако классические акторы Akka при использовании ask-паттерна возвращают Future[Any]. Разумеется, это приводит к необходимости небезопасного приведения типа в рантайме, как можно было заметить в предыдущем примере:
Таким образом, если актор ответит сообщением типа, отличного от String, фьюча сфейлится. В качестве экспериментального решения данной проблемы можно использовать экспериментальный же модуль Akka Typed. Подключим его:
Теперь напишем ещё один актор, на сей раз типизированный:
Теперь наша фьюча типобезопасна. Впрочем, соображения дизайна и перформанса остаются в силе, как и в пункте 2. Кроме того, мы можем и не хотеть использовать в продакшн-коде API с пометкой "может измениться".
В случае использования рассматриваемого паттерна с акторами проблема возникает, фактически, из-за того, что мы должны обращаться к актору извне ActorSystem. Можно сделать примерно такое изменение:
При прямом взаимодействии акторов друг с другом необязательно заворачивать ответ в промис, а можно просто вернуть его в ответном сообщении. Однако для консьюмеров API часто удобнее просто получить фьючу, чем разбираться с протоколом взаимодействия с нашим актором.
В исходном коде Akka этот паттерн часто используется, что в некоторой степени показывает его жизнеспособность и удобность. Если мы осознанно используем его, следует помнить о рекомендации, которую желательно применять всегда, но особенно важно в данной ситуации: близкое к 100%-ному, а ещё лучше 100%-ное покрытие кода, связанного с промисами, обязательно. Компилятор не может сообщить нам о незакрытом ресурсе, поэтому придётся весьма агрессивно тестировать, что мы всегда сами закрываем его руками. Кроме того, необходимо осознавать, что мы не сможем передавать промис актору по сети, что может привести к проблемам при масштабировании.
Если мы всё же решаем использовать рассматриваемый паттерн, в таком случае бывает полезно выделять часть кода, в которой мы с высокой долей вероятности уверены, что она вернёт завершающуюся фьючу. Например, для создания фьючи используются только сторонние библиотеки с малой вероятностью содержания багов в месте вызовов. Для примера отрефакторим код актора, обращающегося к букингу:
Этот код имеет следующие преимущества относительного старого:
Этот подход часто применим при передаче промиса в другие асинхронные функции, а не только в actor.tell().
Теперь сравним рассмотренные методы по нескольким критериям:
Кроме того, хотелось бы уточнить, что метод с таймаутом относительно метода без него имеет также недостаток в виде немного усложнённого кода.
В заключение хочется сказать, что хотелось бы иметь некую конструкцию, которая будет работать с промисами, переданными в актор, как ресурсами. Впрочем, с этим есть множество проблем, например:
Возможно, было бы реально хотя бы в некоторых случаях выявлять баги посредством статического анализа кода.
Если вам что-либо известно о таких или иных способах борьбы с проблемами паттерна, расскажите о них в комментариях!
Scala-разработчик
=====
Развертываем Parallels RAS в Microsoft Azure за полчаса
2017-12-14, 16:02
Пользователь
=====
Руководство по написанию защищённых PHP-приложений в 2018-м
2017-12-15, 13:30
Приближается 2018 год, и технари — в частности веб-разработчики — должны отбросить многие старые методики и верования в сфере разработки защищённых PHP-приложений. Особенно это относится ко всем, кто не верит, что такие приложения вообще возможны. 
Это руководство — дополнение к электронной книге PHP: The Right Way с сильным уклоном в безопасность, а не общие вопросы программирования на PHP (вроде стиля кода).
Вкратце: ничего не поделаешь, но в 2018-м вы будете пользоваться PHP 7.2, а в начале 2019-го — планировать перейти на 7.3.
PHP 7.2 вышел 30 ноября 2017 г.
На момент написания статьи только PHP 7.1 и 7.2 активно поддерживаются разработчиками языка, а для PHP 5.6 и 7.0 ещё примерно год будут выходить патчи безопасности.
В некоторых ОС есть долговременная поддержка уже не поддерживаемых версий PHP, но это считается в целом порочной практикой. Например, бэкпортирование патчей безопасности без инкрементирования номеров версий затрудняет оценку системы безопасности только по версии PHP.
Соответственно, вне зависимости от обещаний вендоров всегда старайтесь использовать только активно поддерживаемую версию PHP, если это возможно. Если даже вы какое-то время будете работать с версией, для которой выходят только патчи безопасности, регулярные обновления версий избавят вас от многих неприятных сюрпризов.
Вкратце: используйте Composer.
Composer — это шедевральное решение по управлению зависимостями в PHP-экосистеме. В книге PHP: The Right Way целый раздел посвящён началу работы с Composer, очень рекомендуем его прочесть.
Если вы не пользуетесь Composer для управления зависимостями, то рано или поздно (надеюсь — поздно, но, скорее всего, рано) окажетесь в ситуации, когда одна из библиотек, от которой вы зависите, сильно устареет, а преступники начнут активно эксплуатировать уязвимости в старых версиях.
Важно: не забывайте обновлять свои зависимости по мере разработки ПО. К счастью, это можно сделать одной строкой:
Если вы делаете что-то особенное, требующее использования PHP-расширений (написанных на С), то вы не можете установить их с помощью Composer. Вам также потребуется PECL.
Вне зависимости от того, что вы создаёте, наверняка эти зависимости будут вам полезны. Это в дополнение к тому, что рекомендует большинство PHP-разработчиков (PHPUnit, PHP-CS-Fixer и т. д.).
Пакет Roave security-advisories использует репозиторий Friends of PHP, чтобы ваш проект не зависел от любых пакетов с известными уязвимостями.
Или можете загрузить свой файл composer.lock в Sensio Labs в качестве стандартной процедуры автоматической оценки на уязвимости, чтобы получать предупреждения о любых устаревших пакетах.
Psalm — инструмент статичного анализа, помогающий определять возможные баги в вашем коде. Хотя есть и другие хорошие инструменты (например, замечательные Phan и PHPStan), но если вам нужна поддержка PHP 5, то Psalm — один из лучших инструментов статичного анализа для PHP 5.4+.
Использовать Psalm просто:
Если вы впервые применяете этот код к имеющейся базе данных, то увидите много красных отметок. Если вы не создаёте приложение масштаба WordPress, то маловероятно, что вам придётся совершить подвиг Геркулеса, чтобы пройти все эти тесты.
Вне зависимости от того, какой инструмент статичного анализа вы выбрали, рекомендуем внедрить его в рабочий процесс непрерывной интеграции (если это возможно), чтобы инструмент запускался после каждого изменения кода.
Вкратце: HTTPS, который нужно тестировать, и заголовки безопасности.
В 2018-м сайтам уже будет непозволительно работать по незащищённому HTTP. К счастью, можно было бесплатно получить TLS-сертификаты и автоматически обновлять их благодаря протоколу ACME и сертификационной компании Let's Encrypt.
Интегрировать ACME в свой веб-сервер — пара пустяков.
Вы могли подумать: «Ладно, у меня есть TLS-сертификат. Теперь нужно потратить несколько часов на поиск конфигурации, чтобы сайт стал безопасным и быстрым».
Нет! Mozilla вам поможет. Для создания рекомендованных шифронаборов для своей аудитории можете использовать генератор конфигураций.
HTTPS (HTTP через TLS) совершенно безальтернативен, если хотите сделать свой сайт безопасным. Использование HTTPS моментально исключает несколько видов атак на ваших пользователей (внедрение контента «человек посередине», перехват данных, атаки повтором и манипуляции с сессиями ради подмены пользователя).
Хотя применение HTTPS на вашем сервере даёт много преимуществ по безопасности и производительности, можно пойти ещё дальше и воспользоваться другими функциями браузера по повышению безопасности. Большинство из них подразумевает отправку с контентом заголовков HTTP-ответов.
Аналогично, если вы используете встроенные PHP-свойства управления сессиями (что рекомендуется), то, возможно, захотите вызвать session_start():
Тогда ваше приложение при отправке идентификационных кук будет использовать только безопасные HTTPS-флаги, что предотвратит успешные XSS-атаки с помощью кражи пользовательских кук, они будут отправляться только по HTTPS. Пару лет назад мы уже писали о безопасных PHP-сессиях. 
Однажды в будущем вы станете работать над проектом, использующим CDN для выгрузки традиционных Javascript/CSS-фреймворков и библиотек в центральное расположение. Неудивительно, что специалисты по безопасности предсказали очевидную проблему: если много сайтов используют CDN для предоставления части своего содержимого, то взлом CDN и подмена данных позволит внедрять произвольный код на тысячи (если не миллионы) сайтов.
Поэтому придумали целостность подресурсов (subresource integrity).
Целостность подресурсов (SRI) позволяет закреплять хеш содержимого файла, которое вам должна предоставить CDN. Текущая реализация SRI позволяет использовать только криптографические хеш-функции, поэтому злоумышленники не смогут сгенерировать вредоносные версии контента, приводящие к таким же хешам, как у оригинальных файлов.
Реальный пример: Bootstrap v4-alpha использует SRI в примере кода их CDN.
Веб-разработчики часто задают гиперссылкам атрибут target (например, target="_blank" для открытия ссылки в новом окне). Но если вы не передаёте также тэг rel="noopener", то позволите целевой странице получить контроль над исходной страницей.
Не делайте так
Это позволит example.com получить контроль над текущей страницей.
Делайте так
В новом окне открывается example.com, но не получает контроля над текущим окном.
Для дальнейшего изучения.
Если безопасность ПО для вас в новинку, можете начать с введения A Gentle Introduction to Application Security.
Многие специалисты по безопасности с самого начала обращают внимание разработчиков на ресурсы вроде OWASP Top 10. Но многие распространённые уязвимости можно считать особыми случаями одних и тех же высокоуровневых проблем (код/данные не разделены адекватно, ошибочная логика, небезопасная операционная среда, сломанные криптографические протоколы).
Мы считаем, что, если прививать неофитам в безопасности более простое, более фундаментальное представление о проблемах безопасности и их решениях, это поможет в долгосрочной перспективе улучшить ситуацию с безопасностью.
Подробнее: Предотвращение SQL-внедрений в PHP-приложениях
Если вы сами пишете SQL-запросы, проверьте, что вы используете подготовленные выражения (prepared statements) и что любая предоставляемая сетью или файловой системой информация передаётся в виде параметров, а не конкатенируется в строку запроса. Также удостоверьтесь, что вы избегаете эмулированных подготовленных выражений.
Лучший выбор — EasyDB.
НЕ ДЕЛАЙТЕ ТАК:
Делайте так:
В базах данных есть и другие слои абстракций, предоставляющие эквивалентный уровень безопасности (EasyDB под капотом использует PDO, но любыми способами старается отключить эмулирование подготовленных выражений в пользу настоящих подготовленных выражений, чтобы предотвратить проблемы). Пока вводимые пользователями данные не могут влиять на структуру запросов (это относится и к хранимым процедурам) — вы в безопасности. 
Подробнее: Как безопасно разрешать пользователям загружать файлы
Принимать пользовательские файлы рискованно, но это можно делать безопасно, если принять ряд предосторожностей. В частности, закрыть прямой доступ к загружаемым файлам, чтобы они не могли быть исполнены или интерпретированы.
Загружаемые файлы должны иметь атрибуты «только для чтения» или «только для чтения или записи» и никогда не быть исполняемыми.
Если на вашем сайте корневая директория для документов /var/www/example.com, то не надо хранить загружаемые файлы в /var/www/example.com/uploaded_files.
Лучше хранить их в отдельной директории, к которой нет прямого доступа (например, /var/www/example.com-uploaded/), чтобы они случайно не выполнялись как серверные скрипты и не открывали дверь удалённому исполнению кода.
Более чистое решение — переместить свою корневую директорию для файлов на один уровень вниз (т. е. в /var/www/example.com/public).
Другая проблема загружаемых файлов связана с их безопасным скачиванием.
Подробнее: Всё, что вам нужно знать о предотвращении межсайтового скриптинга в PHP
В идеальном мире предотвратить XSS было бы так же легко, как и SQL-внедрение. У нас был бы простой в использовании API для отделения структуры документа от его содержимого.
К сожалению, в реальном мире большинство веб-разработчиков генерируют длинный HTML и отправляют его в HTTP-ответе. Это характерно не только для PHP, просто таковы реалии веб-разработки. 
Закрытие XSS-уязвимостей — задача вполне решаемая. Однако содержимое раздела о браузерной безопасности неожиданно обретает большую важность. Вкратце:
Подделка межсайтовых запросов — это разновидность атаки с подменой делегата: можно обмануть пользовательский браузер и заставить его выполнить вредоносный HTTP-запрос с повышенными пользовательскими привилегиями.
В целом эта проблема легко решается:
Мы написали библиотеку Anti-CSRF, которая идёт ещё дальше:
Если ваш фреймворк не заботится о CSRF-уязвимостях, то применяйте Anti-CSRF.
В скором будущем куки SameSite позволят прекращать CSRF-атаки с гораздо меньшими усилиями.
Есть две основные уязвимости, проявляющиеся в приложениях, которые много обрабатывают XML:
XXE-атаки, помимо прочего, могут использоваться как стартовая площадка для эксплойтов локального/удалённого внедрения файлов.
Ранние версии Google Docs были уязвимы к XXE-атакам, но они мало известны за пределами бизнес-приложений, обрабатывающих большие объёмы XML.
Главное, что нужно сделать для защиты от XXE-атак:
XPath-внедрение очень похоже на SQL-внедрение, только здесь речь идёт об XML-документах.
К счастью, в PHP-экосистеме редко возникают ситуации, когда вводимые пользователями данные передаются в XPath-запросе.
К сожалению, это также означает, что лучшее доступное решение (для заранее скомпилированных и параметризованных XPath-запросов) в PHP-экосистеме отсутствует.
Рекомендуем использовать белые списки разрешённых символов для любых данных, имеющих отношение к XPath-запросам.
Белые списки безопаснее чёрных.
Подробнее: Безопасная (де)сериализация в PHP
Если вы передаёте в unserialize() недоверенные данные, то напрашиваетесь на два варианта развития событий:
Многие разработчики предпочитают использовать вместо этого JSON-сериализацию, что является заметным улучшением безопасности ПО. Но имейте в виду, что json_decode() уязвима для DDoS-атак посредством хеш-коллизий. К сожалению, полное решение проблемы хеш-DOS в PHP ещё предстоит найти.
Полностью защититься от этих атак поможет мигрирование с djb33 на Siphash с назначением 1 в качестве старшего бита для хеша строкового входного значения, 0 для целочисленного и с заранее запрошенным ключом, его предоставит CSPRNG.
К сожалению, создатели PHP не готовы частично пожертвовать производительностью, которой они добились в PHP 7, поэтому трудно убедить их отказаться от djb33 (очень быстрого, но небезопасного) в пользу SipHash (тоже быстрого, хотя и не как djb33, но куда более безопасного). Значительное снижение производительности может даже помешать разработке будущих версий, что не пойдёт на пользу безопасности.
Поэтому лучше поступать так:
Подробнее: Как в 2016-м безопасно хранить пользовательские пароли
Безопасное хранилище паролей раньше было темой активной дискуссии, но сегодня его просто реализовать, особенно в PHP:
Вам даже не нужно знать, какой там алгоритм, потому что если вы используете самую свежую версию PHP, то будете использовать и самые последние технологии, а пользовательские пароли будут автоматически обновлены, как только окажется доступен новый алгоритм по умолчанию.
Что бы вы ни делали, не делайте так, как WordPress.
Если интересно: с PHP 5.5 по 7.2 алгоритмом по умолчанию является bcrypt. В будущем его могут заменить Argon2, победителем в Соревновании по хешированию паролей.
Если до этого вы не использовали API password_* и вам нужно мигрировать легаси-хеши, то сделайте это именно так. Многие компании, например Yahoo, поступили неправильно. Похоже, недавно причиной бага с iamroot у Apple стала некорректная реализация обновления легаси-хешей.
Мы много писали на эту тему:
Для криптографии на уровне приложения всегда выбирайте библиотеку Sodium (libsodium). Если вам нужно поддерживать PHP ниже 7.2 (вплоть до 5.2.4), можете использовать sodium_compat и притвориться, что пользователи тоже применяют 7.2.
В особых случаях из-за выбранных алгоритмов и взаимозаменяемости вам могут понадобиться другие библиотеки. Если сомневаетесь, проконсультируйтесь у криптографа по поводу выбора шифра и у инженера по шифрованию по поводу безопасности реализации.
Вы получили представление, как в 2018-м нужно создавать защищённые PHP-приложения. Давайте теперь рассмотрим некоторые специфические случаи.
Подробнее: Building Searchable Encrypted Databases with PHP and SQL
Многим хочется иметь шифрованные базы данных с возможностью поиска, но считается, что их трудно реализовать. По ссылке выше вы найдёте статью, в которой мы последовательно ведём читателя по разработке такой БД. В частности:
На любом шаге вы можете идти на компромиссы в зависимости от того, что в вашем случае оправдано.
Подробнее: Сплит-токены: протоколы аутентификации на основе токенов без побочных каналов
Если говорить о базах данных (предыдущий раздел): вы знали, что запросы SELECT теоретически могут быть источником утечек информации о тайминге?
Простые меры защиты:
Даже если воспользоваться утечкой данных о тайминге для кражи половины токена, оставшаяся половина потребует брутфорс-атаки.
Подробнее: С помощью Sapient закаливаем ваши PHP API
Мы написали SAPIENT, Secure API ENgineering Toolkit, чтобы упростить задачу межсерверного аутентификационного обмена сообщениями. Sapient позволяет шифровать и/или аутентифицировать сообщения с помощью шифрования на основе общего (shared) или публичного ключа в дополнение к средствам безопасности HTTPS.
Это позволяет с помощью Ed25519 аутентифицировать и отвечать на API-запросы или шифровать сообщения для целевого сервера, которые можно расшифровать лишь с помощью секретного ключа на принимающем сервере, даже несмотря на атаку «человек посередине» в сочетании с фальшивой/скомпрометированной сертификационной организацией.
Поскольку тело каждого HTTP-сообщения аутентифицируется с помощью безопасного шифрования, его можно использовать вместо протоколов, оперирующих токенами с проверкой состояния (например, Oauth). Но если говорить о самом шифровании, то прежде чем делать что-то нестандартное, всегда нужно быть уверенными в том, что выбранный вами алгоритм проанализирован специалистами.
Вся используемая в Sapient криптография предоставлена шифровальной библиотекой Sodium.
Дополнительно почитать:
Paragon Initiative Enterprises уже использует Sapient во многих своих продуктах (включая open source проекты) и продолжит расширять портфолио пользователей Sapient.
Подробнее: Chronicle заставит задуматься, нужна ли вам технология блокчейна
Chronicle — криптографический журнал, обновляемый только путём добавления новых записей. Он основан на использующей хеш-цепочки структуре данных, свойства которой, безо всяких излишеств, привлекают многие компании в стан технологии «блокчейна».
Помимо более изощрённых способов применения такого журнала, Chronicle превосходно себя проявляет в SIEM, поскольку вы можете отправлять важные с точки зрения безопасности события в личный журнал, после чего они становятся неизменяемыми.
Если ваш Chronicle настроен на перекрёстную запись (cross-sign) суммарного хеша в другие экземпляры Chronicle и/или если есть другие экземпляры, сконфигурированные на репликацию содержимого вашего Chronicle, то атакующему будет крайне сложно подделать ваши журналы событий безопасности.
С помощью Chronicle вы получите надёжность блокчейна без распространённых проблем с приватностью, производительностью или масштабируемостью.
Для публикации данных в локальный Chronicle можно использовать любой API, совместимый с Sapient, но самое простое решение — Quill.
Проницательный читатель мог заметить, что мы много ссылаемся на собственные работы (статьи и open source проекты), но мы ссылаемся не только на свои работы.
Это неслучайно.
Наша компания с самого основания в начале 2015-го пишет библиотеки для обеспечения безопасности и участвует в повышении защищённости экосистемы PHP.
Мы много путешествуем, и наш инженер по безопасности (чьи недавние усилия по использованию более сильной криптографии в ядре PHP только что отразились в PHP 7.2), по его собственному признанию, не слишком силён в генерировании хайпа или интереса к своей работе. Наверняка вы не слышали и о половине инструментов или библиотек, созданных нами за эти годы. 
Но мы не можем стать пионерами во всех направлениях, поэтому везде, где это возможно, связываемся с экспертами индустрии, которые, как нам кажется, больше ориентируются на общественное благо, чем на мелкий эгоизм. Поэтому большая часть раздела, посвящённого безопасности в браузере, снабжена ссылками на работы Скотта Хелме (Scott Helme) и компании. Он вложил много сил в то, чтобы эти новые возможности по обеспечению безопасности стали доступны и понятны разработчикам.
Конечно, это не исчерпывающее руководство. Существует почти столько же способов писать небезопасный код, сколько способов самого написания кода. Безопасность — это больше мышление, чем цель. Мы надеемся, что с помощью всего сказанного и приведённых ниже источников разработчики с сегодняшнего дня смогут создавать защищённые PHP-приложения.
Если вы уже изучили всё предложенное и хотите больше, почитайте курируемый нами список по изучению безопасности приложений.
Если вы считаете, что адекватно пишете безопасный код, и хотите покритиковать нас с точки зрения инженера по безопасности, то именно такую услугу мы и предлагаем своим клиентам.
Если вы работаете в компании, которая заинтересована в оценке соответствия требованиям (PCI-DSS, ISO 27001 и т. д.), можете нанять нас для аудита своего исходного кода. Мы работаем гораздо более дружественно к разработчикам, чем другие консультанты по безопасности.
Ниже — список источников от PHP-сообщества и сообщества по информационной безопасности.
¯\_(ツ)_/¯
=====
АТОЛ Онлайн: взгляд из ЦОД
2017-12-19, 11:24
User
=====
Что нужно знать, чтобы стать системным архитектором
2017-12-19, 10:43
Облачная платформа
=====
Установка HA Master Kubernetes кластера с помощью Kubespray
2017-12-14, 16:28
Пользователь
=====
6 привычек проектного бизнеса, которые убивают продуктовый
2017-12-15, 14:34
Пользователь
=====
IoT в роли мотиватора для NAT в IPv6
2017-12-14, 16:33
User
=====
Война клонов или как привлекать таланты
2017-12-14, 18:30
Менеджер
=====
Проблемы локализации iOS и macOS
2017-12-14, 16:54

Любая успешная компания, занимающаяся разработкой продуктов, ориентированных на массового пользователя, рано или поздно сталкивается с проблемой их локализации. Под ней я понимаю куда более сложную задачу, чем банальный перевод программных интерфейсов на другой язык. Ведь настоящая локализация должна учитывать максимально возможный спектр особенностей, влияющих на пользовательский опыт в той или иной стране.
К сожалению, у Apple с локализацией наблюдаются реальные проблемы. В этой статье я хочу рассмотреть ряд примеров, на которых становится очевидным недостаточно качественный подход компании к адаптации iOS и macOS под российского пользователя. Я бы даже сказал, местами откровенно наплевательский.
Это моя первая публикация на Хабре, надеюсь, вам понравится.

На голосовую команду активации Siri в iOS не жаловался только самый ленивый.
В оригинале, то есть на английском языке, Apple использует для этого фразу “Hey Siri”, что переводится как «Эй, Siri» и не вызывает никаких претензий.
Эй, Siri, какая на улице погода?
Эй, Siri, ещё вопрос, сколько рублей в тысяче долларов?
Но в случае с русской локализацией неизвестные светлые умы решили, что «эй» звучит недостаточно вежливо, поэтому лучше будем здороваться с голосовым ассистентом при каждом обращении, даже если делаем это по двадцать раз в минуту.
Привет, Siri, какая на улице погода?
Привет, Siri, ещё вопрос, сколько рублей в тысяче долларов?
Со стороны такой диалог выглядит глупо. Но куда хуже тот факт, что выбранная Apple для русского языка команда разбивает о бетонную стену всю идею голосового ассистента, который по возможности должен быть максимально похож на живого личного помощника. Ведь мы не здороваемся с человеком каждый раз в течение дня, когда хотим у него что-то спросить.

Как поставить точку или запятую? Для пользователя обычного ПК либо англоязычного человека такой вопрос может показаться глупым. Нажми соответствующую клавишу, и готово.
На Mac это делается не так. Точка ставится через сочетание «Shift + 7», а запятая по «Shift + 6», зато свободная клавиша рядом с правым Shift вводит «/» либо «?». Назвать такое решение допустимым язык не поворачивается. В русском языке точки и запятые ставятся постоянно, и пользователь вынужден через каждые полтора введённых слова тянуться двумя руками к этим совершенно неудобным шорткатам.
Единственное относительно простое решение проблемы — это открыть системные настройки macOS и заменить русскую раскладку Apple на предназначенную для ПК, однако в таком случае половина символов, изображённых на физической клавиатуре компьютера, перестаёт совпадать с их фактическим расположением.

Изменение текущего языка ввода с помощью «Alt + Shift» либо «CTRL + Shift» всё-таки не является удобным. Клавиши это зачастую достаточно маленькие, а для их нажатия приходится выгибать пальцы неестественным образом. Отсюда вечная проблема пользователей Windows с тем, что раскладку вроде бы переключил, написал кучу текста, а потом выяснилось, что получилась лишь ненавистная абракадабра.
В случае же с macOS это исторически было крайне удобное сочетание «⌘ + Пробел». Клавиши большие, расположены для руки удобно, нажатие срабатывает всегда железобетонно. Однако в обновлении системы до El Capitan (10.11) внезапно Apple решила поменять местами шорткаты для переключения клавиатурных раскладок и для открытия поиска Spotlight, то есть теперь язык ввода меняется через «Control + Пробел».
С точки зрения англоязычных пользователей, которыми несомненно являются сотрудники и руководство Apple, это прекрасное решение. Раскладка в системе как правило одна — английская, зато поиском пользуются многие и часто. 
Однако для людей из всех неанглоговорящих стран мира это настоящее издевательство. Как ни крути, переключение раскладок через «Control + Пробел» ещё хуже, чем способ, реализованный в Windows. Ведь клавиша Control на клавиатурах Apple не только сравнительно мелкая, но ещё и расположена относительно пробела так, что применить нужное сочетание клавиш можно только неведомым образом раскорячив пальцы.
К счастью, проблема достаточно легко фиксится через системные настройки. Но разве это нормальное решение, заслуживающее оправданий?

С последним мажорным обновлением iOS была представлена замечательная функция быстрого ввода пунктуации свайпами по клавишам программной клавиатуры iPad, но с русским языком ожидаемо возникла печаль.
Как мне ввести с такой клавиатуры вопросительный либо восклицательный знак? Никак. Зато без проблем доступны номер, рубль, звёздочка, андерскор, решётка и другие.

Кто-нибудь может сказать, что это, конечно, неудачно, но ведь можно по старинке переключиться на ввод символов. Не тут-то было. Даже здесь они доступы лишь по свайпу. 
Зато простым и лёгким тапом можно ввести амперсанд, а также наши любимые номер, рубль, звёздочку, андерскор, решётку и другие.

Напоследок упомяну и просто usability-проблему, не связанную с локализациями. Apple приучила нас переключать раскладки по шорткату, однако на физической Smart Keyboard для iPad Pro вынесла для этого отдельную клавишу. И как я ни стараюсь, постоянно вместо нажатия на неё машинально жму сочетание клавиш из macOS, после чего лишь обретаю разочарование.

Не знаю точно, как на западе, но в России с Wi-Fi много проблем. Публичные сети встречаются относительно редко, требуют авторизации, работают медленно, нередко ограничивают доступ к половине ресурсов в сети. Зато мобильный интернет и LTE в частности распространились весьма широко и стоят относительно недорого.
Но как бы ни хотелось, в 2017 году я не могу установить на свой iPhone или iPad по мобильной сети приложение объёмом более 100 мегабайт. Хоть ты тресни. Почему, зачем? Отчего бы не дать пользователю возможность самостоятельно принять решение? Apple молчит.
Здесь налицо тот факт, что компания при адаптации macOS и iOS к российским реалиям не интересуется обратной связью от пользователей. Ведь на отечественных ресурсах по этому поводу жаловались уже бесчисленное количество раз.
Не исключено, что я преувеличиваю масштаб тех или иных недостатков. Но ведь Apple на каждом шагу говорит о том, что её продукты идеальны до мелочей. И дух Стива Джобса, конечно, не должен бы закрывать глаза на подобное. Так что проблемы есть, и это факт.
Мораль сей басни такова, при создании качественных продуктов, ориентированных на интернациональный рынок, помните о том, что простой перевод интерфейсов на другой язык совсем не означает надлежащей адаптации для представителей других культур, языков и стран.
Разработчик
=====
й
2017-12-14, 17:42
Разработчик
=====
Разработчик-детектив: занимательные задачки из реальной жизни
2017-12-14, 18:02
User
=====
Big Data в Hadoop по подписке в облаке SAP
2017-12-19, 12:36
Пользователь
=====
Анализ резюме hh.ru: много графиков и немного сексизма и дискриминации
2017-12-18, 10:21
User
=====
Краткий обзор Symfony. Актуальность. Стоит ли попробовать?
2017-12-14, 19:33
Редактор
=====
Расширение моделей в Eloquent ORM
2017-12-14, 20:14

Мы прошли долгий путь, с тех дней когда мы в ручную писали SQL запросы в наших веб приложения. Инструменты, такие как Laravel’ий Eloquent ORM позволяют нам работать с базой данных на более высоком уровне, освобождают нас от деталей более низкого уровня — таких как синтаксис запросов и безопасность. 
Когда вы начнете работать с Eloquent, вы неизбежно придете к таким операторам как where и join. Для более продвинутых есть заготовки запросов (scopes), читатели (accessors), мутаторы (mutators) — предлагающие более выразительные альтернативы, старому способу построения запросов.
Давайте рассмотрим другие альтернативы, которые могут быть использованы как замена часто повторяющемуся оператору where и заготовкам запросов (scopes). Эта технология заключается в создание новой модели Eloquent которая будет наследоваться от другой модели. Такая модель будет наследовать весь функционал родительской модели, сохраняя возможность добавлять собственные методы, заготовки запросов (scopes), слушателей (listeners), и т.д. Обычно такое называется "Однотабличное Наследование" (Single Table Inheritance), но я предпочитаю называть это "Модельным Наследованием" (Model Inheritance).
Большинство веб приложений имеют концепцию "администратор." Администратор это обычный пользователь с повышенными правами и доступом в служебные части приложения. Для того чтобы отличить обычных пользователей от администраторов мы пишем что то подобное:
Когда выражение where часто повторяется в вашем приложение, его полезно заменить на локальную заготовку запроса (local scope). Внедрив заготовку запроса isAdmin в модель User, мы сможем писать более выразительный и переиспользуемый код:
Давайте пойдем дальше и используем наследование модели. Наследуясь от модели User и добавляя глобальную заготовку запроса, мы достигаем более аккуратного результата чем получали прежде, но сейчас с совершенно новым объектом. Этот объект (Admin) может иметь собственные методы, заготовки запросов, и другие функциональные возможности.
Теперь когда у вас есть модель Admin вам будет проще разделять функциональность с моделью User. Например:
Простые операции, такие как отправка нотификаций всем администраторам, стала проще с новой моделью Admin.
Всегда когда операции с моделью User ограничиваются администратором, нам требуется проверить что пользователь олицетворяет администратора.
Так как Admin’ая глобальная заготовка запроса ограничивает нас только администраторами, метод impersonate можно вызывать сразу для класса Admin.
Во время тестирования, вам может понадобиться создать модель User c привилегиями администратора, используя фабрику моделей как в примере ниже.
Мы можем улучшить этот код добавив состояние для фабрики инкапусулировав то — что пользователь является администратором.
Стало несомненно лучше, но мы по прежнему получаем экземпляр модели User. Определив новую фабрику для модели Admin, мы также получим пользователя с правами администратора, но теперь фабрика будет возвращать экземпляр модели Admin.
Аналогично тому как Eloquent определяет имена таблиц, имена класса моделей используется для для определения внешних ключей и промежуточных таблиц. Следовательно доступ к отношениям из модели Admin проблематичен.
Eloquent не может получить доступ к отношению так как предполагает что каждый экземпляр модели Post имеет поле admin_id вместо поля user_id. Мы можем исправить это передав внешний ключ user_id в модели User:
Эта же проблема существует в отношение многие ко многим. Eloquent предполагает что имя промежуточной таблицы соответствует имени текущего класса модели:
Мы так же мы можем решить эту проблему явно указав имя сводной таблицы и имя удаленного ключа:
Несмотря на то что явное определение удаленных ключей и сводных таблиц позволит модели Admin получить доступ к отношениям модели User, это решение далеко от идеального. Существование этих на вид не нужных определений, не улучшает наш код.
Однако, вы можете создать трейт HasParentModel который автоматически решит эту проблему. Данный трейт заменит имя класса модели на имя класса родительской модели. Код трейта GitHub.
Давайте посмотрим на новую модель Admin которая использует этот трейт:
Сейчас наши отношения модели User могут вернуться к тому состоянию когда они полагались на значения по умолчанию.
Трейт HasParentModel очищает нашу модель и дает разработчику понять что что-то особенное происходит внутри нее.
Мы выявили общие характеристики Eloquent модели и сделали их чище, используя их наследование. Эта технология позволяет создавать нам более лучшие имена объектов и инкапсулировать их в нашем приложение. Помните что наследование доступно для всех моделей Eloquent'а, не только для Users и Admins. Возможности безграничны!
Творите, получайте удовольствие и делитесь полученными знаниями. Поделитесь со мной, как вы используете этот паттерн в ваших проектах! (Твитер @calebporzio и @tightenco)
Удачи!
Пользователь
=====
История Илона Маска – Инфографика
2017-12-15, 21:48
COO
=====
PVS-Studio 2018: CWE, Java, RPG, macOS, Keil, IAR, MISRA
2017-12-14, 20:37





DevRel
=====
Непутевые заметки об облачном хостинге от Azure
2017-12-21, 09:06
.NET Core, WPF, UWP, Xamarin, IoT
=====
Проекционное моделирование
2017-12-14, 21:33
Пользователь
=====
Информационная безопасность банковских безналичных платежей. Часть 1 — Экономические основы
2017-12-26, 01:36
Информационная безопасность
=====
Подключаем оплату через Apple Pay на сайте
2017-12-18, 11:39
Пользователь
=====
Be my burger
2017-12-15, 12:09
Пользователь
=====
Анонс HolyJS 2018 Piter: как 1988-й сказывается на 2018-м
2017-12-15, 11:00
ContentProvider
=====
18 новых IT-специальностей которые появились из-за криптовалют
2017-12-15, 10:57
Пользователь
=====
Обнаружение аномалий в данных сетевого мониторинга методами статистики
2017-12-15, 11:10








User
=====
Блокчейн в сфере образования
2017-12-15, 11:16
Community Manager
=====
Отображение переменных окружения в Redis
2017-12-15, 11:18

Redis — это такое хранилище вида ключ-значение. Переменные окружения (environment variables) — напоминают то же самое. А что если это как-то объединить?
Для любителей пятничных постов, несложного хакинга и странных желаний — прошу под кат
Существует внушительный список клиентских библиотек для Redis на почти всех языках программирования. Но что, если:
В моем случае задача возникла после создание очередного CGI-like сервиса, которому необходимо было сохранять состояние. При этом выполнение этого скрипта может происходить на разных машинах.
Так как дело было примерно часа в 2 ночи и для меня уже наступила пятница, было принято решение расслабится и сделать что-нибудь несложное и интересное.
Можно ли перехватить системные вызовы так, что при записи в переменные окружения (setenv), данные
записывались в Redis, а при чтении (getenv) наоборот доставались из кэша?
Схематично выглядит так:
Да, есть хорошая статья, где описывается как делать перехватчики системных вызовов.
Есть малопопулярная возможность указать функции инициализации (constructor) и финализирования (destructor) в разделяемой библиотеки. В них и будем подключаться.
Пришлось изучить спецификацию POSIX'a и Linux'a по этой теме.
Функции, которые необходимо было перехватить:
Код тут.
Подвохов нет — типичный CMake с Github'a
Зависимости
Сборка
Важный момент: некоторые приложения не меняют переменные окружения, а только внутренний массив. В таких приложениях данные из Redis будут получены, но обратно не отобразятся.
Хороший вариант — python. Согласно документации, изменение в os.environ отображается в переменных окружения.
Допустим, уже поднят Redis на локальной машине.
Задача заняла примерно два часа, с учетом изучения предметной области. Сделано больше для фана, нежели для реальных целей. 
Тем не менее кому-то может пригодится.
Пользователь
=====
Шпаргалка по улучшению интерфейса
2017-12-15, 11:37
Ведущий продуктовый дизайнер
=====
FZF. Нечеткий поиск или как быстро ставить npm пакеты и убивать процессы
2017-12-15, 12:55

Я работаю в MacOS, почти не использую Finder и все время провожу в консоли. Именно поэтому стараюсь сделать работу из консоли как можно более удобной. 
Относительно недавно мне на глаза попалась утилита FZF. И уже через неделю стала незаменимой. 
FZF представляет возможность нечеткого поиска в стиле UNIX: умеет быстро и относительно хорошо искать по строкам, которые передали ей на вход, и интегрироваться с другими моими любимыми программами.
Я с удивлением обнаружил, что об этой программе нет ни одной статьи на хабре, кроме некоторых упоминаний вскользь. Я решил восполнить этот пробел. Если вы уже знаете о FZF, то статья скорее всего покажется вам неинформативной, а всем остальным добро пожаловать 
У нас в Wrike под каждую задачу должна быть создана своя ветка. В имени ветки в том числе есть номер тикета. Хочешь переключиться — помни название и номер. Или ищи. Ищу я примерно так:
Только вместо grep использую ripgrep
Но каждый раз набирать эту длинную команду лень. Да и вместо angular у меня часто получается agnular. Короче, одно расстройство. Поэтому достаточно в .zshrc или .bashrc прописать простенькую функцию (взята из документации). 
И теперь можно поискать по всем веткам. Выглядит это следующим образом:
Примерно так же можно убить зависший процесс. Только функция теперь будет называться fkill
FZF поддерживает множественный выбор. То есть при помощи кнопки Tab можно выбирать строку а по Enter отправлять все выбранное на вывод (если, конечно, запустить FZF с флагом -m).
Я использую пакет all-the-package-names, чтобы загружать все имена пакетов, которые есть в официальном регистре пакетов npm:
Дальше пишу совсем простую функцию:
И все. Теперь можно делать как-то так:
Очень много полезных примеров есть в документации. 
На просторах github я нашел еще две утилиты, которые делают примерно то же самое:
GitHub — jhawthorn/fzy: A better fuzzy finder
GitHub — calleerlandsson/pick: A fuzzy search tool for the command-line
Если вы съели собаку в написании скриптов под Bash или Zsh, то статья скорее всего покажется вам наивной. Возможно, вы даже знаете, как улучшить примеры, которые я привел. А может, уже давно используете FZF у себя, и вам есть что сказать. 
Если так, то пишите в комментариях и я обязательно обновлю и дополню статью вашими примерами и замечаниями.
Team Lead
=====
Как из UML диаграммы получить каркас Vue.js приложения
2017-12-15, 11:50
Здравствуйте, уважаемые Хабражители. Представляю вашему вниманию перевод статьи From Draw.io to Vue.js app автора Francesco Zuppichini.
Это моя первая публикация на Хабре и я решил начать с перевода статьи об одном классном инструменте, который позволяет сгенерировать Vue.js приложение из UML диаграммы.
Удивлены? Я был просто восхищен, когда наткнулся на него.
Конечно же, как и в любой бета версии, там есть над чем поработать. Например первым делом я связался с автором и сказал, что хочу внести исправления в структуру шаблона компонентов и именование путей. Автор вышел в течении часа на связь, выложил код на GitHub и написал небольшой туториал. После принятия PR было получено разрешение на перевод статьи, с указанием ссылки на оригинал.
Кому интересно — прошу под кат.
Из Draw.io в Vue.js приложение
Что, если я скажу вам, что вы можете трансформировать это:

Граф, нарисованный в draw.io
в это:

Структура приложения
Получился проект Vue.js со всеми файлами и импортами, которые вам нужны для начала создания своего клевого продукта. Круто, да?
Я сделал небольшое демо-видео, которое вы можете посмотреть здесь:
Каждое веб-приложение может быть выражено в виде графа
Давайте немного подумаем. Когда вы используете фреймворк, такой как React, Angular или Vue, у вас всегда есть один корневой компонент.
В нашем примере корневой компонент — это компонент App, все остальное — всего лишь узел графа. Мы можем идентифицировать на первом уровне Home и Index nodes как прямые дочерние элементы App.
Общим стандартом при разработке веб-приложений является хранение компонентов в структуре каталогов на основе графов. Поэтому обычно для каждого компонента создается каталог с тем же именем, где размещается сам компонент и все его дочерние элементы.
Например, Index — это корень подграфа, составленного самим собой, User и Post. Поэтому имеет смысл имитировать эту абстракцию в структурах приложения.

Index и дочерние компоненты
Это дает два преимущества: масштабируемость, поскольку подграфы являются независимыми, и легкое понимание структуры и логики приложения.
Кроме того, всегда можно видеть общую структуру приложения, просто глядя на граф.
Итак, мы сказали, что каждое веб-приложение на самом деле является графом, поэтому мы можем генерировать их из него.
В конце концов, каждый файл, начинающийся с графа, прост. Вам просто нужно найти пересечение деревьев и создать каждый файл в его локальном корневом каталоге, и вы можете сделать это рекурсивно.
Возникает проблема, мы знаем, что в современных веб-приложениях компоненты импортируют и используют другие компоненты. Поэтому нам нужно связать каждый из них с его зависимостями и создать динамический шаблон, основанный на текущем языке программирования, в котором внутри него содержится правильный синтаксис для их импорта.
В JavaScript файлы импортируются примерно так:
Поэтому, чтобы перейти от графа к приложению, нам нужен создать каждый файл, поместить его в правильную позицию на основе самого графа и отобразить правильный шаблон для импорта зависимостей
Я создал пакет, который позволяет вам нарисовать ваше приложение в draw.io и использовать экспортированный XML-файл для создания приложения Vue.js. Он называется graph2app-drawio2vuejs.
Пакет можно найти здесь:
FrancescoSaverioZuppichini/DrawIo2Vuejs
На самом деле, это не новая идея, я разработал некоторое время назад способ сделать практически то же самое, используя python:
FrancescoSaverioZuppichini/drawIoToVuejs
Но имейте в виду, что эта новая версия пакета npm намного лучше.
Поэтому, прежде всего, установите пакет глобально с помощью npm:
Теперь можно использовать его в терминале с помощью команды:
Но, конечно, нужно передать несколько аргументов:
Обязательно нужно передать путь к xml-файлу draw.io.
Пришло время рисовать! Перейдите на draw.io, выберите UML на левой панели и нажмите на Object:

Объект используется для описания узла в графе
Теперь вы можете начать с создания первого узла. Помните, что это будет ваш корневой компонент. Для моих кейсов, корневой компонент всегда является первым узлом, который нарисован на диаграмме.

Наш первый узел: App
Затем, на основе приложения, которое вы хотите создать, вы можете добавить другой узел.

теперь у нас есть два узла!
Теперь мы хотим, чтобы Home был дочерним элементом App. Так что нажмите на Home и используйте стрелку для подключения к App.


Home дочерний компонент в App
Что делать, если мы хотим также в App импортировать Home в качестве зависимости? Нажмите на use arrow в разделе UML слева и поместите его из App в Home.

App импортирует Home как зависимость
Хорошо! Вы создали свой первый граф! Давайте используем его для создания приложения Vuejs на его основе.
Мы сказали, что нам нужен файл xml, поэтому экспортируйте его без сжатия. Нажмите Файл > Экспортировать как > XML > Сжатый (нет).
Теперь создайте базовое приложение Vue.js, используя командную строку Vue:
После того, как вы это сделаете, мы готовы сгенерировать приложение из графа:
Для меня будет такая команда:
Если все работает правильно, вы должны увидеть следующий результат:

результат
Файл App.vue будет обновлен, поскольку он уже был там, добавив правильный импорт для Home. Также будет создан новый компонент Home. Если мы откроем App.vue, мы должны увидеть:

Компонент правильно импортируется, и папка Home с файлом Home.vue была правильно создана!
Пакет drawio2vuejs разработан с использованием другого пакета, который я разработал: graph2app.
https://www.npmjs.com/package/graph2app-core
Скоро я собираюсь сделать статью о этом, как применить модуль, используя три части:
App, где основная логика, создает каталог и файлы из графа. Граф выполнен с использованием экземпляра GraphBuilder. В нашем случае я создал DrawIoGraphBuilder, который расширяет его, чтобы проанализировать XML-файл из draw.io.
graph2app-drawio-graph-builder
Разработчики смогут расширять базовый экземпляр для анализа графа с помощью других типов интерфейсов.
File — это абстракция узлов на графе. Он имеет шаблон, из котороо генерируется компонент. Таким образом, когда graph2app получает граф, ему также нужен экземпляр File, чтобы вызвать метод рендеринга на нем и сохранить файл правильно.
Для Vue.js я создал:
graph2app-vue-core
Как видите, пакет имеет модульную структуру. Мы могли бы использовать тот же DrawIoGraphBuilder с другим экземпляром File для создания, например, React приложений из того же графа draw.io.
Надеюсь, вам понравится эта статья. Я твердо убежден, что визуализация приложения может повысить производительность. Библиотека по-прежнему является бета-версией, и ей нужно некоторое улучшение. Я думаю, что людям понравится эта идея и они будут способствовать развитию.
Пожалуйста, дайте мне знать о вашем мнении. Спасибо за проявленный интерес.
Франческо Саверио
JavaScript Developer
=====
Smart IDReader SDK — как написать Telegram-бота на Python для распознавания документов за 5 минут
2017-12-20, 06:54

Мы, Smart Engines, продолжаем цикл статей про то, как встроить наши технологии распознавания (паспортов, банковских карт и других) в ваши приложения. Ранее мы уже писали про встраивание на iOS и Android, а сегодня мы расскажем про то, как работать с Python-интерфейсом библиотеки распознавания Smart IDReader и напишем простого Telegram-бота.
Кстати, список поддерживаемых нами языков программирования расширился и теперь включает C++, C, C#, Objective-C, Swift, Java, Python, а также такие эзотерические языки, как Visual Basic и, разумеется, PHP. Как и раньше, мы поддерживаем все популярные и многие непопулярные операционные системы и архитектуры, а наши бесплатные приложения доступны для скачивания из App Store и Google Play.
По традиции, демо-версия Smart IDReader SDK для Python вместе с исходным кодом реализации Telegram-бота выложены на Github и доступны по ссылке. 
От SDK нам потребуется несколько файлов:
Для написания Telegram-бота мы выбрали telepot.
С подробной информацией о библиотеке можно ознакомиться в документации, а сейчас рассмотрим только самое необходимое. 
Подключим библиотеку и сконфигурируем движок распознавания:
Теперь можно написать функцию распознавания изображения:
Мы пойдем по простому пути и воспользуемся примером из документации telepot. Нам нужно написать класс, объект которого будет создаваться на каждый чат, и реализовать в нем функцию on_chat_message. Также в конструктор мы будем передавать ранее созданный движок распознавания, чтобы каждый раз не тратить время на его создание: 
Наконец, создадим и запустим бота:
Вместо args.token следует подставить свой уникальный token бота, получаемый после его регистрации. Если вы никогда не создавали бота, то на официальном сайте Telegram есть подробная инструкция.
Вот и все! Мы рассказали, как с помощью Python-интерфейса Smart IDReader SDK написать своего Telegram-бота для распознавания документов. 
Заметим, что особенностью наших продуктов является их полная автономность — им не нужен интернет. Но если вдруг очень захочется, то с помощью Telegram можно очень просто распознавать документы удаленно. Однако, по российскому законодательству распознавать удаленно можно только свои документы. Чтобы работать с данными документов других людей, необходимо не только стать оператором по обработке и хранению персональных данных, иметь необходимую инфраструктуру для защиты этих данных, но также защищать все телефоны и компьютеры, на которых происходит распознавание. Поэтому, наши коллеги из Sum&Substance с помощью наших библиотек разработали платформу для удаленного распознавания и проверки данных документов, при этом заботясь о юридической стороне вопроса.
Пользователь
=====
Живой митап #RuPostgres: вопросы и ответы с экспертами Avito. Расшифровка прямого эфира
2017-12-15, 12:42
Около месяца назад мои коллеги из DBA-команды приняли участие в живом митапе на youtube-канале #RuPostgres Live, где отвечали на вопросы Николая Самохвалова и зрителей, которые присылали их в форму и подключились к трансляции. Получилась интересная и содержательная беседа про PostgreSQL, опыт работы с разными версиями и задачами. Поэтому мы решили сделать текстовую расшифровку этой встречи, обогатив её полезными ссылками. В комментариях задавайте вопросы, если они возникнут — постараемся на них ответить!

Сначала представлю участников беседы: 
 Николай Самохвалов — евангелист PostgreSQL, сооснователь российского PostgreSQL-сообществ­а (2008) и ряда успешных стартапов, использующих PostgreSQL

 
 Константин Евтеев — лидер OLTP-юнита Avito

 
Сергей Бурладян — архитектор баз данных

 
Дмитрий Вагин — DBA Team Lead

 
А теперь — к расшифровке! 
Николай Самохвалов: Привет, уважаемые Youtube-зрители. Меня зовут Николай Самохвалов. Это #RuPostgres Live #2: вопросы и ответы с экспертами Avito, и со мной сегодня Константин Евтеев, Дмитрий Вагин и Сергей Бурладян. Это далеко не все Postgres-эксперты в компании Avito, я так понимаю. Сколько у вас людей в команде, которая знает Postgres?
Константин Евтеев: Postgres-команда — это, по факту, OLTP-юнит — состоит из 13 человек, которые в свою очередь делятся: часть занимается платформой, часть — продуктовой разработкой.
Николай Самохвалов: Женщины есть?
Константин Евтеев: Да.
Николай Самохвалов: Прямо сейчас в чатике Youtube можно писать вопросы. У нас есть и заранее подготовленные вопросы, в том числе — от меня лично. Поговорим, как в компании Avito устроена «готовка» Postgres. И первый вопрос: правда ли, что все ещё 9.2?
Константин Евтеев: Нет. 9.2 уже нет. Мы используем все версии Postgres, которые официально поддерживаются комьюнити.
Николай Самохвалов: Там end-of-life был совсем недавно…
Константин Евтеев: В сентябре. Мы к этому подошли, и с 9.2 мы проапгрейдились на разные версии, в том числе 9.4, 9.5, 9.6.
Николай Самохвалов: 10...?
Константин Евтеев: Пока ещё нет. 10 у нас есть в тесте.
Николай Самохвалов: С чем связана такая разношёрстная картина?
Константин Евтеев: Разношёрстная картина связана прежде всего, с тем, что перед апгрейдом мы сначала тестируем версию, и далее проводим апгрейд. Смотрим, как она себя показала, какой показывает перформанс, и после этого (для следующего инстанса) мы можем тестировать другую версию, апгрейдиться на 9.4-9.5. Потому что любой апгрейд связан с даунтаймом, и апгрейда ради апгрейда как такового нет. Мы делаем его ради получения необходимой функциональности и производительности.
Николай Самохвалов: Получается, у вас есть четыре разных версии?
Константин Евтеев: 9.3 у нас нет. Есть 9.4, 9.5, 9.6. 10 в тесте.
Николай Самохвалов: Если у кого-то из вас 9.3, срочно апгрейдьтесь. Был один из вопросов в Youtube: «Стоит ли мне из 9.3 сразу на 9.6 апгрейдиться, или мне нужна промежуточная версия?». Я бы сказал, что апгрейдиться нужно сразу на десятку.
Сергей Бурладян: Можно пропустить промежуточную версию.
Николай Самохвалов: Я немного не понимаю логику этого вопроса. Люди, наверное, думают, что эти версии более сырые. Но 9.6 существует уже год.
Сергей Бурладян: Кого-то может беспокоить, что, допустим, формат поменялся, индексы пересоздавать. Но pg_upgrade поддерживает пропуск версии, можно быть спокойным.
Николай Самохвалов: На 10 ты бы стал перетаскивать свой проект?
Сергей Бурладян: Пока нет. У нас такая идея, что мы апгрейдимся где-то на четвертый минорный релиз. На 10,4 мы бы уже смогли, наверное, перейти.
Николай Самохвалов: Это прямо официальная политика?
Сергей Бурладян: Полуофициальная.
Константин Евтеев: Но при этом у нас есть dev- и test-среды, там мы проводим тестирование самых новых версий. В том числе 10 версию мы тестировали начиная с альфы и написали несколько багрепортов: 1, 2.
Николай Самохвалов: Как это делаете?
Константин Евтеев: У нас есть семплирование, с одной стороны, с другой стороны — фикстуры для того, чтобы проводить тестирование нашего приложения. После семплирования данных мы готовим image, который деплоим в наше облако. Помимо того, что поднимается база, dev- и test-среда — это еще и большое количество инфраструктуры. Там поднимаем внешние индексы, различные кэши, очереди и другие элементы инфраструктуры. После деплоя нужно проинициализировать внешние индексы. У нас очень активно используется логическая репликация. На семпловых данных нужно поднять всю связанную инфраструктуру. Таким образом, накат семпловых данных приводит к инициализации около десятка баз данных, через механизм очередей PgQ, наполняются наши копии матвьюх, осуществляется доставка данных во внешние индексы и т. д.
Николай Самохвалов: Подходим к вопросу: какой у вас объем данных в Postgres хранится?
Константин Евтеев: Общий объем наших баз данных — более 15 ТБ. Работают они на десятках серверов. Типовая рабочая нагрузка на них — около 10 тыс. транзакций в секунду.
Николай Самохвалов: Очень интересный вопрос недавно проскакивал на Facebook. Есть продакшн-данные, которые можно было бы потестировать в тест-среде, а, может быть, даже в dev. Это идеальный случай: ты можешь посмотреть, где что тормозит. Ясно, что во-первых, не всем разработчикам нужно показывать все данные. Во-вторых, огромное количество данных размером в 15 ТБ мы легко на dev-среду не заберем. Как готовите эти среды, как забираете данные?
Константин Евтеев: Как нарезать семпл? Бывают данные связанные, бывают несвязанные. У нас на проекте есть две основные сущности — пользователь и объявление. Мы берем какие-то выборки пользователей, собираем все их объявления, далее идет выборка по остальным таблицам. Вероятнее всего, нужно ограниченное количество наборов — мы задаем лимиты в нашем скрипте.
Николай Самохвалов: Сколько процентов?
Константин Евтеев: Зависит от параметров и задачи, которую мы решаем в тесте. В большинстве случаев большой объём не нужен.
Дмитрий Вагин: Я немного расскажу весь механизм. Каждое утро у нас запускается скрипт, который забирает определенных пользователей и их объявления: рандомный набор плюс заранее зашитые тестовые юзеры. Плюс куча зависимостей с небольшими ограничениями. Вся сборка с продакшн-баз занимает около получаса. Затем на этом строятся docker-образы и пушатся в registry.
Николай Самохвалов: Как выбирается рандом? Table simple или что-то своё?
Дмитрий Вагин: Нет. У нас есть счетчики, сколько у каких пользователей активных объявлений, рандомно берется пачка пользователей. Критерий такой: чтобы выдача в этом тестовом образе на сайте была 2 страницы в Москве, чтобы можно было тестировать более-менее наглядно. Простые условия.
Николай Самохвалов: Это не много, получается, данных.
Дмитрий Вагин: Там куча справочников, куча зависимых данных.
Николай Самохвалов: Но самих объявлений получается не много. Если вы что-то новенькое делаете, например, индекс забыли, поймете об этом только на продакшене? Или есть что-то в тестовом окружении?
Дмитрий Вагин: Это мы увидим на тестировании.
Константин Евтеев: «Не много» и «забыли индекс» — это все зависит от объёма данных. Если есть несколько гигабайт, индекс будет работать, и будет заметно. А мы говорим, что там несколько гигабайт. А если разработчикам нужно нагрузочное тестирование, то можно поднять необходимую базу из бэкапа на dev-кластер. Мы иногда можем его делать. Главное, что я хотел упомянуть — при сборке этих сред в том числе идет обфускация, все данные анонимизированы.
Николай Самохвалов: Написали программисты специально?
Константин Евтеев: Чтобы не видеть настоящих пользовательских данных.
Николай Самохвалов: Это круто. Но подразумевается, что если схема меняется, мы должны переписывать. Требует дополнительного времени.
Константин Евтеев: В том числе. В случае, если работаете с семплом данных, это неизбежно, поднимая dev-среды. Перед тем, как вести разработку в прод, нужно делать двойную работу: сначала делать разработку в dev- и test-средах. Поэтому когда появляются новые проекты, мы всей командой агитируем за то, чтобы использовать не семплирование данных, а чтобы писали фикстуры, чтобы не иметь единую точку отказа везде.
Николай Самохвалов: В основном что за языки используются? Ruby или PHP?
Константин Евтеев: Фикстуры можно делать на различных языках.
Николай Самохвалов: У вас какие?
Константин Евтеев: PHP.
Николай Самохвалов: Вы стремитесь от семплирования уйти в сторону синтетических данных, чтобы на тестировании использовать?
Константин Евтеев: Да, во-первых, будут сначала тесты писаться, с другой стороны, не будет необходимости отдельно поддерживать семплинг данных. И решать в том числе вопросы аварий семплера. Он же может ломаться, если кто-то поправил структуру, но не написал миграцию, не добавил в семпл…
Николай Самохвалов: Как говорил один известный постгрессоид, жизнь богаче. Живое распределение данных — оно другое. Правильно? Может быть, продакшн-данные живые тоже есть смысл брать? Я просто услышал мысль что круто — фикстуры, но мне кажется, что нужно посмотреть живое распределение.
Константин Евтеев: В этом случае как раз стейджинг. Необходимость нагрузочного тестирования, и есть набор серверов, куда можно развернуть полный бэкап прод-базы.
Николай Самохвалов: Про мониторинг интересно спросить. Как понимаете, что там индексов не хватает? Что используете? Знаю, у вас есть доклад про мониторинг, Дмитрий делал. Можно пару слов: какие инструменты?
Дмитрий Вагин: Идея простая. Берете все, что есть, и кидаете в Graphite или куда-то еще. А потом строите дэшборды. Когда где-то на очень загруженной табличке не хватает индекса, обычно это сразу видно в том, что на сайте ничего не работает, или видно это в топе запросов где-то в pg_stat_activity и прочем. Более сложные кейсы приходится разбирать по графикам в дэшбордах. Допустим, тут чтения с диска слишком много на этой табличке. Смотришь, где какие там запросы идут, и где-то, и возможно, надо индекс чуть другой сделать.
Николай Самохвалов: Почему не взять что-то существующее?
Дмитрий Вагин: Можно, но когда берешь что-то существующее, приходится разбираться, как это существующее берет эти метрики и что ты в итоге видишь. Приходится разбираться, что оно берет, как обрабатывает, куда складывает и что ты в итоге видишь. И вот я не могу так просто взять и доверять какой-то циферке, которую говорит какая-нибудь утилита, например munin. Но в плагин munin можно залезть и посмотреть, что и как там…
Николай Самохвалов: В Zabbix тоже ведь можно увидеть, как там всё работает.
Дмитрий Вагин: Zabbix просто не вывезет на наших объемах.
Николай Самохвалов: Интересно.
Дмитрий Вагин: У нас Graphite-то не всегда всё вывозит.
Николай Самохвалов: Вопрос из чата: как вы делаете vacuum full огромной таблицы более 100 ГБ?
Дмитрий Вагин: Есть два варианта. Первый – не делать.
Николай Самохвалов: Плохой вариант.
Дмитрий Вагин: Ну… Зачем vacuum full?
Николай Самохвалов: Кстати, правильно. Я не сразу понял этот вопрос. Зачёркиваем «full». Никто не делает vacuum full просто так, не нужно это делать.
Дмитрий Вагин: Vacuum работает.
Николай Самохвалов: Ок. Мы перед трансляцией мы обсуждали, что размер 15 ТБ даже уменьшается. За счет чего?
Константин Евтеев: Мы это назвали «хвост». Есть данные, которые активные, горячие объявления. Потом есть, допустим, удаленные, заблокированные, а есть удаленные навсегда, то есть когда ты их удаляешь и больше не видишь их в интерфейсе. Но мы обязаны всю эту информацию хранить. Но это можно делать уже в другом месте — тоже в Postgres.
Николай Самохвалов: В 9.2?
Константин Евтеев: (Смеётся). Ну, например, да. Мы написали скрипт, который собирает все старые данные и переносит их в другую базу, доступную только для бэкофиса. У нас была высокая скорость роста объема основной базы данных. После запуска скрипта, переноса в данных в «хвост» и последующего реиндекса объем уменьшился и скорость роста замедлилась, почти остановилась. Периодически мы делаем реиндекс.
Николай Самохвалов: А всякие pg_repack не используете, или аналоги?
Константин Евтеев: В том числе используем. Сейчас Дима расскажет случай.
Дмитрий Вагин: Я один раз выключил специально на одной табличке автовакуум. И забыл его включить. Через две недели увидел, что табличка вместо 20 ГБ стала 300 ГБ. Думаю: «Ладно, надо исправить». Автовакуум отработал честно, она наполовину пустая, и надо ее как-то ужать. Вариант какой? Сделать даунтайм, выключить продакшн-нагрузку, сделать vacuum full — не очень. Второй вариант — использовать скрипт PG Compactor, который написал Сергей. Это старая всем известная идея — апдейтить с конца таблицы странички…
Николай Самохвалов: Я тут вступлю. Это пример epic fail опенсорса, когда Hubert depesz Lubaczewski поднимает какую-то тему, потом ребята легируют, потом Сергей Коноплёв включается, потом Максим Богук включается, и у всех своя версия. Они вот такие расфорканные три форка, а теперь я слышу про четвертый форк! Не знаю, может в NDA что-то такое, что каждый свое пишет, не могут договориться. Я пытался использовать все это, но для себя лично остановил выбор на pg_repack. Как раз в тот момент, когда я об этом задумался, Amazon добавил официальную поддержку pg_repack. Это практически индустриальный стандарт становится. А вы говорите, что используете апдейты. Для тех, кто не знает, pg_repack делает отдельную таблицу, триггерами следит за изменениями. Почему не pg_repack? Второй вопрос: почему всё-таки 4-я версия?
Сергей Бурладян: pg_repack редактирует служебный каталог Postgres. Нам это кажется страшным.
Николай Самохвалов: Напрямую?
Сергей Бурладян: Да.
Николай Самохвалов: Amazon не кажется.
Сергей Бурладян: Да. И потом, он использует триггеры, это дополнительная нагрузка во время продакшна. Занимает место, чуть ли не двукратный объем. Поэтому мы не хотели использовать pg_repack. И подход с постоянным апдейтом этих строчек, чтобы они сдвигались в свободное место, нам кажется более подходящим.
Николай Самохвалов: Похоже на дефрагментацию Windows.
Сергей Бурладян: Да.
Николай Самохвалов: Почему своя версия?
Сергей Бурладян: Просто нам хотелось сделать простую версию, чтобы было понятно, как она работает. Там нет фишек, которые есть в тех версиях, о которых ты говорил. У нас там, грубо говоря, вызов апдейтов и вызов вакуума. Тоже написан на Perl. Сейчас буду, наверное, на Python. Эта штука используется редко, но иногда используется.
Николай Самохвалов: Давайте немного вернемся назад. Как понять, что таблица распухла? Есть одна компания, они не показывают график блоттинга, объясняя это тем, что чтобы нормально показать, нужно использовать расширения, которые создают большую нагрузку. То есть чтобы нормально понять блоттинг, нужно влезть в кишки, потрясти их, и это очень заметно. А как вы это делаете?
Дмитрий Вагин: Мы не смотрим на блоат. Вакуум работает нормально. Достаточно не выключать автовакуум, и все будет отлично. Он чуть более агрессивен, чем дефолтный.
Николай Самохвалов: 1%, 2%? Там есть threshold, который говорит, когда analyze делать, и когда делать сам вакуум в зависимости от того, сколько в таблице грязного.
Сергей Бурладян: autovacuum_vacuum_scale_factor = 0.0014. По умолчанию vacuum_cost_limit, по-моему, стоит цифра 200, у нас 700. Это тоже у нас выкручено, чтобы он чаще срабатывал.
Николай Самохвалов: Самое интересное — это сколько процентов.
Сергей Бурладян: Пока есть место на дисках, то можно не задумываться над этим. Можно задуматься, если видно, что запросы тормозят, допустим.
Николай Самохвалов: Мониторинг?
Сергей Бурладян: Да.
Николай Самохвалов: Был вопрос в другую сторону: про безопасность. Храните ли вы ACL в Postgres?
Сергей Бурладян: ACL не используем. Row level security не используем.
Николай Самохвалов: Какие инструменты используете для файловер резервной репликации больших данных?
Сергей Бурладян: Свои инструменты, готовых не используем. Есть набор скриптов для файловера. До сих пор используем Londiste, причем наш, патченный, который позволяет после файловера автоматически восстановиться.
Николай Самохвалов: Londiste сейчас у вас основной инструмент?
Сергей Бурладян: Да, основной для логической репликаци.
Николай Самохвалов: PL/pgSQL код есть у вас?
Константин Евтеев: Есть, много.
Николай Самохвалов: Нравится?
Константин Евтеев: Зависит от случая. Но вообще нравится.
Николай Самохвалов: Расскажите, как готовите, тестируете, дебажите.
Константин Евтеев: Я больше расскажу про версионирование PL/pgSQL кода. Есть вопрос: как версионировать код и схему базу данных вместе с кодом приложения? Ведь когда у вас много серверов приложений, и они выкатываются постепенно, необходимо поддержать старую и новые версии PL/pgSQL кода.
Если мы говорим про схему базы данных, то мы это делаем через миграции. Все они должны быть обратно совместимыми, потому что в бою одновременно может находится как уже новые апликейшены, так и старые. Что же делать с кодом? Мы пришли к двум подходам, и сейчас в процессе перехода с одного на другой.
Первый подход — когда мы клали все наши хранимые процедуры на PL/pgSQL вместе с кодом апликейшена. Далее написали обвязку, которая читает каталог, каждая процедура хранится в отдельном файле, читает от него чек-сумму, обращается к базе, непосредственно в базе хранится контрольная сумма, то есть сравнивает, есть ли такая версия процедуры или нет. Она создает новую хранимую процедуру с припиской определенной версии, регистрирует в таблице. Например, создать пользователя, версия 1. В таблице хранится просто 1, а потом из этой таблички билдится словарик, который хранится вместе с приложением, и приложение, если видит обвязку «создать пользователя», у него есть словарик в котором написано, что этот суффикс должен быть 1, 2 или 3.
Есть недостатки. 
Николай Самохвалов: Один раз при создании. Прикольно. Видел в презентации, что вы достаточно много используете переменные сессий в Postgres — GUC Variable (Grand Unified Configuration).
Константин Евтеев: Да. Расскажу. Бояться их не нужно. Этот кейс мы изучили. В каком случае он у нас возник? Мы делали доставку в нашу аналитическую подсистему. Мы ее делаем с помощью очередей PgQ и deferred-триггера. Вешаем на таблицу deferred-триггер на первое срабатывание. Если запись меняется несколько раз, нам нужно поймать именно первое изменение. В момент первого изменения мы по старым данным видим все записи, а новые данные можем выбрать селектом непосредственно из базы.
Дальше у нас возникла следующая задача. Что, если у нас много связанных таблиц, а нам хочется непосредственно вместе с изменением этой таблицы еще прокинуть сигнал о том, что менялся объект в соседней таблице? В данном случае, когда меняем данные в соседней таблице, мы берем и выставляем сессионные переменные. Например, выставляем ключ: user 5 сделал какое-то действие. Только в текущей сессии. И в эту переменную делаем key value пары: вдруг несколько будет таких пользователей. И далее уже непосредственно в deferred-триггере проверяем эти ключи, сессионные переменные, и если что, подмешиваем некие сигналы. Таким образом у нас получается производная от данных: в таком виде на источнике их нет, а на приемник уже придет совершенно другой набор. Мы все это потестировали под нагрузкой, поняли, что никакого overhead при выставлении и использовании сессионных переменных не идет. (Во всяком случае, мы его не увидели).
Николай Самохвалов: А JSON-B или JSON используете?
Константин Евтеев: В большинстве случаев нет. В некоторых местах начинаем использовать, но мы не используем функции для работы с JSON-B. Почему? Как выяснилось, мы CPU bound. Сам формат JSON-B удобен для хранения и для работы, но когда мы CPU bound, то эта вся работа ведется на уровне приложения. Но, кстати, при этом мы активно используем Hstore. Он исторически раньше появился, но там все те же проблемы с CPU.
Николай Самохвалов: Видел в твоих слайдах про это. Когда ты говоришь про key value, это про hstore?
Константин Евтеев: Да.
Николай Самохвалов: А индексы? Они хранятся, hstore? GIN? Вообще GIN, GIST используются как-нибудь?
Константин Евтеев: В одном случае использовали. С ним есть некая история. Нужно внимательно читать документацию. В частности, у нас был кейс, когда надо выставлять fast update off. Потому что он до определенного размера очень хорошо работал, запись была достаточно активная в эту таблицу. А потом начало замирать при апдейтах.
Сергей Бурладян: Это написано в документации Postgres.
Дмитрий Вагин: Когда идут изменения по колонке, на которой есть GIN-индекс, то, чтобы делать изменения быстро, он строит у себя в небольшом поле маленькое дерево, и потом при чтении обращается к основному дереву и к изменениям в памяти. В какой-то момент место заканчивается, ему надо все это размазать — перенести изменения в большой индекс. В этот момент у нас все вставало на две минуты, и все упиралось в это.
Николай Самохвалов: Это означает, что в единицу времени было слишком много апдейтов?
Дмитрий Вагин: Апдейтов всегда много, потому что продакшен у нас ого-го.
Николай Самохвалов: 10000 транзакций, или сколько…
Дмитрий Вагин: Да-да-да. И просто нет момента, когда он может тихонько взять и сбросить. В итоге он говорит нам: «Я уперся в лимит, мне надо все сбросить. Подождите». И мы ждём. В итоге мы отключили fast update, и всё стало хорошо.
Николай Самохвалов: В целом каждый апдейт стал медленнее?
Дмитрий Вагин: Не заметили.
Николай Самохвалов: Из этих 10000 транзакций сколько модифицирующих?
Константин Евтеев: 1500. 90 000 в минуту. На разных базах, кстати, по-разному. Этот пример — в контексте одной из них. На других базах везде примерно 10 тысяч транзакций в секунду. Где-то запись, грубо говоря, 500 транзакций в секунду, а все остальное — чтение. А есть базы, где при общей нагрузке 8 тысяч транзакций в секунду запись 4 тысячи.
Николай Самохвалов: Разные версии, базы – это разные сервисы, у них разные команды?
Константин Евтеев: Есть разные сервисы, там где микросервисная архитектура, один сервис – одна база. Есть еще legacy – это монолит, у которого есть несколько баз, часть связана, в том числе и Londiste и event streaming, мы события через PgQ гоним. Часть из них связывается на уровне приложения.
Николай Самохвалов: Используете ли PostgreSQL в качестве аналитической базы данных?
Константин Евтеев: Раньше использовали, сейчас нет. У нас Vertica, и мы очень довольны.
Николай Самохвалов: Как часто из Postgres пуляете данные?
Константин Евтеев: В реальном времени с минимальной задержкой. Гоним через PgQ, а Vertica забирает. У нас для этого есть один выделенный сервер. Дальше команда Vertica забирает их оттуда в определенном порядке. За счет того, что у нас есть PgQ, на sequence все данные упорядочены.
Николай Самохвалов: Если что-то удаляется, обновляется, в Vertica тоже обновляется, или вы складируете новую версию, старую тоже храните? Строчка, допустим, обновилась.
Константин Евтеев: В Vertica хранятся все версии. Есть очень хороший доклад Николая Голова и статья на Хабре, как непосредственно реализовано хранение в Vertica.
Николай Самохвалов: Что с другими БД? Что c SQLite?
Константин Евтеев: В целом у нас такой подход в компании — когда команда начинает делать новый проект, она вольна выбирать из технического радара набор технологий, который ей удобней и какой более оптимален для решения ее задач.
Николай Самохвалов: ClickHouse нет в радаре?
Константин Евтеев: Есть. ClickHouse используем для хранения метрик.
Николай Самохвалов: CockroachDB?
Константин Евтеев: Нет. Для одного из решений хорошо подошел SQLite. Он уже в продакшене. В том числе для этого хорошо подходил и Postgres… Но определенное количество TPS выдал SQLite, мы его выбрали.
Николай Самохвалов: Вы помогаете всем командам по Postgres?
Константин Евтеев: Да. Делаем DBaaS, предоставляем базу данных под определенные требования. Если необходимо, выступаем в качестве некого SWAT, который помогает реализовывать бизнес-логику тем или иным командам. Можем научить или реализовать определенную бизнес-фичу.
Николай Самохвалов: Нет такого, чтобы использовать не только Postgres? Хотите — MongoDB, хотите — Tarantool, например?
Константин Евтеев: Для определенных задач есть и Tarantool, и MongoDB.
Николай Самохвалов: А MySQL, MariaDB используете?
Константин Евтеев: MySQL в продакшене нет. Может быть, есть на внутренних проектах.
Николай Самохвалов: Еще вопросы в чате. Что можете посоветовать для онлайн-миграции с 9.3 на 9.5 или 9.6? Londiste, Bucardo или что-то ещё? Сергей, может, лучше знает?
Сергей Бурладян: Мы просто не рассматривали такие процессы миграции, потому что, скорее всего, на первый взгляд, они просто не справятся с нашим трафиком. Ни Londiste, ни Bucardo не смогут нам логически реплицировать всю базу.
Николай Самохвалов: Давайте представим ситуацию (я знаю, что это не так), что у вас всё на 9.3. Ну, допустим, не повезло, и вам нужно что-то более свежее. Ваши действия?
Сергей Бурладян: Мы сделаем даунтайм и используем pg_upgrade.
Николай Самохвалов: На сколько ляжет сайт Avito?
Сергей Бурладян: Мы используем pg_upgrade, штатную утилиту Postgres, а у нее есть замечательный режим работы с хардлинками. Благодаря этому режиму даунтайм займет приблизительно 3-5 минут. 10 — максимум.
Николай Самохвалов: То есть берем документацию, изучаем, все штатными средствами, с таким подходом с хардлинками pg_upgrade. Вот вам ответ.
Сергей Бурладян: Да.
Николай Самохвалов: Как реплику ввести?
Сергей Бурладян: pg_upgrade может проапгрейдить только master, потому что ему нужно писать в сервер. А standby проапгрейдить не может. У нас есть в бою standby. Мастера нам не хватает, часть нагрузки идет на читающий standby. Мы уже не можем запустить просто один master без standby. (Раньше мы так и делали: апгрейдили мастер, потом спокойно пересоздавали standby, и все). К счастью, авторы Postgres написали в документации специальный алгоритм, который с использованием утилиты rsync позволяет после pg_upgrade проапгрейдить ещё и standby. Его хитрость заключается в том, что просто используется возможность rsync скопировать хардлинки. Грубо говоря, он воссоздает на standby такую же структуру хардлинков, как и на мастере. На standby есть те же все данные, что и на мастере. Если создать там такие же хардлинки во время даунтайма, получится… При запуске rsync мастер должен быть выключен.
Николай Самохвалов: Если есть несколько реплик, даунтайм увеличится, нужно ещё за этим последить.
Сергей Бурладян: Да.
Николай Самохвалов: Делаете вручную или автоматизируете?
Сергей Бурладян: Подготавливаемся к задаче, готовим набор команд, которые выполнить надо, и выполняем их. Вручную.
Николай Самохвалов: Скажите о каких-то подводных камнях, с которыми столкнулись при работе с Postgres, вещи, которые хотелось бы улучшить, проблемы, баги, недавно найденные.
Сергей Бурладян: Находим баги периодически в Postgres, потому что у нас нагрузка серьезная. Пару дней назад выяснили, что, оказывается, планы в триггере не инвалидируются при изменении таблицы. Есть триггер, сессия, она работает, потом мы делаем alter table, меняем колонку, а триггер не видит изменений, начинает падать с ошибками. Приходится после alter колонки в той же транзакции альтерить хранимку, чтобы сбросила свой кэш. Еще находили баги в вакууме. Про кэш не репортили.
Николай Самохвалов: Может быть, не как баг, а как…
Сергей Бурладян: Можно об этом написать в рассылку.
Николай Самохвалов: Нужно.
Сергей Бурладян: Согласен.
Николай Самохвалов: Бывают ли у вас дедлоки, как боретесь?
Сергей Бурладян: Бывают. Смотрим места, в которых появляются, пытаемся решить их.
Николай Самохвалов: Мониторинг об этом докладывает?
Сергей Бурладян: Да, в логах видно, что произошел дедлок.
Николай Самохвалов: Но логи же не будешь каждый день читать в таком количестве. Скажи, ну вот например, за последнюю минуту 10 дедлоков, смс получишь?
Константин Евтеев: Нет, но выявятся сразу. Как с ними бороться? Дедлоки, которые выявляет Postgres, один из запросов будет убит. Причем какой — неизвестно. Бороться с ними — брать блокировки в одном и том же порядке. Сортировку при взятии блокировок нужно взять. Или идти от более частного объекта к общим отсортированным.
Но более страшные дедлоки — когда их Postgres не может детектить. Если у вас, допустим, микросервисная архитектура, вы открываете транзакцию, берете lock на ресурс, и дальше приложение делает запрос в другой сервис, а он неявно обратится к этой же базе и попробует взять lock на этот же ресурс. В таком случае это будет вечный дедлок, который никак не задетектится. Дальше только по таймауту, возможно, отвалятся.
Дмитрий Вагин: Еще по поводу смс с десятью дедлоками. Если Deadlock возникает, одна транзакция отваливается, если это как-то аффектит прод, там вывалится исключение, ошибка, и в sentry создастся ошибка, она придет потом на почту. В принципе, все видится.
Николай Самохвалов: Придут последствия. Есть вопрос, который хотелось подробнее осветить. Как делаете бэкапы? Какие инструменты используете, как проверяете, в каком объеме хранится, насколько можете взять назад бэкап и восстановиться?
Сергей Бурладян: Делаем их с помощью pg_basebackup, вокруг него есть наша обвязка в виде скриптика. Используем Point-in-Time Recovery (PITR) у нас идет постоянное архивирование WAL-файлов, и мы можем восстановиться на любую точку из прошлого. Дополнительно используется задержка проигрывания WAL на одном из standby на 12 часов.
Николай Самохвалов: Был доклад на эту тему в прошлом году.
Сергей Бурладян: Да.
Николай Самохвалов: Интересующиеся могут найти видео, PDF, статью. Мы сможем восстановить бэкап месячной давности?
Константин Евтеев: Да.
Сергей Бурладян: Костя имеет в виду, что наш бэкап баз, который мы делаем, еще бэкапит дополнительно отдел DevOps, складывают у себя в хранилище. Они бэкапят с помощью Bareos, раньше был Bacula. Они берут наши бэкапы и складывают у себя отдельно.
Николай Самохвалов: Сколько места занимают?
Сергей Бурладян: Наши, по-моему, около 60 ТБ. Сколько занимают потом эти бэкапы у DevOps, не знаю.
Николай Самохвалов: Утилиты типа wal-e рассматривали?
Сергей Бурладян: Нет, у нас собственный скрипт. Знаем, что он делает, он простой, просто архивирует валы к нам в архив.
Николай Самохвалов: Пришла просьба выложить твою версию скрипта, этого компактора. Можешь это сделать?
Сергей Бурладян: Думаю, что могу. Мы планируем выложить до HighLoad нашу пропатченую версию Londiste на GitHub, и, возможно, этот скриптик тоже.
Николай Самохвалов: Присылай мне, я тоже твитну.
Теперь тема, которую невозможно обойти стороной – это поиск. Как у вас всё устроено, в том числе фуллтекст, что используете? Почему опять слово «зоопарк» тут вылезает?
Константин Евтеев: Полнотекстовый поиск в Postgres на текущий момент мы не используем. До недавнего времени мы использовали его, но в связи с тем, что достаточно большие объемы данных и большое количество запросов, соответственно, мы упираемся в CPU, мы CPU bound. Но у нас есть отличная команда «Sphinx».
Николай Самохвалов: Если CPU bound, почему бы не раскидать по нескольким машинкам, зашардить?
Константин Евтеев: Мы использовали его для очень узкоспецифичной задачи, когда там был очень небольшой объем данных. Мы поняли, что объем будет расти, после этого конкретно эту задачу перенесли на сторону Sphinx, который у нас был давно.
Николай Самохвалов: На сколько шард?
Константин Евтеев: Если не ошибаюсь… Лучше не буду говорить цифру.
Николай Самохвалов: Примерно.
Константин Евтеев: Есть статьи, и расшифровки, в том числе на Хабре, и видео с HighLoad, лучше посмотреть там. Я знаю, как мы доставляем данные в эти поисковые подсистемы.
Данные, которые необходимо доставить в Sphinx, если мы говорим про выдачу активных объявлений на сайте, мы материализуем и с помощью Londiste отправляем на отдельную машину, где в свою очередь, эти данные мы, наоборот, берем и раскладываем на много кусочков, то есть, грубо говоря, партицирование идет. Далее во много потоков мы отправляем эти данные Sphinx. Таким образом за полчаса мы можем полностью пересоздать индекс активных объявлений сайта несколько раз. При этом также мы реализовали непосредственно real-time доставку в sphinx-индексы тоже через PgQ, когда эти данные приезжают на сервер для индексации, далее мы их перекладываем еще в одну очередь, и она уже непосредственно работает через консумер со Sphinx.
Николай Самохвалов: Еще такой вопрос. Я правильно понимаю, что сейчас задержка довольно большая получается именно поисковая? То есть если я размещаю объявление…
Константин Евтеев: Это продуктовые требования. У нас они сейчас полчаса.
Николай Самохвалов: То есть если я сам объявление добавил, я его сам не найду по полнотексту?
Константин Евтеев: В выдаче оно появится через полчаса. В том числе это ограничение со стороны продукта. Ведь неизвестно, какое объявление ты добавишь. Хорошее оно или плохое.
Николай Самохвалов: В списке он сразу появится? Или как? Когда на slave попадёт.
Константин Евтеев: С технической точки зрения, мы его можем доставлять в индекс очень быстро. Но перед тем, как его по-настоящему показать, его надо проверить.
Николай Самохвалов: А, тут имеется в виду модерация.
Константин Евтеев: Да, модерация. Мало ли что в этом объявлении написано, какой товар можно продавать. Если говорить про наши очереди, то технически скорость доставки во внешний индекс — несколько секунд.
Николай Самохвалов: Я отвлекался на вопросы, правильно понял, что очереди в Sphinx через PgQ тоже?
Константин Евтеев: Да.
Николай Самохвалов: То есть вы так очень любите PgQ?
Константин Евтеев: Да, мы очень любим. И это очень быстро. В общем виде это гарантированный SLA несколько минут. А в общем виде это несколько секунд.
Николай Самохвалов: А Kafka вообще у вас отсутствует?
Константин Евтеев: Отсутствует. У нас есть nsq ещё.
Николай Самохвалов: А он почему?
Константин Евтеев: Очень хорошо себя зарекомендовал, хорошая очередь. И, соответственно, она тоже у нас используется. То есть, с одной стороны, у нас есть очередь, которая идет со стороны базы через PgQ, а через NCQ идут в том числе clickstream события, события от приложений…
Николай Самохвалов: «Используете ли PgBouncer?». Я на самом деле этот вопрос тоже не буду задавать. Как раз ваш коллега Виктор Ягофаров докладывал в Сан-Франциско. Я лучше в Twitter ссылку помещу. И там можно будет посмотреть pdf, может, даже видео есть.
Константин Евтеев: И в Санкт-Петербурге он тоже рассказывал этот же доклад.
Николай Самохвалов: Вы используете Materialized views?
Константин Евтеев: В терминах Postgres мы не используем Materialized views в связи с тем, что их надо обновлять вручную. Мы используем Materialized views в нашей интерпретации. Есть отличные материалы. Наверное, я тебе дам ссылку, можно ее будет в том числе затвитить. Потому что очень классный подход, то есть realtime materialized view, когда она реализована на триггерах. [Ссылки: 1, 2].
Николай Самохвалов: Скорее всего, я уже постил.
Константин Евтеев: Наверное, да. То есть когда мы ставим некие триггеры, которые в now() время обновления записи. И далее в deferred триггере мы можем посмотреть: если это время еще не равняется now(), то мы просто берем, удаляем запись, и заново из view её селектим и далее мы инсертим, соответственно, в таблицу, которая является как раз нашей мат.вьюхой.
Николай Самохвалов: То есть это на самом деле таблица?
Константин Евтеев: Да, это просто таблица. Поддерживаем ее на триггерах.
Николай Самохвалов: Много таких? Каков объем данных?
Константин Евтеев: Есть на десятки миллионов записей, есть на сотни миллионов записей.
Николай Самохвалов: А что за задачи?
Константин Евтеев: Задача денормализации данных.
Николай Самохвалов: Кстати, вопрос был по денормализации.
Константин Евтеев: То есть сначала удобно хранить данные нормализованно. Но когда мы говорим о том, что их нужно будет выбирать с высокой частотой. Ну, например, выдача или индексация. Соответственно, когда идет на тысячи запросов в секунду, вот эти все join’ы не оказывается очень подходят для этого. Соответственно, мы данные денормализуем, и таким образом мы в том числе реализуем выдачу. То есть все, что нашлось в Sphinx, дальше работает из этого материализованного представления, которое хранится на отдельном выделенном сервере. И далее там даже мы выкрутили размер shared-буферов до размера этого материализованного представления.
Николай Самохвалов: Получается, что этот deffered-триггер отправляет обновления на другой сервер.
Константин Евтеев: Мы получаем материализованное представление, а дальше уже с помощью Londiste мы его реплицируем на отдельный сервер.
Николай Самохвалов: То есть когда выйдет десятка, вы Londiste будете выпиливать?
Константин Евтеев: Мы реализуем все наши подходы к восстановлению.
То есть, грубо говоря, существуют аварии, когда у вас может ваша логическая репликация оказаться или в будущем относительно источника, или в прошлом относительно источника. И в данном случае нужно согласовать данные: либо ее отмотать в прошлое, либо на источнике как-то подвинуть текущие события, чтобы они еще раз проигрались на реплике в связи с тем, что она в прошлом оказалась.
Николай Самохвалов: То есть Londiste останется, или она будет заменена на логическую репликацию, нативную в десятке?
Константин Евтеев: Надо переходить на нативную репликацию.
Николай Самохвалов: То есть сейчас еще не готовы, если коротко?
Сергей Бурладян: Да. Когда мы все реализуем, то, что мы патчили в Londiste и тому подобное…
Николай Самохвалов: Сколько, по-твоему, времени это может занять?
Сергей Бурладян: Не знаю. Если этим заниматься, то немного времени.
Николай Самохвалов: Ещё вопросы из чата. Используете ли вы ltree?
Сергей Бурладян: По-моему, сейчас уже нет. Раньше пытались использовать для категорий. Но это был просто эксперимент. Вообще не используем.
Николай Самохвалов: На каком Linux Postgres крутится? Какие настройки ядра тюнить приходилось?
Сергей Бурладян: Postgres крутится на Debian GNU Linux. Сейчас восьмая версия, насколько я помню. Но тюнить особо ничего не тюнили.
Николай Самохвалов: То есть все дефолтовое. OOM-killer там по умолчанию (я не сисадмин) включен или нет?
Сергей Бурладян: Да, у нас включен.
Николай Самохвалов: Ой-ой-ой.
Сергей Бурладян: Но у нас он обычно не падает. Мы тюнили read-ahead, так как у нас индекс-сканы, чтобы они не зачитывали там большие блоки. Что еще тюнили? Ну, так вроде больше ничего не тюнили. Мы выключаем NUMA, это мы не используем.
Николай Самохвалов: А huge pages?
Сергей Бурладян: Huge pages не используем.
Николай Самохвалов: Ок. Вопрос такой еще. Администрирование – это не мой конек, а программирование больше. Какие типы данных еще есть, кроме hstore? Что там экзотического? Начиная, может, даже с какого-нибудь типа данных Range.
Сергей Бурладян: Range, по-моему, появились в 9.4.
Николай Самохвалов: Я думаю, раньше.
Сергей Бурладян: В 9.3, наверное, да?
Николай Самохвалов: Честно говоря, я не знаю.
Сергей Бурладян: В 9.2 были тоже?
Николай Самохвалов: Еще раньше должны были появиться.
Сергей Бурладян: Ну, мы, по-моему, не используем их.
Константин Евтеев: Использовали. Мы делали некую задачу. Как раз использовали в том числе расширение btree_gist. Когда нужно было сделать непересекающееся расписание, то есть CONSTRAINT vs строили.
Николай Самохвалов: По датам?
Константин Евтеев: Да.
Николай Самохвалов: То есть это constraint exclusion.
Константин Евтеев: Да.
Николай Самохвалов: То есть это понравилось, да? И вы не в двух колонках храните начало и конец периода, а в одной колонке.
Константин Евтеев: В данном примере была одна реализация, и в принципе она работала.
Николай Самохвалов: Именно непересекающихся. Я понял. Еще какие-то типы данных? Какие-то экзотические постгресовые… Tsvector ушел? Жаль. JSON-B еще не пришел. Понятно.
Конечно, интересно, про Vertica отдельно было бы поговорить. На самом деле как-нибудь, может быть, мы бы сделали событие, связанное именно с Vertica, потому что это тоже BI всякие, очень интересно было бы.
Константин Евтеев: Надо привлечь Николая Голова, Артема Данилова. И можно организовать событие. Я думаю, будет очень интересно.
Николай Самохвалов: Да. Я думаю, это близко к Postgres. Я помню, мы про Greenplum в Тинькове делали. И это было тоже интересно. Так что это на будущее.
Ещё вопрос. Используете ли вы SSD под Postgres? И как часто их меняете?
Константин Евтеев: SSD себя очень хорошо зарекомендовали.
Николай Самохвалов: Какие SSD, знаете?
Константин Евтеев: Не знаю, не скажу. У нас есть машины как на SSD, так и просто на жестких дисках. SSD-диски еще ни разу не меняли.
Николай Самохвалов: Да ладно? Круто.
Дмитрий Вагин: Один раз какой-то вылетал. Но не потому что он SSD. Не как SSD умирает, что у него ресурс кончился. Когда мы, по-моему, переезжали откуда-то куда-то и посмотрели, сколько ресурса осталось, и там было 90 с чем-то процентов. Можно смело использовать SSD, мне кажется. Хорошие — можно смело.
Николай Самохвалов: Ок, а партиционирование? Pg_pathman?
Дмитрий Вагин: Партицирование есть. Там по датам просто. Таблица, которая растет, разбивается по датам.
Константин Евтеев: И также еще используется решение на PL/Proxy. Соответственно, у нас есть 16 шардов, где мы храним, например, отправленную почту. Можно раскладывать там.
Николай Самохвалов: И от этого отказываться не собираетесь?
Константин Евтеев: Она себя очень хорошо зарекомендовала, работает. И необходимости переделывать нет.
Николай Самохвалов: Наверное, мы будем закругляться. Еще что-то интересное можете рассказать? Там был еще вопрос про факапы. Из последнего что-нибудь припоминаете? Как делать не стоит c Postgres? Выключать автовакуум…
Константин Евтеев: В случае, если у вас есть Master и Standby, когда вы делаете реиндекс и создали новый индекс уже, не надо его удалять сразу с Master.
Николай Самохвалов: Удалять старый?
Константин Евтеев: Да, старый индекс не надо удалять с Master. Потому что на Standby не инвалидируется кэш. И, соответственно, у вас сломаются планы запросов.
Николай Самохвалов: А какое время нужно? Когда его нужно?
Константин Евтеев: Время жизни вашей сессии бэкенда. Но лучше его умножить в несколько раз. Тогда можно быть спокойным. Чтобы когда вы уже удалили индекс, все бэкенды на stand-by были новые.
Николай Самохвалов: Да, это интересный кейс. А какие у вас есть долгие сессии, например, час?
Константин Евтеев: Когда вы используете PgBouncer, соответственно, вы там выставляете какое-то время жизни этой сессии. Мы эмпирически выставили 20 минут.
Николай Самохвалов: То есть долгих сессий у вас нет?
Константин Евтеев: Долгих транзакций нет. Это неприемлемо.
Николай Самохвалов: Про факапы. Ещё давайте какой-нибудь.
Константин Евтеев: Другой случай, которого мы избежали. Сергей рассказывал про апгрейд. Там в том числе необходимо было включать при апгрейде вместе со standby на hardlink с rsync.
Николай Самохвалов: С чего началось? С какой версии?
Константин Евтеев: Когда мы обновлялись с 9.2 на 9.4, мы это обнаружили. Там было необходимо после апгрейда master выключить vacuum, запретить все коннекты к master, в том числе включить master и выключить. И главное — чтоб в этот момент никто не записал. И после этого только делать rsync. В случае, если у вас кто-то запишет на master в это время, соответственно, далее вы получите битый standby. И когда вы об этом узнаете… Всё зависит от времени того, когда вы будете обращаться к тем битым данным.
Николай Самохвалов: Это все в тест-окружении словили?
Константин Евтеев: Мы изучали, как нам произвести апгрейд. Непосредственно как раз Сергей в том числе этим занимался вместе с Виктором Ягофаровым.
Сергей Бурладян: В тестах мы словили, да. Еще было в рассылке обсуждение такой проблемы.
Николай Самохвалов: Вопрос: Как по-умному готовиться к апгрейду?
Сергей Бурладян: Читать release notes новой версии. Там написано, что нужно делать после апгрейда.
Николай Самохвалов: Прочитать release notes всех минорных версий. От вашей точки старта до вашей точки финиша. Прочитать все внимательно.
Сергей Бурладян: И попробовать на тестовом сервере провести апгрейд.
Константин Евтеев: С полными данными.
Николай Самохвалов: Да.
Дмитрий Вагин: Ну и у нас тестовое окружение. Вот эти сэмплы все собираются быстренько. Мы быстро обновили версии в образах, накатили туда сэмплы и прогнали все наши тесты, что есть. И все было хорошо. Проверили, что продукт готов к новой версии.
Николай Самохвалов: Потом какое-то время пожили с этим, наверное… Или прям уже все?
Дмитрий Вагин: И ночью понеслось.
Николай Самохвалов: Уже ночью? Ок.
Константин Евтеев: И еще один совет, раз говорили про битые данные. На конференции в Оттаве был отличный доклад про то, как могут ломаться ваши данные, как могут биться. Что может быть: авария, битые страницы, битые индексы, commit lock, все, что угодно. И там основной позыв был к тому, что железо сейчас достаточно дешевое, и покупайте хорошую систему хранения, которая правильно делает fsync, потому что после аварии потом восстанавливать будет очень сложно и дорого.
Николай Самохвалов: Кто доклад делал? Помнишь?
Константин Евтеев: Christophe Pettus.
Николай Самохвалов: CEO PostgreSQL Experts из Сан-Франциско. Он хорошие доклады, кстати, делает. Это правда. Я вам хотел задать вопрос. Почему про такие штучки не пишите у себя в блоге, на Хабре или где-то еще? Можно же маленькую статеечку сразу написать. Не нужно что-то большое.
Константин Евтеев: Пишем и будем писать. То есть интересная статья от нас была, когда Дима с Николаем Воробьевым реализовали решение, когда был такой кейс, что хранимка где-то неоптимально была написана. Но из нее вызывалась еще одна, из нее еще одна. И стек вызовов был очень большой. Такое место тупило, было непонятно. Соответственно, было придумано решение – через notify отправлять непосредственно время исполнения каждого блока. И это решение было реализовано буквально за пару часов. И, соответственно, мы написали статью про это — PG Metricus.
Николай Самохвалов: А pg_stat_statements, есть опция, чтобы потроха функции было видно. Не помогало?
Дмитрий Вагин: Там слишком много будет потрохов. И в этом всём просто не найти.
Николай Самохвалов: Вам нужен был stack trace.
Дмитрий Вагин: Нам нужна была просто одна функция, посмотреть, что именно в этой функции работает дольше, чем нам хотелось бы. Она работала, допустим, в среднем нормально, но были какие-то всплески.
Николай Самохвалов: И это, получается, в listen/notify notify вы шлете.
Дмитрий Вагин: Да, мы оттуда notify делаем. И у нас есть демон, который слушает именованный канал, и это все просто берет и переправляет в brubeck. И все. И в grafana строим dashboard.
Николай Самохвалов: А у вас Хабр, получается, основной канал, где вы такие штуки пишете?
Константин Евтеев: Да, основной канал, где мы что-то публикуем и анонсы делаем – это Хабр.
Николай Самохвалов: Наверное, все. Если больше не хотите ничего сказать, я думаю, очень хорошо и насыщенно получилось.
Константин Евтеев: Хотел добавить, что все вопросы, вообще в целом вся наша запись будет расшифрована и на Хабре появится. И все вопросы в том числе можно будет также задавать будет в этой статье и обсуждать в комментариях.
Николай Самохвалов: Спасибо большое. Очень было интересно. Я для себя тоже кое-что интересное узнал.
Константин Евтеев: Спасибо.
Николай Самохвалов: До встреч в следующих эфирах.
User
=====
Исследование движений глаз: айтрекинг без видеокамеры и иные решения
2017-12-15, 13:08
User
=====
Как я стала тестировщиком. Спойлер: не сразу
2017-12-15, 13:29

Когда у молодого спеца что-то не получается, он занимается самокопанием и начинает думать, что у него не получится вообще ничего. Так как людям часто свойственно видеть корень зла в окружающем — обстоятельствах/начальниках/коллегах, — на поиски причин в себе времени уже не остается. Но, как мне кажется, оба подхода не до конца верные.
В начале карьеры мы склонны принимать на свой счет все неудачи в работе, что в конечном итоге приведет к демотивации и самокопанию. Нечто подобное в свое время случилось и со мной. Возможно, происходило и с вами — или даже происходит сейчас. 
Поэтому я просто расскажу свою историю смены специальности — надеюсь, кому-то она послужит недостающим толчком к действию.
Сейчас я работаю тестировщиком в Яндекс.Деньгах. Мне 23 года, и по специальности я «Информатик-экономист» (и это не «информатик» минус «экономист»).
Работать начала с первых курсов университета, хотя это скорее была подработка: репетитор по математике, оператор в отделе бронирований паромной компании. А перед последним курсом университета попала инженером Service Desk в крупную японскую туристическую корпорацию.
Это была не обычная «первая линия поддержки», которая в общих случаях регистрирует обращения и направляет их ответственным инженерам. Мы администрировали внутреннюю систему документооборота, настраивали рабочие станции, систему безопасности, разбирались в сетевых проблемах, заявках связанных с офисными продуктами, VPN и т.п.
В общем, в народе таких специалистов называют «шивами».
Во время обучения в университете я всячески старалась получить новый опыт, чтобы по окончании найти достойную работу: участвовала в научных конференциях (это быстро разочаровало, так как на таких конференциях мало науки, в основном пересказы уже существующих трудов), писала научные статьи, занималась фрилансом, проходила обязательную ежегодную практику. В общем, классическое «учись на отлично — и все у тебя будет».
С практикой вообще всегда интересная история. Когда поступаешь в университет, волонтеры в приемной комиссии рассказывают про крупные компании, с которыми сотрудничает учебное заведение, про обеспечение рабочими местами. Ты ждешь лета, чтобы скорее начать применять все свои знания на практике, познакомиться с реальными рабочими процессами и задачами, пусть и простыми. Но фактически мест для практики очень мало, а университет предлагает летом поработать на кафедре, и то не всем, поэтому все приходится искать самостоятельно.
Через год я попала на практику в банк. Тогда мне казалось это чем-то невероятным, потому что отбор был серьезный. Несколько собеседований, тесты, много кандидатов. Перед началом работы сотрудники банка (рекрутеры, будущие руководители, специалисты ИБ) спрашивали подробный план и цели практики. Помню, как тогда рассказывала им что-то про «описание ключевых бизнес-процессов в модных нотациях, описание архитектуры ИТ, как рекомендует TOGAF».
В первый день практики нас (практикантов) посадили в кабинет руководителя резать ножницами устаревшие рекламные буклеты. Мы справились. В последующие дни нам дали более ответственную работу… Раскладывать по алфавиту юридические дела клиентов. Задание, конечно, сложное, долгое, но я забила фамилии всех клиентов в Excel, отсортировала (прикладная информатика же), а потом просто раскладывала стопки папок по списку. 
Никто не ожидал, что мы так быстро справимся, поэтому надо было придумывать еще какое-то задание. В конце концов меня посадили работать в БИС (банковская информационная система), опыт взаимодействия с такими системами у меня был в университете, было интересно. В банке внедряли новый модуль анализа, а я находила ошибки, которые потом уходили в качестве пожеланий к разработке. 
Тогда я еще ничего не знала о тестировании в целом и существовании такой специальности в частности, но уже всем этим занималась.
Приближался последний курс университета, времени было много, а опыта мало, поэтому я и пошла работать инженером Service Desk. 
В первые месяцы работы я сразу заметила, что в ИТ-инфраструктуре компании не все гладко. Например, у всех инженеров из-за специфики работы в международной компании было разное расписание (чтобы поддерживать офисы в Китае или Америке, кто-то работал ночью, а на следующий день после ночной смены отдыхал и был недоступен). Сотрудники ИТ были раскиданы по миру (эксплуатация — в Санкт-Петербурге, ИТ-директор — в Копенгагене, разработка — в Индии и еще много локальных сапортеров по всем офисам). По этим причинам порой было трудно найти быстрое решение или ответственного за выполнение каких-то задач. 
Я взялась за несколько проектов по улучшению и оптимизации процессов, поэтому приходилось много общаться со всеми сотрудниками и менеджерами, собирать их пожелания, писать документацию и решать возникающие проблемы. Еще довелось рисовать интерфейсы, оформлять четкие и понятные ТЗ для разработчиков. Порой это все очень затягивалось, поэтому я стала писать код сама, чтобы быстрее внедрить какую-то свою задумку.
Помимо таких задач, я часто делала выгрузки и анализ инцидентов. Хотелось понять, где в эксплуатации самое узкое место, каких знаний не хватает, каких специалистов. Так сложилось, что в ИТ-департаменте все привыкли «тушить пожары» вместо того, чтобы их предотвращать. Попутно я рассказывала коллегам про ITIL и ITSM, всем очень нравилось на словах, но поддерживать изменения горели желанием немногие.
Так прошло чуть больше года, из-за работы на энтузиазме задач прилетало все больше, на свои мини-проекты времени не оставалось совсем. Из-за некорректно выстроенных процессов количество задач не убывало совсем. Складывалось ощущение бурной, но бесполезной деятельности. 
В это время в компании случились крупные инциденты в области безопасности, в сеть утекли персональные данные клиентов. У японцев с этим очень строго, компании было сделано предупреждение от правительства, и следующий такой инцидент мог привести к закрытию фирмы. Я стала инженером в группе информационной безопасности, а компании предстояло внедрить разные системы ИБ. 
Так получилось, что новых профильных специалистов решили не нанимать и нагрузка по новым задачам легла на имевшийся штат. А чтобы было еще интереснее, четкого представления о конкретных ожиданиях и задачах тоже не поступило. Опыта, чтобы на чем-то остановиться и приступить к делу, не хватало: вот ты читаешь описание системы, уточняешь детали у вендоров, но на деле получаешь два абсолютно одинаковых решения, которые отличаются названием и ценой. И, конечно, тонкими техническими особенностями реализации, которые понятны тем самым отсутствующим профильным специалистам.
Долгое топтание на месте, отсутствие результатов деятельности, привело к мыслям о том, что ИТ — не мое. Совсем. 
Большая часть моих друзей тоже из ИТ, системные администраторы, разработчики, специалисты в ИТ-консалтинге. Общаясь со знакомыми, я поняла, что в других сферах все устроено совсем иначе, чем в эксплуатации. 
Стать разработчиком в короткие сроки показалось невозможным, хотя я и писала код на разных языках, но в основном для решения прикладных задач — без использования фреймворков, конвенций. С работы хотелось уйти быстрее, а всякие курсы «Стань Java Junior Developer за 9 часов» не внушали доверия. Разработчик должен знать алгоритмы, фреймворки, паттерны, особенности языков. Если ты разработчик в Agile команде, то часто выступаешь еще и архитектором системы, а для этого нужен опыт. 
Мой друг посоветовал мне пойти в тестирование, и я решила попробовать. Прочитала пару книг по тестированию, посмотрела известных QA инженеров, а также курсы Яндекс.Денег и Mail.ru. Меня впечатлили два совершенно разных подхода: одни учат тебя администрированию, техническим особенностям того, с чем тебе придется работать, базовому языку ИТ, другие — общим концепциям, анализу, тест-дизайну, планированию, покрытиям, оптимизации. 
Все это было на словах, а хотелось больше практики тестирования веб-сервисов. Поэтому для подготовки я решила написать свое небольшое веб-приложение на Java (спасибо ресурсу StackOverFlow), с несколькими «ручками» и БД, а потом тестировать это приложение по «правилам». Такой опыт помог мне больше узнать об особенностях архитектуры веб-сервисов и инфраструктуры, а еще о рабочих инструментах. Эти знания я использую каждый день при тестировании или подготовке тестовых сред.
Уже прошло около семи месяцев, как я — довольный жизнью тестировщик. Передо мной всегда есть список задач, расставленных по приоритетам менеджером проектов, вытекающих из общих целей. Я знаю, куда движется продукт и команда, вижу результаты нашей работы.
По работе нужно взаимодействовать с разработчиками, поэтому я узнаю много тонкостей разработки. А также получаю опыт организации и контроля работы, ведь многие вещи (баги, например) в первую очередь проходят именно через наш отдел:
тестировщики первые настраивают новые фичи на тестовых средах (кстати, тут очень помогают навыки администрирования);
Университетские навыки прикладника мне помогают смотреть на системы под разными углами, а значит, и проверять их качество эффективнее.
Если говорить о советах, которые, исходя из своего опыта, я могу дать молодым специалистам, то я бы выделила следующее:
Если кажется, что работа вам не подходит — скорее всего, так и есть. Не надо бояться менять род деятельности, лучше попробовать рано, чем осознать, что вы занимались «не вашим» делом много лет. За вас жизнь никто лучше не сделает.
Относитесь к любой задаче как к отличной возможности получить новый опыт. Даже если кажется, что сейчас этот опыт ни к чему, а задача скучная и неинтересная, вы не знаете, как сложится в будущем. 
Кстати, хочу поделиться небольшим списком литературы, который в свое время мне очень помог быстро втянуться в работу тестировщика:
«A Practitioner's Guide to Software Test Design», Lee Copeland;
«Foundation Level Syllabus», ISTQB;
Тестировщик
=====
Снятся ли IT-рекрутерам круглые канализационные люки?
2017-12-15, 13:46
Разработчик JavaScript
=====
Выпуск#4: ITренировка — актуальные вопросы и задачи от ведущих компаний
2017-12-22, 14:10
.NET разработчик
=====
Нескучный API
2017-12-15, 14:22
Как создать АПИ для умных? Такое апи, чтобы создание клиента для него было не скучным механическим процессом, а настоящим приключением с элементами детектива, хоррора и мистики? Такое апи, о котором пользователи будут взахлёб рассказываете коллегам? Апи взрывающее мозг, заставляющее смеяться, кричать и плакать? Я постарался отобрать лучшие практики, с которыми пришлось столкнуться.


Не повторяйся.
a. Придумай разные системы для обозначения состояний разных объектов. К цифровой, описанной выше, можно добавить цветовую: green, red, yellow.
b. Меняй не только формат данных, но и формат записи названий полей — srart_date: 25/12/05, endDay: 2012.11.23.
c. Не забудь про огромный выбор разделителей: запятая, точка, точка с запятой. Добавь в него что-нибудь нестандартное — знак процента или решетку.
j. Список можно продолжать, будь креативен.
Добавьте в запрос обязательный параметр, у которого может быть только одно значение.
Документация не нужна.
a. Если пришлось документировать, пропусти наиболее “очевидные” места.
b. Не усложняй документацию примерами.
c. Задокументируй отсутствующий функционал, который ты обязательно допишешь потом.
d. Раздели документацию на несколько частей и каждую отдавай только тем пользователям, которые догадаются про неё спросить.
Этот список основан на реальных событиях, имена изменены в интересах конфиденциальности, все совпадения не случайны. 
P.S. Напишите о своих героях и их былинных деяниях в комментариях.
Заинтересованный наблюдатель
=====
WebRTC: как два браузера договариваются о голосовых и видеозвонках
2017-12-18, 13:57
Программист, Технический писатель, DevRel
=====
Как теория ограничений помогает зарабатывать больше — личный опыт Логомашины
2017-12-15, 16:37
Предприниматель
=====
Почему компания меня потеряла
2017-12-15, 14:56
Research&Development
=====
Мониторинг ошибок на страницах сайта с помощью Яндекс.Метрики
2017-12-15, 15:26
Программист
=====
Intel vs AMD: сравнительные тесты
2017-12-21, 11:13
Пользователь
=====
Новый подход к спортивному анализу данных: какие шаблоны «рвет» SDSJ
2017-12-25, 15:07

=====
Объяснение SNARKs. Спаривание эллиптических кривых (перевод)
2017-12-26, 19:05
Архитектор программного обеспечения
=====
Краткий справочник информатики
2017-12-15, 19:12
Область ИТ растёт, и легко заблудиться в зоопарке подходов, фреймворков и технологий, которые громко заявляют о своей "новизне" и "эффективности". Но за обёрткой обычно скрываются старые добрые идеи, заново "изобретённые" в другом контексте. В итоге распространяется не самая простая и эффективная, а самая разрекламированная реализация. Разработчики не успевают вдумчиво произвести выбор из-за постоянного недостатка времени, а менеджеры выбирают самое распространённое, чтобы снизить риски при поиске разработчиков.
Для себя я стараюсь свести используемый в индустрии термин или технологию к простому определению или наглядному примеру. Предлагаю справочник, очень краткий, и поэтому неполный и не претендующий на точность.
Тип — это то, над чем компьютер умеет рассуждать. Рассуждать в смысле привычной человеческой или формальной логики, то есть по правилам строить некоторые заключения из исходных посылок. Хорошая новость в том, что компьютер делает это автоматически. Бывают разные системы типов: одни упраздняют определённый вид ошибок, таких как несоответствие типов, утечка ресурсов (Rust Borrow Checking); другие автоматически генерируют реализацию (Haskell Type Class, Scala & Rust Trait). Когда типы легко воспринимается человеком, то служат документацией. Сильные системы типов могут за вас делать больше работы, чем слабые. А статическая обработка типов сделает эту работу раньше, при компиляции, а не позже, при выполнении, как динамическая.
Класс в ООП — это одна из систем типов, которая позволяет рассуждать о внутренней структуре, какие внутри есть поля и методы. Аналоги: запись, кортеж, тип-произведение.
Наследование в ООП смешивает как минимум 4 идеи, которые стоит реализовывать отдельными способами:
Pattern Matching деконструирует алгебраический тип-сумму или тип-произведение, то есть это обратная конструктору операция.
Объект в ООП — замыкание, где свободные переменные являются полями, желательно приватными.
ООП — популярный бренд, исторически сложившийся, но не уникальный набор идей без единого центрального стержня.
Полиморфизм бывает разным.
Типаж, интерфейс — тип-произведение, состоящее, в основном, из функций.
Шаблон проектирования — неформальное реализация идеи вне языка программирования, поскольку данный язык не поддерживает необходимый уровень абстракции. В другом, более высокоуровневом языке, шаблон возможно реализовать средствами языка.
Visitor — шаблон проектирования, реализующий Pattern Matching.
Builder — шаблон проектирования, реализующий функцию высшего порядка, которая принимает части и выдаёт продукт.
Dependency Injection — шаблон проектирования, реализующий функцию высшего порядка, которая принимает зависимости и выдаёт продукт. Изоморфен шаблону Builder с поправкой на то, что зависимости, обычно, являются функциями.
Клиентская Web разработка — востребованный исторически сложившийся, но не уникальный набор идей, основанный на монопольном положении JavaScript.
MVC, MVP, MVVC — чистая функция и абстрактный автомат для обработки внешних событий.
Event Loop (Node.js, Rust tokio) — абстрактный автомат для обработки внешних событий.
Программирование — инженерная наука о композиция кода.
Теория категорий — фундаментальная теория о композиции чего угодно.
Изоморфизм — превращение или замена чего-либо во что-то другое и обратно, то есть суть изоморфных вещей одна.
Рефакторинг — изоморфизм, выполняемый людьми.
Оптимизация — изоморфизм, выполняемый компьютером.
(Чистая) функция — превращение одного в другое всегда одинаково.
Функтор превращает один тип в другой (функция на типах), да так, что можно оптимизировать композицию чистых функций. Например функтор "список" может превратить тип "строка" в тип "список строк", а тип "число" в тип "список чисел". Применим функцию "длина" к каждому элементу списка строк и получим список чисел. Добавим 1 к каждому элементу списка чисел и получим новый список чисел. А можно заменить (вручную отрефакторить или автоматически оптимизировать) пару проходов по спискам на один составной, который найдёт длину строки и сразу добавит 1, но главное, пропадёт промежуточный список.
Монада превращает один тип в другой, да так, что можно компоновать функции с побочным эффектом. Например, монада "ожидание" (Future, Promice) имеет побочным эффектом ожидание завершения длительной операции и преобразует тип "массив байт" в тип "ожидание массива байт", а тип "подтверждение" в тип "ожидание подтверждения". Передадим функции чтения имя файла и подождёт массив байт с диска. Затем передадим полученный массив байт функции отправки по сети и подождём, когда клиент подтвердит получение. А можно заменить пару ожиданий на одно составное (комбинатором (>>=), bind, and_then), которая сначала подождёт массив байт с диска, а потом подождёт подтверждения от клиента по сети, но главное, пропадёт явное промежуточное ожидание, позволив среде исполнения в это время заниматься другими полезными делами.
Двойственность — вывернутое на изнанку тоже работает. Например, из того, что тип-сумма двойственен типу-произведению, следует, что функцию, принимающую яблоки или груши (тип-сумму) можно заменить на пару (тип-произведение) функций: одна говорит, что делать с яблоками, а другая — что с грушами.
Вариантность говорит, что вывернутое на изнанку работает прямым (ковариантность) или вывернутым образом (контравариантность). Например, функции, которая ест тип яблоки или груши можно скормить яблоки, но нельзя тип яблоки или груши или персики потому, что персиками она подавится. Более всеядную функцию создать сложнее и их меньше. Больше аргумент — меньше функций с таким аргументом — это и есть "работает вывернутым образом", то есть функция контравариантна по аргументу. А вот по результату функция ковариантна.
Вычисления можно производить да хоть на камушках.
Квантовые вычисления работают не с битами, а с кубитами. Например, вместо шариков в Marble adding machine можно кидать Котов Шрёдингера. Например, пара котов даст 4 варианта: оба живых, первый живой, второй живой, нет выживших. В итоге за один запуск машины мы получим суммы всех возможных чисел. Проблема только в том, как превратить Котов Шрёдингера внизу машины в обычных котов, полезных в хозяйстве.
Зависимые типы — работа с типами и оптимизациями кода тем же способом (Pattern Matching и вычисления), что и со значениями. Зависимые типы позволяют переложить на компьютер работу с вычислимыми шаблонами и даже саму математику. Например, можно указать, что тип "множество элементов", когда элементов не более 64, можно заменить на тип u64, который поместится в регистр. Или компилятор может удостовериться, что размеры складываемых векторов одинаковы.
Дополнения и исправления принимаются.
Full Stack
=====
Что говорили о JavaScript в 1995-м
2017-12-15, 16:19
Пользователь
=====
Кривые Безье и Пикассо
2017-12-22, 11:23
























Переводчик-фрилансер
=====
Задачки с ZeroNights 2017: стань королем капчи
2017-12-15, 18:06
Пользователь
=====
Нагрузочное тестирование на фреймворке Gatling
2017-12-15, 18:10
Статья публикуется от имени Масленникова Сергея, sergeymasle
UPD. Добавлен раздел "Реализация расширения для Gatling"

Продолжаем цикл статей про нагрузочное тестирование на фреймворке gatling.io. В этой статье расскажем про основные приемы использования Gatling DSL, которые в большинстве случаев используются при разработке любых скриптов нагрузочного тестирования. Итак, прошу под кат.
В предыдущей нашей статье мы писали про установку SBT и настройку окружения для фреймворка Gatling. После выполнения всех действий у вас должен появится проект(1), структура которого представлена на изображении ниже.

В файле plugins.sbt(2) должен быть подключен плагин gatling для sbt, если файл не был создан, то создайте его вручную. Обратите внимание, что необходимо указывать свежую версию плагина, на момент написания статьи она является 2.2.2. Код файла ниже.
Далее в каталоге src/test необходимо создать каталог resources(3). В нем располагаются файлы настроек, а также тестовые данные. Файл gatling.conf содержит основные настройки, logback.xml отвечает за уровень логирования и интерфейсы вывода логов. Эти файлы можно взять бандла https://gatling.io/download/.
Директория scala содержит пакеты с тестами. Имена пакетов можно называть как угодно, но как правило компании используют свое инвертированное имя ru.tcsbank.load. 
Файл BasicSimulation основной файл тестов и является точкой входа запуска скриптов.
В директории target\gatling(6) генерируются отчеты по логу запуска (тот что вы видите в консоли). Следует заглядывать туда чаще — она очень быстро растет.
Главный файл проекта — build.sbt(7). Содержит зависимости на все библиотеки, что вы подключаете. Именно в нем указывается ссылка на фреймворк Gatling, его код ниже.
Последний важный файл — это gatling.log. именно в нем можно увидеть отправляемые запросы и ответы. Чтобы видеть все запросы не забудьте раскомментировать строчку "ALL HTTP" в файле logback.xml.
На первый взгляд скрипты и DSL Gatling могут показаться сложными, но если понять идею построения скриптов, то все станет довольно просто.
Gatling представляет виртуального пользователя в виде сценария scenario(). Для сценария указывается количество пользователей через метод inject, модель нагрузки, а также настройки протокола http и различные условия эмуляции. Все это указывается в конструкции setUp(). Для примера, при проведении нагрузки интернет магазина для покупателя и администратора будут несколько сценариев:
Такое разделение позволяет легко корректировать нагрузку увеличивая количество виртуальных пользователей для конкретных сценариев.
Сценарий представляет собой цепочку выполнений именно в эту цепочку внутри функции exec() помещаются запросы.
Цепочка выполнений начинается с выражения scenario(<Name>) и далее собирается путем вызова функции exec(). 
Если проводить сравнения эмуляции виртуальных пользователей JMeter и Gatling, то можно выделить некоторую особенность. В JMeter пользователи помещаются в катушку ThreadGroup, где задается их количество и именно она(катушка) многократно воспроизводит скрипт виртуальных пользователей по циклу. Т.е. при "поднятии" двух виртуальных пользователей они будут выполнять один и тот же сценарий пока не закончится время теста. 
Gatling управляет виртуальными пользователями несколько иначе. При поднятии двух виртуальных пользователей они выполнят свой сценарий и на этом закончат свою работу. Для того, чтобы пользователи выполняли сценарий в цикле необходимо помещать цепочку в блок цикла. Рассмотрим простой скрипт теста, который представлен на сайте https://gatling.io/docs/current/quickstart/#gatling-scenario-explained, его можно взять за основу.
Сценарий скриптов на первый взгляд может выглядеть сложно, но если разобраться, то DSL Gatling довольно простой и для написания нагрузочных тестов углубленное знание Scala не требуется. 
Сценарий скриптов начинается с присвоения константе функции scenario(). Имя сценария должно быть уникальным! Далее вызывается функция exec(), которая принимает на вход другие функции, реализующие тестовые сценарий: http и websocket. Именно в ней выполняются все действия для эмуляции запросов.
Когда сценарий скриптов написан, в функции setUp() мы указываем какое количество пользователей будет его выполнять и как эти пользователи будут выходить на нагрузку. Далее разберем подробно, как с этим всем работать.
Фреймворк по умолчанию поддерживает следующие методы GET, POST, PUT, PATCH, DELETE, OPTIONS. Рассмотрим в качестве примера написание запросов GET и POST. Для начала присвоим константе scn функцию сценария и напишем в exec() простой GET-запрос:
Если же необходимо установить headers, то добавляем следующее:
Передаем параметры в запрос:
Переметры также можно передавать непосредственно через функцию .get("http://bar.com/foo.php?param=value"). Если есть загрузка статических ресурсов, то используем resources() для параллельной загрузки.
Для метода POST при передаче параметров используются функция formParam().
Чтобы передать данные напрямую через тело запроса необходимо использовать body().
При эмуляции запросов требуется проверять код ответа или наличие какого-либо текста в теле ответа. Также нередко требуется извлечь данные из ответа. Все это может выполнить с помощью функции check(). Проверки необходимо производить после функции http-метода.
В Session хранятся все данные виртуального пользователя и переменные. Если вы хотите что-то предать в рамках сценария, то это нужно делать через сессию.
Динамические значения нельзя напрямую передать в DSL-функции, так как Scala использует CallByValue они будут получены при компиляции и далее всегда использоваться без получения новых.
Как и в других инструментах тестирования нередко применяются логические конструкции, чтобы выполнять запросы в зависимости от каких-либо условий.
Для проведения качественной нагрузки и тестирования системы, а не ее кэша необходимо отправлять данные в запросах, которые меняются динамически во время теста. Для хранения и получения таких данных самым простым способом является чтение из файла. Файл должен содержать данные разделенные символом. Gatling имеет функции для чтения таких данных.
csv("foo.csv") // данные, разделенные запятой
tsv("foo.tsv") // данные, разделенные табуляцией
ssv("foo.ssv") // данные, разделенные точкой с запятой
separatedValues("foo.txt", '#') // данные разделенные другим символом
На примере небольшого csv файла покажем работу с тестовыми данными:
Gatling при чтении файла использует первую строку как имена параметров, и в последствии при чтении значений сохраняет их под этими именами. Таким образом, в параметр ${model} будут подставляться значения с именами ноутбуков, описанных в csv файле.
Чтобы читать csv-файл, необходимо вызвать функцию csv().
Итак, мы создали переменную feeder и указали имя файла, который лежит в src\test\resources\data.csv. В сценарии мы вызываем функцию feed() и указываем константу feeder. Чтение нового значения происходит каждый раз, когда вызывается функция feed(). 
При варианте #1 функция feed() вызывается до repeat(), таким образом, в переменной ${model} будет использоваться первое считанное значение на 5 итераций. 
При варианте #2 значение будет считываться перед каждым запросом.
Gatling поддерживает различные модели нагрузки. Эти модели отвечают за "подъем" пользователей и генерируемую интенсивность.
nothingFor(duration) — указывается длительность паузы duration перед стартом нагрузки
atOnceUsers(nbUsers) — виртуальные пользователи в количестве nbUsers будут “подниматься” сразу (по готовности).
rampUsers(nbUsers) over(duration) — в течение времени duration будут "подниматься" виртуальные пользователи в количестве nbUsers через равные временные интервалы. 
constantUsersPerSec(rate) during(duration) — указывается частота “поднятия” виртуальных пользователей rate (вирт. польз. в секунду) и временной интервал duration. В течении duration количество виртуальных пользователей будет увеличиваться на rate каждую секунду.
constantUsersPerSec(rate) during(duration) randomized — аналогично верхней конструкции только временные интервалы между "поднятием" виртуальных пользователей будут случайными.
rampUsersPerSec(rate1) to (rate2) during(duration) — в течение времени duration виртуальные пользователи будут увеличиваться с частоты rate1 до частоты rate2.
rampUsersPerSec(rate1) to(rate2) during(duration) randomized — аналогично верхней конструкции только временные интервалы между "поднятиями" виртуальных пользователей будут случайными.
splitUsers(nbUsers) into(injectionStep) separatedBy(duration) — через каждый временной интервал duration будут добавляться виртуальные пользователи по модели injectionStep, пока их количество не достигнет nbUsers. В injectionStep можно указать модели описанные выше.
splitUsers(nbUsers) into(injectionStep1) separatedBy(injectionStep2) — аналогично верхней конструкции только разделителем модель injectionStep2.
heavisideUsers(nbUsers) over(duration) — виртуальные пользователи в количестве nbUsers будут подниматься ступенями за время duration.
Для запуска нагрузки самый простой способ — это использовать bundle. Необходимо поместить файл скрипта в gatling-charts-highcharts-bundle-2.3.0\user-files\simulations\ и далее запустить gatling-charts-highcharts-bundle-2.3.0\bin\gatling.bat. В консоли будет предложен выбор скрипта для запуска.

Наш скрипт под вариантом 6. После выбора произойдет генерация нагрузки с выводом информации в консоль.
Этот вариант предполагает запуск нагрузки непосредственно из IDE IntelliJ IDEA Community.
После того, как произвели все действия по настройке библиотек, нажимаем ALT+F12 и открываем терминал. В терминале набираем команду sbt.

После загрузки всех компонентов производим запуск скриптов командой gatling:testOnly.

В консоли будет отображаться текущее состояние нагрузки.
Чтобы производить запуск из панели запуска IDEA, необходимо добавить нашу команду на запуск в SBT Task.

В документации по Gatling написано, что из коробки существует поддержка протоколов только для HTTP/1.1 и WebSocket. Также имеются официальные и неофициальные расширения для Gatling, которые доступны по ссылке(https://gatling.io/docs/2.3/extensions/).
Нередко случаются задачи, когда необходимо протестировать под нагрузкой систему у которой протокол прикладного уровня отличный от HTTP или WebSocks. В таком случае для Gatling можно написать свое расширение и реализовать необходимый функционал.
И так, нам необходимо реализовать вот такую возможность:
Так как функция exec() может принимать тип ActionBuilder необходимо написать свой класс и расширить его типом ActionBuilder.
В переопределенной функции build нужно создать экземпляр класса, который будет реализовывать необходимый код. Данный класс необходимо расширить от ChainableAction.
Ниже рабочий пример данного подхода. Важно отметить, что такой способ не является лучшим решением, но оно максимально "простое" для реализации.
В этой статье были описаны основные моменты, которые помогут самостоятельно разработать скрипты для проведения нагрузки на фреймворке Gatling. Если возникли вопросы, то с удовольствием ответим на них в комментариях.
Пользователь
=====
Грязные трюки и оперативка
2017-12-16, 10:42
Переводчик-фрилансер
=====
Глубокое обучение при помощи Spark и Hadoop: знакомство с Deeplearning4j
2017-12-15, 17:36
User
=====
Микроядро vs. монолит и «торжество» MINIX
2017-12-15, 18:43
Недавно Эндрю Таненбаум, профессор Амстердамского свободного университета, автор учебной и миниатюрной Unix системы Minix, вновь оказался в центре событий благодаря эпистолярному жанру. В своем письме Интел он поблагодарил компанию за использование Minix, посетовал на то, что та не трубила об этом на каждом шагу и заявил, что из-за этого мало кто знает о том, что Minix — на сегодняшний день самая популярная ОС на свете.

Надо отдать должное профессору, он умеет выбирать адресата, время и место для того, чтобы вызвать громкий и продолжительный эффект с помощью простого сообщения, отправленного по электронной почте. Его предыдущим корреспондентом был Линус Торвальдс, а их переписка о монолитном и микро ядре вошла в анналы истории ИТ. Без этого трудно понять, почему Эндрю Таненбаум так экзальтирован из-за мнимого успеха Миникс, которая всего лишь в течении десятка лет обеспечивала работу интеловского бэкдора IME.
26 лет назад изучение программирования для Unix было нетривиальным делом для обычного студента, так как все разновидности Unix были платными. Чтобы освоить эту операционную систему Линус решает поставить Minix. Интернет в ту пору еще только зарождался, заказ ОС шел через обычную почту, так же как и доставка. Ради Minix пришлось раскошелиться на 169 долларов.
Вскоре будущий создатель Linux обнаружил серьезные недостатки Minix. Так как это был всего лишь обучающий вариант Unix, то профессор преднамеренно исковеркал ее. Многие из этих недостатков можно было устранить заплаткой самого известного хакера Minix Брюса Эванса, но для того, чтобы ее поставить нужно было изрядно провозиться. Самым же существенным недостатком для Линуса была программа эмуляции терминала, которую пришлось заменить на свою собственную. Затем понадобился драйвер файловой системы и понеслось, ядро новой ОС зародилось по принципу каши из топора.
25 августа 1991 г. Линус отправляет свое знаменитое сообщение о том, что работает над бесплатной операционной системой, но это будет не такой крупный и профессиональный проект как GNU. Помимо всего прочего внимание заслуживает тот факт, что этот и другие ранние анонсы свое операционной системы Линус отправляет в конференцию Minix, оттягивая на себя пользователей последней.
Эндрю Таненбаум до поры до времени никак на это не реагировал, но Linux рос как снежный ком. Уже в январе 1992 г. вышла версия 0.12, в котором была реализована страничная подкачка на диск — то чего не было в Minix. Вскоре после этого профессор снизошел до выскочки, чтобы лично ему ответить и вот 29-го января Линус получает сообщение в конференцию comp.os.minix с нравоучительным содержанием. Начало было обнадеживающим.
Далее следовали справочные сведения о монолитном ядре, микроядре и об ОС, исповедующих тот, или иной принцип. Затем следовал несостоятельный с точки зрения логики довод о том, что среди специалистов по разработке операционных систем споры по данному вопросы уже прекратились ввиду явного преимущества микроядра. Дальше декларации о том, что Minix прогрессивна, а Linux — возврат в 1970-е. Кроме того, Linux привязан к одной архитектуре в то время как Minix был перенесен с Intel процессоров на другие платформы: Atari, Amiga, Macintosh, SPARC и NS32016.

В начале 90-х микроядро действительно было в фаворе у проектировщиков операционных систем. По их мнению ядро ОС должно быть минимальным и содержать лишь самое необходимое: управление памятью, планировщик и IPC, а все остальное реализуется в виде сервисов. Разбив целое на множество простых частей, сложность исчезает, а легковесные сервисы без труда обмениваются данными с микроядром. Сбой в драйвере файловой системы или сетевой карты, таким образом элементарно восстанавливался перезагрузкой соответствующего сервиса.
Линус придерживался другого мнения на сей счет. Отдавая должное элегантности и изяществу архитектуры микроядра с теоретической точки зрения, он тем не менее считал микроядро не пригодным для практических целей. Мнимая простота микроядра оборачивается тем, что взаимодействие и интерфейс между простыми частями микроядра создает сложности, которые нивелируют все ее «бумажные» преимущества. В своем ответе он изложил свое видение по данному вопросу. После дерзких выпадов против своего оппонента Линус переходит к сути.
Затем перечисляет проблемы Minix с многозадачностью в файловой системе.
Перепалка в конференции продолжается, в спор вступают новые участники. Эндрю Таненбаум и Линус Торвальдсь продолжают спор, но уже более в более сдержанной манере. Ниже вольный перевод избранных цитат.
Аргументы в пользу микроядра в то время действительно перевешивали, но на сегодняшний день опыт использования обоих принципов построения ОС внес коррективы.
На этом первая часть Марлезонского Балета завершилась. Первый раунд битвы остался за профессором, Линус взял свои слова обратно, первым вышел из спора, однако не изменил своего мнение, так в главном он был прав — его ОС выигрывала, потому что была готова. Спустя 14 лет спор знаменитого теоретика и практика продолжился.
BigData
=====
Квантовые вычисления: отжиг с выключателями и прочее веселье
2017-12-18, 17:07
Пользователь
=====
Оставайся ленивым с angular/cli
2017-12-15, 18:19
Пользователь
=====
Во Франции биткоины — не деньги
2017-12-15, 18:39
юрист IT, personal data
=====
Нехватка оперативной памяти в Linux на рабочем ПК: оптимизация и действия при зависании
2017-12-15, 19:47
Много чего делаю…
=====
Туториал по Unreal Engine. Часть 6: Анимация
2017-12-21, 11:25
Переводчик-фрилансер
=====
Описание бизнес процессов. Использовать осторожно
2017-12-18, 19:13
It консультант
=====
Суррогаты
2017-12-15, 21:10
Биоробот
=====
Непоправимые последствия HolyJS 2017 Moscow
2017-12-18, 13:44
Director of Development
=====
Как подружить Skype и прокси
2017-12-16, 08:04
User
=====
Две компетенции аналитика
2017-12-16, 07:28
Пользователь
=====
Parcel — пишем плагин
2017-12-22, 08:34

В прошлой статье я рассказал про новый бандлер Parcel, который не требует конфигурирования и готов к бою сразу после установки. Но что делать, если вдруг стандартного набора ассетов не хватает? Ответ прост — написать свой плагин.
Напомню, что из коробки нам доступны следующие ассеты:
Для создания своего ассета мы можем выбрать два пути — использовать существующий (JSAsset, HTMLAsset и т.д.), в котором переписать или дописать часть логики, или написать с нуля, взяв за основу класс Asset.
В качестве примера я расскажу, как был написан плагин для Pug.
Для начала нужно разобраться, каким образом Parcel работает с плагинами и что они вообще могут делать?
При инициализации бандлера (Bundler) происходит поиск пакетов в package.json, начинающихся с parcel-plugin-. Каждый найденый пакет бандлер подключает и вызывает экспортированную функцию, передавая ей свой контекст. В этой функции мы и можем зарегистрировать свой ассет.
Наш ассет должен реализовать следующие методы:
А так же можно реализовать необязательные методы:
Для работы с AST есть несколько официальных пакетов:
Создадим следующую структуру проекта:
Файл index.ts будет являться точкой входа в наш плагин:
Для работы с ассетом нам понадобится базовый класс Asset. Напишем TypeScript-обвязку для нужных нам модулей:
Файл PugAsset.ts — наш ассет для преобразования файлов шаблонизатора в HTML.
Начнем с превращения текста шаблона в AST. Как я уже говорил, когда бандлеру попадается какой-либо файл, он пытается найти его ассет. Если он был найден — происходит цепочка вызовов parse -> pretransform -> collectDependencies -> transform -> generate. Наш первый шаг — реализовать метод parse:
Далее нам нужно пройтись по построенному дереву и найти любые элементы, в которых могут находиться ссылки. Механизм работы достаточно прост и был подсмотрен в стандартном HTMLAsset. Суть — составить словарь с атрибутами HTML-узла, которые могут содержать ссылки. При прохождении по дереву нужно найти подходящие узлы и скормить содержимое атрибута со ссылкой в метод addURLDependency, который попробует найти необходимый ассет в зависимости от расширения файла. Если ассет найден — метод вернет новое название файла, попутно добавив этот файл в дерево сборки (таким образом и происходит вложенное преобразование других ассетов). Это название нам нужно подставить вместо старого пути. Так же нам нужно учесть то, что все подключенные файлы (include и extends) нам нужно добавить как зависимости данного ассета, в противном случае при изменении подключаемого или базового файла у нас не будет происходить пересборка всего шаблона.
Финальный штрих — получение итогового HTML. Это — обязанность метода generate:
Если собрать все воедино мы получим следующее:
Наш плагин готов. Он умеет принимать на вход шаблоны, превращать текст в AST, разрешать все внутренние зависимости и выдавать на выходе готовый HTML, корректно распознает встроенные конструкции include и extends, а так же умеет пересобирать весь шаблон, в котором присутствуют данные конструкции.
Из мелких недоработок — при возникновении ошибки ее текст дублируется, что является особенностью вывода Parcel, который оборачивает в try catch вызовы функций и красиво печатает вылетающие ошибки.
Frontend-разработчик
=====
Утренний дозор, или вступайте в радио-робингуды
2017-12-16, 13:59
Я всегда считал себя везучим. Так редко бывая в Москве, я все же поймал сигнал, который переносит всех во Внуково. Но не всё коту масленица. Последние два раза я не попал на этот праздник РЭБ и РР (радио-электронной борьбы и радио-разведки), пришлось уехать в Питер ни с чем. Это понятно, что после белой полосы всегда приходит черная, но мне хочется взять ситуацию под контроль с вашей помощью. 
Москва и уже даже Питер полнится слухами. Именно слухами, а не четкой информацией, хочу заметить. Вот заметка, под которой в комментариях все, что угодно, но только не подтверждение. Кто-то пишет, что было, кто-то, что не было. Вот сообщение о телепортации в Пулково жителей Питера в конце октября. И опять ни одного подтверждения от читателей. Когда это было? Было ли здесь такое раньше? Откуда шло излучение?
Тут можно понять людей, безопасность первых лиц и все такое, но вот заголовки про таксистов, использующих помехи и спуффинг — один, два и три — уже что-то из ряда вон! Кто их поймает, если даже нельзя сказать правда это или нет? Всегда можно сказать, что пользователю такси-сервиса что-то почудилось.
А ведь за границей уже работают над созданием систем мониторинга помеховой обстановки в навигационном диапазоне, и даже призы получают на конкурсах. Они собираются мониторить помехи GPS с использованием смартфонов пользователей.
Мы пойдем другим путем!
Я предлагаю другой путь, более точный и профессиональный.
Сейчас у населения России на руках скопилось огромное количество радио-оружия — недорогих SDR-плат. И власти изымать это оружие не собираются, да и не смогут, конечно. Я предлагаю использовать их малую часть для мониторинга радио-обстановки.
Идеально было бы сделать местоопределение источников помех и тонкий анализ их сигналов по записи. Правда, это потребует создания системы единого времени для измерителей и пеленгаторов, что в условия подавления главного источника времени — спутников GPS или ГЛОНАСС — задача нетривиальная. 
Поэтому на первых порах я предлагаю задачу максимально упростить. Пусть у нескольких человек, назовем их радио-партизанами, будет на подоконнике стоять цветочек комплект, как на картинке ниже.

Он может состоять из платы LimeSDR или другой подобной, недорогого одноплатного компьютера ROCK64 или любого другого с USB3, который есть под рукой, и антенны, которая принимает на частотах 1.6 ГГц. На этом железе пусть работает софт, который, например, раз в минуту отправляет на центральный сервер информацию об уровне сигнала на частотах GPS и ГЛОНАСС L1. Эта информация пусть хранится на сервере и выводится по запросу через интернет в открытом доступе всем, кому будет интересно.
Тогда, если у вас будут сомнения в правильности работы вашей навигации, вы просто сможете зайти на сайт и увидеть, а правда ли сейчас навигация подавлена. Вы также сможете посмотреть статистику наличия помех навигации в этой местности, в это время дня, в этот день недели и т.д.
Всякие радио-развед-хулиганы, такие как я, смогут предсказывать появление помехи и не делать холостых поездок для их анализа в Москву. Если систему развить до местоопределения и анализа сигналов, то мне в Москву можно будет вообще не ездить.
Я призываю всех, кто может помочь с софтом, присоединяться. Нужно сделать несложную часть на стороне SDR-софта, нужно сделать сервер, нужно сделать отображение информации на вебе. Все, конечно, open source. Я уже даже сделал пустой репозиторий на Github для исходников. Сам сервер я поставлю пока у себя на квартире. SDR-плату с маленьким компом тоже поставлю в месте, где смогу кратковременно включать генератор, чтобы человек, который будет делать SDR-софт, смог удаленно отладить систему.
Нужны также ваши идеи, как это все улучшать и изменять, куда развиваться.
Нужны герои хорошие люди, которые установят у себя дома или на работе устройства и подключат их к Сети. Эти устройства они могут собрать сами, из своих средств, или попросить у друзей. В первую очередь такие радио-мониторы нужны в местах, где помехи уже были обнаружены ранее. Это центр Москвы и Питера и аэропорты.
Теперь я опишу свой замысел на цифровую обработку сигналов на этом этапе. Раз в минуту надо настроиться на частоту GPS L1, 1575.42 МГц и снять выборку длительностью, скажем, 10 миллисекунд с полосой 2 МГц, потом усреднить по выборке мощность (найти среднюю сумму квадратов синфазной и квадратурной составляющей) и отправить это число на сервер с меткой времени. Потом тоже самое сделать с центральной частотой ГЛОНАСС — 1605, но полосу нужно увеличить до 10 МГц.
Такой способ не позволит отличить спуффер от простого передатчика помех, но он прост в реализации. От этого варианта можно начать обсуждение, лучше всего отдать конкретную схему ЦОС на откуп тому, кто возьмется ее реализовывать. Этот софт можно делать на базе GNURadio. 
Отсчеты средней мощности надо записывать в базу данных на сервере, чтобы было удобно потом выбирать данные для отображения в браузере настольной и мобильной версий. Здесь моя фантазия ограничена, так как я не специалист по этим вопросам. Полностью полагаюсь на вас.
Антенной для этого диапазона может быть просто проволочка длиной 1/4 волны, 4.5 см, припаянная к антенному разъему SDR-платы.
Как видите, все довольно просто. Нужно только взяться и сделать.
Я с трепетом ожидаю реакции сообщества и помощи в реализации софта. Надеюсь, мы вместе накопим базу данных по радио-помехам и сделаем, в конечном счете, наш радио-эфир чище!
P.S. Дозор утренний, так как GPS-спуффер в Москве обычно включается утром.
радио-инженер
=====
Одних тестов недостаточно, нужна хорошая архитектура
2017-12-18, 10:27
.NET developer
=====
Защищенность переменных в Kotlin на примере Java
2017-12-16, 15:19
В дальнейшем обращение к значениям свойств происходит так же, как и к обычным переменным.
Android разработчик
=====
Решаем Open Day CrackMe, таск Pizza
2017-12-17, 13:40
User
=====
Пойди туда, не знаю куда: по следам конференции SmartData
2018-03-27, 09:49
Пользователь
=====
RFC для слабаков или история одного расследования
2017-12-16, 22:33
Системный администратор Linux
=====
JUnit тесты для логирования
2017-12-16, 21:58
Пользователь
=====
Делаем адаптивный HTML, добавляя одну строку в CSS
2017-12-16, 22:48
Пользователь
=====
Изменения в аудиоконференциях 3CX
2017-12-16, 23:01
User
=====
«Истина в вине» или пробуем программировать NanoCAD под Linux (MultiCAD.NET API)
2017-12-17, 01:41
Технический писатель
=====
Dagger 2 для начинающих Android разработчиков. Dagger 2. Часть 2
2017-12-17, 04:52
Android devepoler
=====
Сверточная сеть на python. Часть 3. Применение модели
2017-12-25, 14:22
Пользователь
=====
Функция, сценарий и аппроксимация событий
2017-12-17, 10:22
Пользователь
=====
Полезное дизайнеру: бесплатные новинки для оптимизации дизайн-процессов. Выпуск 2-й
2017-12-17, 17:26
Готовые дизайн системы в Figma → setproduct.com
=====
Юридические аспекты операций с криптовалютами для резидентов РФ
2017-12-17, 12:17

Да, являются.
Список объектов гражданских прав указан в ст. 128 ГК РФ:
Как видно из текста закона, этот список не исключительный, и туда входят любые имущественные права, результаты работ и оказание услуг, и даже нематериальные блага (пример: "вы мне споете, а я вам станцую" — это обмен нематериальными благами)
Часто встречаемые высказывания о том, что де "нет определения криптовалюты в законодательстве РФ и поэтому операции с ними незаконны" — неграмотны.
В законодательстве в принципе не должно, и не может содержаться определение всех возможных предметов и явлений окружающей действительности, кроме случаев когда определенная деятельность или операции с определенными объектами требуют специального регулирования или запрещения.
Таким образом, отсутствие определения в законодательстве как раз свидетельствует о том, что законодатель не счел нужным вводить специальное регулирование или запрещение соответствующих операций. Скажем в законодательстве РФ не содержится понятий “гусь” или “рассказывание сказок”, но это ни в коей мере не означает что продажа гусей или рассказывание сказок за деньги незаконны на территории РФ
По своей природе получение или передача криптовалюты — это внесение записи в распределенный реестр данных, и в этом смысле она аналогична покупке и продаже доменного имени, которое тоже есть ничто иное как запись в распределенном реестре данных. При этом доменное имя имеет устоявшуюся практику использования, и даже судебную практику по рассмотрению споров о принадлежности доменного имени.
См. также: Анализ судебной практики по вопросам криптовалют в России // RTM Group.
Нет, не являются.
Понятие “денежный суррогат”, используется только в ст. 27 гл. VI “Организация наличного денежного обращения” Федерального закона от 10.07.2002 N 86-ФЗ "О Центральном банке Российской Федерации (Банке России)" И как явствует из названия этой главы относится к сфере наличного денежного обращения, то есть запрещает придание функций наличных денег чему-либо кроме рублей РФ, выпускаемых Банком России.
Об этом свидетельствует и правоприменительная практика в РФ. Так, известное “дело о колионах” (гражданское дело по иску Егорьевской городской прокуратуры к гражданину М. Ю. Шляпникову о признании незаконным использования изготовленных им денежных суррогатов «колионов», в котором Егорьевский городской суд Московской области признал наличие выпуска “денежных суррогатов” касалось именно наличных “колионов” После чего Шляпников выпустил безналичные колионы на блокчейне Emercoin, и прокуратура как видно уже не возражает против этого.
Примечание: Следует учесть что правоприменительная практика в РФ векселя, жетоны метро, фишки в казино, золото также не относит к “денежным суррогатам”
Пресс-служба ЦБ РФ выпустила несколько информационных сообщений
связанных с криптовалютой:
1) "Об использовании при совершении сделок «виртуальных валют», в частности, Биткойн", 27 января 2014 года,
2) "Об использовании частных «виртуальных валют» (криптовалют)", 4 сентября 2017 года,
В отношении которых можно утверждать следующее:
Данные документы изданы пресс-службой, никем не подписаны, не зарегистрированы, и юридически не могут считаться чем-то имеющим какое-то нормативное значение или чем-то применимым при толковании законодательства (см. ст. 7 Федерального закона от 10.07.2002 N 86-ФЗ), что очевидно должно толковаться как отсутствие нормативной позиции ЦБ РФ по данному вопросу.
Несмотря на вышеуказанное, тексты вышеупомянутых сообщений пресс-службы:
а) не содержат прямого утверждения что криптовалюты являются денежным суррогатом,
б) не содержат утверждения о том, что операции с криптовалютой запрещены в РФ
в) не содержат утверждения о том, что банки и небанковские кредитные организации не должны обслуживать операции в которых используются криптовалюты
См. также: Мнение: ЦБ РФ значительно смягчил свою позицию в отношении криптовалют*
То есть, если смоделировать ситуацию в которой банк хотел бы отказать клиенту в проведении платежа по контракту, предусматривающему платную передачу криптовалюты, а клиент настаивал бы на проведении платежа, то вышеуказанные сообщения пресс-службы не являлись бы достаточными чтобы обосновать юридическую позицию банка, и тем более чтобы защитить банк от возможного иска о взыскании убытков связанных с безосновательным отказом клиенту в проведении банковской операции.
Да, разрешены.
Основным официальным документом по данному вопросу является Письмо Минфина РФ и ФНС РФ от 3 октября 2016 г. N ОА-18-17/1027* (текст есть также на http://miningclub.info/threads/fns-i-kriptovaljuty-oficialnye-otvety.1007/), в котором указано :
Предприятия, банки и небанковские кредитные организации не имеют ни оснований ни полномочий отвергать официальную позицию Минфина РФ и ФНС РФ по данному вопросу.
См. также: Письма Минфина и ФНС: точка зрения или закон?
В соответствии с положениями Федерального закона от 10.12.2003 N 173-ФЗ "О валютном регулировании и валютном контроле" (ст. Статья 1. Основные понятия, используемые в настоящем Федеральном законе) биткойн, эфир т.п. не являются иностранной валютой, соответственно на расчеты в этих условных единицах не распространяются ограничения предусмотренные для использования расчетов в иностранной валюте.
Это подтверждается и Письмом Минфина РФ и ФНС РФ от 3 октября 2016 г. N ОА-18-17/1027: 
Таким образом криптовалюты не являются “иностранной валютой” в смысле действующего законодательства РФ и операции с ними не связаны с соответствующими ограничениями и регулированием. Это означает однако что такие операции являются по общему правилу объектом налогообложения НДС.
Криптовалюта не подпадает под определение “нематериальный актив” согласно Положения по бухгалтерскому учету "Учет нематериальных активов"(ПБУ 14/2007))
Так как для того чтобы признаваться нематериальным активом объект должен отвечать следующим требованиям (пп. “г”, “д” п. 3 раздела I. ПБУ 14/2007):
"г) объект предназначен для использования в течение длительного времени, т.е. срока полезного использования, продолжительностью свыше 12 месяцев или обычного операционного цикла, если он превышает 12 месяцев;
д) организацией не предполагается продажа объекта в течение 12 месяцев или обычного операционного цикла, если он превышает 12 месяцев;"
Учитывать в бухгалтерском учете криптовалюту можно как финансовые вложения по ПБУ 19/02 “Учет финансовых вложений”
По ПБУ 19.02:
В данном случае список не исчерпывающий, и термин “пр.” (прочее) может включать в себя также и криптовалюту. При этом криптовалюты в чистом виде (эфир, биткойн) конечно не является ценными бумагами (однако прочие токены на блокчейне могут являться таковыми в некоторых случаях)
Соответственно, отображать криптовалюту в бухгалтерском учете предлагается на счету 58 "Финансовые вложения" (Приказ Минфина РФ от 31.10.2000 N 94н "Об утверждении Плана счетов бухгалтерского учета финансово-хозяйственной деятельности организаций и Инструкции по его применению") Можно для этого завести специальный субcчет или субсчета 58 счета.
Т.е. при покупке криптовалюты (биткойн, эфир) за иностранную валюту кредитуем 52 “Валютные счета”, дебитуем 58 "Финансовые вложения".
При продаже крипты за рубли РФ соотвественно дебитуем Счет 51 "Расчетные счета" (если за валюту — 52 “Валютные счета”, если за наличные рубли — 50 “Касса”), и кредитуем 58 "Финансовые вложения"
Предполагается что первоначальные операции с криптовалютой нужно проводить в небольших суммах, и, возможно, не с биткойн, который иногда фигурирует в частных высказываниях официальных лиц, а с эфиром, который не только не фигурирует в таких высказываниях в негативном контексте, но напротив имеет свидетельства косвенного одобрения со стороны высшего руководства РФ. Основатель проекта Ethereum Виталик Бутерин, принимал участие в работе Петербургского экономического форума (ПМЭФ) вместе с высшими должностными лицами РФ, и также его принимал Президент РФ, что конечно не могло иметь место если бы не имело место благосклонное отношение руководства РФ к проекту Ethereum.
К тому же можно предполагать что в долгосрочной перспективе эфир имеет больший потенциал роста с расширением использования смарт-контрактов на платформе Ethereum. Также нужно учесть что в отличие от биткойна эфир имеет утилитарное использование в качестве “топлива” (gas) при деплое и исполнении смарт-контрактов на платформе Ethereum, и в таком качестве является необходимым для организаций занимающихся разработкой и/или изучением смарт-контрактов на блокчейне. К тому же обмен одной криптовалюты на другую, например eth на btc доступен в автоматическом режиме на платформах типа shapeshift.io
В этом случае между нерезидентом (например, оффшорной компанией) и резидентом РФ заключается договор о том что резидент РФ перечисляет нерезиденту денежные средства в долларах США или в евро, а нерезедент обеспечивает внесение записей в распределенный реестр Ethereum о переводе на указанный в договоре адрес на в сети Ethereum, принадлежащий юридическому или физическому лицу — резиденту РФ указанного в договоре количества эфира или биткойнов.
Возможным вариантом также является использование для расчетов переводного аккредитива. Банк раскрывает аккредитив в пользу оффшорной компании по факту зачисления на указанный в договоре адрес в сети Ethereum или Bitcoin указанного в договоре количества криптовалюты, а оффшорная компания переводит платеж по поставщикам криптовалюты.
В этом случае формально криптовалютой владеет оффшорный инвестиционный фонд, долю в котором приобретает компания — резидент РФ. При этом может быть построена схема в которой компания — резидент РФ получает также приватный ключ и пароль для управления счетом на Ethereum, или иным путем получает возможность в любой момент “обналичить” (т.е. забрать в виде криптовалюты) свою долю в фонде. В этом варианте возможно упрощается для банка (или небанковской кредитной организации) проведение платежа клиента, так так платеж по договору осуществляется не за криптовалюту, а за долю в инвестиционном фонде (что более привычно для банков), при этом в договоре может фигурировать название инвестиционного фонда, а не криптовалюты напрямую, и отсылка к условиям его функционирования.
В бухгалтерском учете так же как было показано выше, юридическое лицо отражает свои вложения на 58 "Финансовые вложения", и при конвертировании вклада в криптовалюту, можно просто переводить на другой субсчет 58 счета.
я тоже шаман, но другой
=====
Постигаем Си глубже, используя ассемблер
2017-12-17, 13:47
User
=====
Советы по созданию современного Android-приложения. Лекция Яндекса
2017-12-17, 15:27
Интернет щей
=====
Дайджест интересных материалов для мобильного разработчика #234 (11 декабря — 17 декабря)
2017-12-17, 14:32
Пользователь
=====
Простая реализация Token для взаимодействия мобильного приложения с WebAPI
2017-12-17, 15:50
User
=====
Настройка Nginx + PHP-FPM и HTTPS от Let's Encrypt на AWS EC2 с Ubuntu Server 16.04 LTS
2017-12-17, 19:04
Программист
=====
Модульная сетка макета с нуля: анализ, расчет и построение
2017-12-17, 19:46
Привет, Хабр. Я читаю тебя без малого 10 лет, но ни разу не писал статей. Сначала сказать было нечего, потом — некогда. Но сегодня звёзды сошлись и подвернулась подходящая тема. Модульная сетка.
Казалось бы, набросать сетку — дело пяти минут. Всё разжёвано до нас, и бутстрапы есть на любой вкус, и «Аннушка уже пролила своё масло...». Но на практике у дизайнеров частенько возникают вопросы. Многих вводят в ступор даже небольшие отступления от привычных 12-колоночных сеток, потому что им не до конца понятны принципы построения. 
Когда-то эту тему хорошо раскрывал цикл статей Алексея Черенкевича, но тексты пропали из открытого доступа. И хотя их ещё можно найти в архивах, сами тексты за прошедшие годы несколько устарели. 
Словом, в очередной раз отвечая на вопрос по модульной сетке и не найдя ни одной подходящей ссылки, решил как-то обобщить в заметке всё, что размазывалось по десяткам комментариев на разных сайтах.
Любой макет содержит элементы. И порой их очень много. Между элементами возникают оптические связи и притяжения, которые подчиняются теории близости и, в частности, правилу внешнего и внутреннего. 
Сетка помогает соблюдать это правило, не вычисляя каждое расстояние и размер в отдельности. Вы закладываете ключевые закономерности один раз, при построении сетки, а затем просто придерживаетесь их.
В отличие от колоночной, модульная сетка задает вертикальный ритм и основные пропорции элементов, поддерживая их во всем макете. Это удобная, гибкая и довольно простая система. При условии, что вы достаточно вникли в ее принципы.
Модульная сетка строится в двух направлениях: горизонтальном и вертикальном. Грубо говоря, это комбинация колонок и рядов с разлиновкой макета на строки.
Последнее интуитивно понятно всем пожилым аксакалам, которым доводилось с помощью карандаша и линейки сотнями чертить горизонтальные линии на листах для рефератов. Впрочем, не удивлюсь, если студенты делают это до сих пор.
Итак, если вам нужно построить сетку с нуля, вы будете отталкиваться от двух вещей. Во-первых, от контента, который нужно разместить в макете: текстов, иллюстраций, таблиц, перечней, медиафайлов. Во-вторых, от пропорций и площади носителя: бумажного листа, экрана, холста или чего-то более экзотического.
Если контент простой и неизменный, и известен вам заранее, то проще всего будет отталкиваться именно от него. Если же контент сложный и непредсказуемый (например, user-generated — пользовательский), то сетка в большей степени будет определяться форматом носителя и общими принципами типографики и композиции.
Когда речь идёт о сайтах или печатной продукции, дизайн во многом зависит от типографики и свойств текста. Поэтому построение сетки удобно начинать с вертикального ритма. 
В первую очередь вам нужно нащупать два взаимосвязанных ключевых параметра: базовую высоту строки и базовый размер шрифта. Проще говоря (хи-хи), базовые интерлиньяж и кегль. И вот эта простенькая задача постоянно вводит людей в ступор. «Как мне узнать нужную высоту строки?», «А какой шрифт брать?», «А сколько строк нужно делать для буклета? А для визитки?» и т.п.
Если у вашего носителя фиксированный размер и вам уже известен весь ключевой контент, то интерлиньяж можно прикинуть заранее. 
Для этого нужно представить, что каждый элемент дизайна, включая отступы, занимает по высоте некоторое количество абстрактных строк. Потом сложить все строки вместе и разделить на них высоту макета. А затем каждую абстрактную строку разбить на N реальных строк, соответствующих нужному интерлиньяжу — так, чтобы в них адекватно поместился заданный текст.
Допустим, вам нужно сверстать на листе А4 список лучших работников месяца. Их у вас трое. Для каждого сотрудника вы делаете блок-карточку: слева фото, справа текст. Фотография крупнее, поэтому высота карточки равна ее высоте. Кроме того, на листе нужно разместить шапку с заголовком, а внизу — блок контактной информации [профсоюза гениев]. 
Вы анализируете задачу и мысленно прикидываете относительные высоты всех элементов. Вы предполагаете, скажем, что будущая шапка по высоте займет примерно половину высоты фотографии. А в подвале окажется много всего, и он получится где-то равным фотографиям по высоте. Плюс-минус лопата. Вы пересчитаете это снова, если понадобится.
Если теперь принять высоту шапки за одну «строку», получится, что весь ваш контент занимает 9 абстрактных «строк». Если вам трудно абстрагироваться от интерлиньяжа, назовите эти «строки» рядами или горизонтальными блоками. Предположим, вы еще захотите добавить по 1/2 строки воздуха перед верхним и нижним краями листа. Итого, ваш макет нужно делить на 10 строк:

В результате вы делите ваш носитель (лист A4) по высоте на 10 «строк». Получаются блоки высотой около 3 сантиметров. Очевидно, что это слишком крупная разлиновка, чтобы верстать по ней тексты, контактную информацию и прочие мелочи.
Вы смотрите на свои текстовки и понимаете, что возле каждой фотографии у вас получается описание где-то на 10—12 строчек текста. То есть, каждый блок нужно разделить еще на 5—6 частей. Предположим, вы перестраховываетесь и берете большее значение, чтобы по одной строчке ушло на отступы. Бинго. Теперь ваш макет представляет собой 10 блоков по 6 строк в каждой. То есть 60 строк. С учетом высоты листа (~300 мм), каждая строка получилась высотой ~5мм. Всё, можно прототипировать прототип, а затем дизайнить дизайн.

А как же быть с размером шрифта? О, теперь всё очень просто! По всё той же логике «правила внешнего и внутреннего», интерлиньяж должен составлять примерно 150—200% от высоты кегля. То есть размер вашего шрифта будет в 1,5—2 раза меньше высоты строки. А это от 2,5 до 3,3мм. Этот размер несложно подобрать в пунктах или даже просто на глаз. 
Подчеркну, что не обязательно вымерять тысячные доли с калькулятором. У вас есть глазомер и чувство пропорции — их должно быть достаточно. И даже если они пока не развиты, спустя сотню-другую макетов вы сможете попадать в сетку пальцем с точностью до 1-2 пикселей, даже при скрытых направляющих и отключенной привязке. И не забывайте, что человеческий глаз воспринимает размеры и расстояния с поправками на физиологию. Поэтому и сама сетка — не догма, а лишь грубое подспорье в расчетах. Последнее слово всегда за оптической компенсацией. К этой теме вернёмся ниже.
Ещё момент. Бывает так, что после всех расчетов выясняется, что шрифт слишком крупный или интерлиньяж слишком мелкий. В этом случае вы либо делаете перерасчет сетки, либо просто используете пропорциональные значения. Как правило, компромиссный вариант — это половинный или полуторный интерлиньяж.
Далеко не всегда у вас будет предсказуемый контент и фиксированный холст. В веб-дизайне вы куда чаще работаете с тотальной неопределенностью. 
Иногда это похоже на художественную лепку из соплей. Высота вашего макета условно бесконечная, ширина — плавающая, основной контент пользовательский, встраиваемые виджеты предоставлены артелью «Пупкин и сыновья», а реальные тексты страниц заказчик собирается показать вам где-то примерно за день до запуска сайта. Но это не точно.
В таких условиях, очевидно, нет смысла пытаться определять количество строк. Зато можно танцевать от обратного: от размера шрифта (кегля). И это даже проще.
Всё, что вам нужно, это выбрать для проекта базовый кегль, который будет достаточно крупным, чтобы хорошо читаться, и при этом достаточно компактным, чтобы в строку основных текстовых блоков помещались 7—8 слов. Строго говоря, для русского языка достаточно и 5—6 слов, потому что в среднем у нас, конечно, длиннее и тверже. Но это ориентир, а не догма. Опираться нужно на конкретный макет, конкретный шрифт плюс собственное зрение и опыт.
В общем случае, ваш базовый кегль для десктопных версий сайта окажется в диапазоне от 14 до 22 пикселей. Причем тенденция идёт в сторону укрупнения. 
Для педантов отмечу: таки да, для кегля уже изобретены относительные единицы измерения, и это прекрасно. Но если мы углубимся в это здесь, то старость наступит незаметно, и первую сетку наш юный читатель достроит примерно к тому моменту, когда в полной мере прочувствует бесперспективность своих отчислений в пенсионный фонд. Поэтому и здесь, и далее по тексту «px» — простые квадратные пиксели. Без учёта ретин, без учета адаптивностей, а также «без колонок, без усилка и без защиты от дурака, которого ты здесь валяешь».

Сразу ответ на частый вопрос: «базовый» не означает «самый мелкий». В любом макете почти всегда будут и менее заметные надписи: сноски, примечания, подстрочники и т.п. Здесь же речь идет о том шрифте, которым вы будете набирать основную массу текста. Вспомните любой текстовый редактор. Вы открываете новый файл и начинаете печатать каким-то «просто шрифтом», лишенным какого-либо специального форматирования — это и есть базовый шрифт, basefont. Если вам нужно сделать надпись помельче, никто не запрещает использовать мелкий кегль, это нормально.
Итак, определились с базовым шрифтом. Что дальше? А дальше — интерлиньяж. По уже упомянутым традициям современной типографики он составит 150—200% от кегля. А иногда и более.
(Подчеркну: речь о современных реалиях. Просьба не попрекать Брингхерстом и другими классиками книжной типографики — они, безусловно, взорвали свой танцпол, но с тех пор мир немного изменился. Те 120—180% всё-таки рассчитывались для книг, да ещё и в латинице). 
Таким образом, ваш базовый интерлиньяж почти всегда окажется где-то в диапазоне от 22 до 40 пикселей. 
По личному опыту, для простых коммерческих сайтов довольно удобен базовый интерлиньяж 15px. (Подразумевается 30px, но на уровне верстки удобнее сразу работать с половинчатым, т.к. он дает хорошие отступы в 15px и 45px. Важно понять, что в контексте модульной сетки 15 и 30 суть одно и то же — вы просто работаете либо с половинным значением «тридцатки», либо с целым значением «пятнашки»). Хотя при всей любви к классическим модульным сеткам, не отрицаю плюсы той же трендовой 4-пиксельной сетки для UI-дизайна. Сетки разные нужны, сетки разные важны. Это инструмент, а не религия.
Теперь, когда у нас есть базовый интерлиньяж, мы можем разлинеить макет и следовать, наконец-то, вертикальному ритму. Это значит, что каждый элемент дизайна будет занимать по высоте некоторое число строк. С ритмом становится удобно и легко работать. Не нужно вычислять точные значения в пикселях. Вы меряете всё строками. Заголовок первого уровня — 4 строки, иллюстрация — 8 строк, аватар — 3 строки, кнопка — 3 строки, меню — 5 строк, отступ — 1 строка и т.п. (Надеюсь, вы понимаете, что это фигуральный пример, а не рецепт успеха).
К слову, жесткий ритм особенно удобен для верстки с CSS-препроцессорами. Верстальщику достаточно заменить одну переменную с базовым интерлиньяжем, чтобы весь сайт пропорционально уменьшился или увеличился. Это не означает, что на этом его работа закончится, но избавит от тонны рутинных операций по адаптации каждого компонента дизайна в отдельности.
В первую очередь, от контента. Здесь всё проще, чем с высотой. Если речь идёт о печатной продукции, вы в подавляющем большинстве случаев можете предположить число колонок еще на стадии первых черновиков.
Например, если вы верстаете блок про времена года, вашим магическим числом наверняка будет 4. Вы можете сгруппировать их в один ряд или в два. При этом число колонок получится кратным либо двум, либо четырем. То есть есть смысл опираться на числа 2, 4, 6, 8, 12 или 16.
Усложним. Предположим, под блоком с временами года у вас идёт блок с тремя рекламными объявлениями. Очевидно, для этой части макета удобнее было бы число колонок, кратное трем: 3, 6, 12… Но сетка в 3 колонки явно неудачна для времен года. Поэтому нужно искать для них какой-то общий знаменатель. Предыдущий абзац подсказывает, что вам нужна сетка на 6 или 12 колонок.
*Еще пример. Вам нужно сверстать типовой блок-визитку, состоящий из логотипа/аватара (слева) и контактной информации (справа). Сколько нужно колонок? Давайте прикинем. 
Если мы хотим сделать равновесную композицию, то можно обойтись даже двумя колонками. В этом случае выравнивание лого будет центральным, а текст придется размещать на глаз. А можно предположить, что текст по ширине занимает места примерно вдвое больше, чем логотип, и соответственно сделать блок в три колонки. А можно пойти еще дальше и заложить в сетку еще и отступы. Допустим, слева и справа будет поля по 1 колонке, логотип займет 2 колонки, текст 4, плюс 1 колонка отступа между лого и текстом — итого 8 колонок.*

Вот таким нехитрым образом вы рассуждаете, продумывая сетку. Если контента много, а глаз не намётан, можно с первого раза пролететь и по мере проработки деталей макета понять, что сетка не годится. Это нормально. Ничего страшного. 
Когда речь идёт об отдельно взятом макете, подогнать дизайн под новую сетку нетрудно. Если же проект большой и предполагает изобилие страниц или макетов с единой канвой (сайт, брошюра, книга и т.д.), лучше заложить в сетку некоторый запас прочности и хорошенько протестировать ее со случайным контентом из разных страниц. Запас прочности, как правило, достигается кратным увеличением числа колонок: например, вам сейчас нужно только 3, а вы закладываете 6 или 9.
Отмечу один нюанс. Если вы делаете нечто с выраженной центральной композицией и активно используете горизонтальное выравнивание по центру, выгоднее сделать число колонок нечетным. Это позволит равномернее распределять отступы и контент. Совет касается и внутреннего дробления колонок. В примере выше, если у вас было 3 колонки и вам понадобилось их детализировать, при центральной композиции вы разобьете каждую колонку еще на 3, а вот при симметричной — на 2 или 4. В итоге, в первом случае колонок станет 9, а во втором — 6 или 12.
Тут всё просто. Число 12 делится на: 12, 6, 4, 3, 2, 1. Поэтому сетка получается гибкой и позволяет органично верстать блоки почти любого количества или ширины. Более того, отбрасывая по краям макета 1 или 2 колонки в качестве полей, вы получаете в центре блок, который делится ещё и на 10, 5 или 8. 
Из личного опыта добавлю, что очень удобно рисовать адаптивные макеты, отталкиваясь от ширины в 1200 пикселей, особенно без межколоночных интервалов. Вы получаете 12 колонок с приятной шириной ровно в 100 пикселей и постоянно оргазмируете от круглых чисел в процессе работы. И когда вам нужно поместить на макет плашку кнопки, вы не тянете границы прямоугольника туда-сюда, а мгновенно и не задумываясь вбиваете размер: 300 на 60 и кликаете на макет. К слову, полезно приучить себя позиционировать элементы не мышкой и стрелками, а вбиванием цифры отступов по иксу и игреку — макеты станут опрятнее.
Если контент не подразумевает верстку встык, бывает удобно сделать 24 колонки и работать с ними так же, как с межколоночным интервалом, просто отступая при необходимости полную колонку в 50px. Это создает вокруг контента достаточно воздуха, и макет смотрится дорого. Если же требуется интервал помельче, берется ровно половина колонки, то есть 25px. Все расчеты на лету, числа удобные.
Зачем и когда нужно делать отступы между колонками? 
Мы не всегда делаем мозаику. Чаще всего контент не верстается встык — между двумя блоками должно быть какое-то расстояние, чтобы они не слиплись и не перекрыли друг друга. 
Кроме того, для снятия нагрузки с глаза зрителя бывает нужен воздух, белое пространство. Когда в макете мало места и много информации (в газете, например), увеличение межколоночного интервала становится практически единственным способом хоть как-то размежевать текстово-графическую кашу.
В большинстве случаев, межколоночный интервал значительно меньше ширины колонки. Его размер тоже определяется особенностями контента. Если вы верстаете интерфейс, где много классических элементов управления, узкое межколоночное расстояние служит удобным разделителем. Например, между поисковой строкой и кнопкой, или между чекбоксом и его лейблом. (Хотя в целом здесь есть смысл подумать о «квадратной» сетке: 4px или иной, вообще без всяких колонок, и для этого тоже есть причины). Если же вы верстаете страницу с крупными текстовыми блоками, организованными всего в 2—3 колонки, то межколоночное расстояние имеет смысл сделать большим, чтобы дать контенту максимум воздуха.
Как уже говорилось выше, из личного опыта, очень многие вещи можно сверстать с нулевым межколоночным расстоянием при большом числе колонок. В этом случае за отступ принимается ширина целой колонки и все маргиналии получаются крупными, характерными для «благородной» типографики.
Только пусть вас не подкупает приятное слово «благородный» — в коммерческом сегменте такой дизайн не всегда хорош. В целом, чем активнее и агрессивнее схема продаж, чем голоднее и злее маркетологи заказчика, тем плотнее будет верстка и тем меньше там останется воздуха. Предельный (или, скорее, запредельный) случай — газеты бесплатных объявлений а ля «Из рук в руки», где занятая площадь напрямую определяет заработок. Разумеется, там не нужны эстетические изыски с гигантскими полями. С тем же успехом можно просто залезть в карман учредителю и позаимствовать оттуда пару сотен долларов до следующей зимы. Разницы никакой, а согласовывать меньше.
Собственно, вот таким непростым путём мы с вами добрались до определяющего понятия «модуль», в честь которого и названа модульная сетка. Обычно с него начинают, но судя по вопросам дизайнеров, такой подход не работает. Так что же это вообще такое и зачем оно нужно?

По сути, это просто пропорция. Ширина модуля равна ширине колонки, а высота — нескольким строкам. Скольким именно? Зависит от вашего дизайна и от эффекта, которого вы хотите добиться.
*Несколько лет назад мне довелось делать сайт, посвященный пилонам — шестам для танцев. Естественно, модуль там был сильно вытянут по вертикали. Я использовал соотношение ширины и высоты почти 1:3. Во-первых, потому что подавляющее большинство иллюстраций было портретной ориентации и тоже вытянуто. Во-вторых, потому что сам продукт и весь стиль предполагал некоторую долю фаллической символики. Модуль определяется и контентом, и композицией, и стилем.
Если вы делаете нечто «стабильное», то наоборот: есть смысл подумать о модуле, чуть вытянутом по горизонтали. Если же вы верстаете огромную длинную таблицу, которая служит основным контентом в макете, то очевидно, что за модуль нужно принимать одну или две базовых строки этой самой таблицы. Словом, думайте.*
В принципе, никто не мешает вам строить и более сложные сетки. В них, например, могут чередоваться модули разной высоты. Главное, чтобы в этом была какая-то логика и закономерность, сохраняющая вертикальный ритм. 
Допустим, вы делаете портал. У вас есть меню высотой в 3 строки, затем следует баннер главной новости высотой в 9 строк, затем ряд каких-то цифр (курсов валют, погоды и др) в 3 строки, затем ряд из нескольких новостей второго плана на 9 строк. То есть весь контент чередуется: 3-9-3-9-3-9-3… На практике этот прием редко оправдан, он не очень-то гибок. Но стоит держать в голове, что ритм может быть сложным. 
Да, бывает и такое. Когда проект сложный и многокомпонентный, вы можете использовать несколько вложенных сеток. Простейший пример: общая раскладка макета (layout) использует гигантские колонки с крупным текстом, а внутри одной из них лежит форма калькулятора с кучей элементов управления, сверстанных по квадратной сетке в 4px. В этом нет ничего особенно криминального. 
Более того, когда речь идёт о сайтах, часть вашего контента может быть вообще посторонней и встраиваемой: плееры, онлайн-карты, виджеты, платежные фреймы и т.д. Миллион вариантов. У этих элементов будут свои внутренние сетки, которые вы не контролируете. И это тоже нормально. 
Что вы можете сделать, так это соблюсти правило внутреннего и внешнего для контейнеров, содержащих эти блоки. Конкретнее, дать вокруг достаточно воздуха (в том числе и по вертикали), чтобы они не лепились к остальному контенту, выглядели обособленно и не перетягивали посторонние элементы в свою визуальную зону. Все расстояния и пропорции в вашем макете относительны, поэтому, грубо говоря, вы можете сбалансировать чужой контент «снаружи» почти с тем же успехом, как если бы меняли размеры его содержимого «внутри».
Дизайнерам-перфекционистам бывает трудно понять этот момент. Сетка — это просто методология для упрощения расчетов. Это не символ веры, не закон мироздания, и не панацея. Более того, математические пропорции по чисто физиологическим причинам не являются идеальными для человеческого восприятия. Сетка не учитывает возникновение оптических иллюзий и искажений.
Поэтому, если ваш глаз сообщает вам, что какой-то элемент нужно сдвинуть на пару пикселей правее направляющей сетки, вы можете сделать это. (Не факт, что верстальщик заметит и сохранит ваш оптический костыль, но всё же).
Отдельный важный момент: элементы в сетке выравниваются по визуальной массе, а не по габаритным границам. Это значит, что (в идеале) круг, выровненный по левому полю, почти всегда окажется на несколько пикселей левее квадрата, который выровнен по тому же полю. А мелкий подстрочник под крупным заголовком почти всегда нужно смещать вправо, потому что оптически левый край первой буквы заголовка окажется правее, чем «по расчетам». Это частности оптической компенсации.
Таких тонкостей довольно много. Но важно понимать, что в случае с веб-дизайном вы почти никогда не получите идеальную картинку. Веб-верстка зачастую формализована, она опирается на формульные зависимости фреймворков, чтобы гибко адаптироваться под разные устройства и платформы. И адекватно масштабировать ваше 3-пиксельное смещение заголовка с учетом всех особенностей сглаживания, антиалиасинга и прочих ругательных слов практически невозможно. Это нужно воспринимать философски.
Пожалуй, изя всё. Текст и без того объемный, особые выводы не нужны: тут либо вникать, либо нет. В любом случае, спасибо за ваше внимание и интерес к основам дизайна и типографики. Успехов.
UX-дизайнер
=====
Битва за сетевой нейтралитет: войны с операторами и первые суды
2018-01-14, 18:43
Пользователь
=====
Новое поколение сетей: представлена первая спецификация 5G
2018-01-22, 17:37
Пользователь
=====
Как рендерит кадр движок Metal Gear Solid V: Phantom Pain
2017-12-18, 13:05


Цветокоррекция выполняется для настройки конечного цвета сцены. Художники должны сбалансировать цвета, применить фильтры и т.д. Всё это выполняется операцией, берущей исходное RGB-значение пикселя и сопоставляющей его с новым RGB-значением; работа происходит в LDR-пространстве.

В некоторых случаях можно прийти к какой-то математической функции, выполняющей такое преобразование (именно это делает оператор тональной компрессии, производящий преобразование из HDR в LDR), но обычно художникам требуется более полный контроль над преобразованием цветов и никакая математическая функция с этим не справится.

В этом случае нам приходится смириться и применить способ грубого перебора: использовать таблицу поиска (look-up table, LUT) сопоставляя каждое возможное RGB-значение с другим RGB-значением.

Звучит безумно? Давайте прикинем: существует 256 x 256 x 256 возможных RGB-значений, то есть нам придётся хранить более 16 миллионов сопоставлений!

Сложно будет эффективно скармливать их пиксельному шейдеру… если мы не прибегнем к какому-нибудь трюку.

И трюк заключается в том, чтобы рассматривать пространство RGB как трёхмерный куб, заданный в трёх осях: красной, зелёной и синей.


Переводчик-фрилансер
=====
Назови мне свою зарплату, и я скажу кто ты
2017-12-17, 20:55
User
=====
Рояль должен быть исчезнут: уровни профессионального развития и их оценка, у программистов
2017-12-17, 21:42
Research&Development
=====
Как мне удалось взломать приложение
2018-01-29, 08:17
System administrator
=====
Digital-мероприятия в Москве c 18 по 24 декабря
2017-12-17, 23:43
Подборка Telegram-канала @mоs_events

DemoDay 13-го Акселератора ФРИИ
Современные применения глубинных нейронных сетей
Атмосфера Стартапов. Конференция
Science Pop Marketing
Digital Elka 2018: Made in China
[Вечеринка «Без правок 2117»]()
Вторая встреча телеграм-канала «Стратегия и маркетинг» ( @stratmarketing )
Хакатон «Happy data saints»
Как отпраздновать Новый Год: от друидов до оливье
Хакатон ""Lovely GIS""
User
=====
Дайджест свежих материалов из мира фронтенда за последнюю неделю №293 (11 — 17 декабря 2017)
2017-12-18, 00:28

Просим прощения за возможные опечатки или неработающие/дублирующиеся ссылки. Если вы заметили проблему — напишите пожалуйста в личку, мы стараемся оперативно их исправлять. 
User
=====
Как оживить картинку в браузере. Многопроходный рендеринг в WebGL
2017-12-18, 14:04
Каждый, кто сталкивался с трехмерной графикой, рано или поздно открывал документацию на методы отрисовки, которые предполагают несколько проходов рендерера. Такие методы позволяют дополнить картинку красивыми эффектами, вроде свечения ярких пятен (Glow), Ambient occlusion, эффекта глубины резкости.
И «взрослый» OpenGL, и мой любимый WebGL предлагают богатую функциональность для отрисовки результатов в промежуточные текстуры. Однако управление этой функциональностью — довольно сложный процесс, в котором очень легко получить ошибку на любом из этапов, начиная от создания текстур нужного разрешения до именования юниформ и передачи их в соответствующий шейдер.
Чтобы разобраться, как правильно готовить WebGL, мы обратились к специалистам компании Align Technology. Они решили создать специальный менеджер для управления всем этим зоопарком из разных текстур, которым было бы удобно пользоваться. Что из этого получилось — будет под катом. Важно, что неподготовленного читателя, который никогда до этого не сталкивался с необходимостью организации многопроходного рендеринга, статья может показаться непонятной. Задача довольно специфическая, но и безумно интересная.
Чтобы вы понимали всю серьезность сиутации, коротко расскажу о компании. В Align выпускают продукт, который позволяет людям исправлять улыбки без традиционных брекетов. То есть их непосредственные потребители — это доктора. Это довольно ограниченная аудитория со специфическими запросами, накладывающими фантастические требования на надежность, производительность и качество пользовательского интерфейса. В свое время основным инструментом был выбран С++, но у него было серьезное ограничение: только десктопное приложение, только для Windows. Примерно два года назад начался переход на веб-версию. Возможности современных браузеров и стека технологий позволили быстро и удобно пересоздавать пользовательский интерфейс и адаптировать кодовую базу, которая до этого писалась почти 15 лет. Конечно, это привело к необходимости решать кучу задач на фронте и бэкенде, в том числе — к необходимости оптимизировать объемы данных и скорости загрузки. Этим задачам будут посвящены эта и следующие статьи.
И дабы дважды не вставать, я постараюсь не загромождать пост исходниками. То есть все, что входит в детали реализации и навевает тоску читателям кода, будет по возможности поскипано или сокращено до чистой, незамутненной идеи. Повествование будет вестись от первого лица, как это рассказывал Василий Ставенко — один из специалистов Align Technology, который согласился приоткрыть нам завесу тайны внутренней кухни WebGL-фронта. 
Для начала стоило бы рассказать, что, собственно, мы хотели реализовать и что для этого требовалось. Наша специфика не подразумевает большое количество визуальных эффектов. Мы решили реализовать Screen Space Ambient Occlusion (или SSAO) и простенькую тень.
SSAO — это, грубо говоря, подсчет суммарного затенения в точке, окруженной другими точками. Вот суть этой идеи:
Функция textureLookup выбирает из подключенной текстуры пиксель, представляющий собой не цвет, а позицию точки. Далее вычисляем его освещенность как отношение его глубины к удалению от текущего, рисуемого фрагмента в координатах gl_FragCoords. Потом мы делаем особу магию с волшебными числами, чтобы получить значение в нужном диапазоне.
Получившаяся текстура будет иметь примерно вот такой вид:
Вот так выглядит окончательный результат:
Заметно, что SSAO-текстура имеет меньшее разрешение, чем полное изображение. Это сделано нарочно. Сразу после отрисовки позиций в фрагменты мы ужимаем текстуру, и только после этого вычисляем SSAO. Меньшее разрешение означает более быструю отрисовку и процессинг. А значит, что перед тем, как мы будет компоновать конечное изображение, нам надо увеличить разрешение промежуточного изображения.
Резюмируя, нам необходимо отрисовывать следующие текстуры:
Большая часть текстур может быть отрисована, только если уже есть какие-то отрисованные текстуры. Причем некоторые из них могут быть использованы несколько раз. То есть необходим механизм, работающий с зависимостями.
Для отладки процесса рендеринга может быть полезно вывести любую из текстур в имеющийся контекст.
Поскольку для нашей работы мы уже используем фреймворк THREE.js, следующие требования уже вытекают из взаимодействия с ним. Мы решили не скатываться в чистый WebGL и задействовали THREE.WebGLRenderTarget, который, к сожалению, дает оверхед фреймбуферов, связывая воедино текстуру и созданный объект фреймбуфера, не позволяя использовать имеющиеся буфера для других текстур. Но даже с этим оверхедом наш рендеринг работает на приемлемых скоростях, а управление таким объектом намного легче, чем управление двумя связанными, но при этом независимыми объектами.
Нам бы очень хотелось иметь возможность «играться» с параметрами даунсэмплинга, с магией чисел и пределов освещенности и не заморачиваться тем, что надо полностью менять код вывода изображения — менять его разрешения, матрицы и прочие вещи. Поэтому было решено «зашить» механизм сэмплинга в наш менеджер.
Материал всех объектов в THREE.Scene должен быть заменен для отрисовки позиций, с учетом видимости объектов, а потом восстановлен без потерь. Тут следует отметить, что можно было бы воспользоваться параметром Scene.overrideMaterial. Но в нашем случае логика оказалась несколько более сложной.
Что же мы сделали в итоге?
Во-первых, сделали менеджер, описание которого вы найдете ниже. И написали такие классы, которые автоматически читают шейдеры и смотрят, какие текстуры им нужны для отрисовки самих себя. Менеджер должен уметь понять, что есть зависимости для отрисовки текстуры, и должен отрисовать необходимые зависимости.
Этот менеджер должен был, по задумке, быть проинициализирован экземплярами класса Pass. То есть понадобится еще один объект, который добавит в него проходы и будет уже application-specific. Из-за того, что в современных WebGL-шейдерах мы не можем задать имя исходящей текстуры, нам пришлось сделать ScreenSpacePass безымянным и давать ему имя при добавлении. А могли бы вычитывать его из текста шейдера.
Вот такой вот метод:
Да, на этот же менеджер мы повесили и управление состоянием screenSpaceScene. К счастью, это один единственный меш с геометрией, чтобы закрыть весь экран.
Вот такой вот метод нам понадобился для отрисовки конкретного прохода на экран:
Немного комментариев:
Таким образом, у нас нарисовался общий класс для наших проходов с вот таким вот интерфейсом:
Для начала надо настроить наш менеджер. Для этого мы его инстанциируем и добавляем в него некоторое количество Pass-ов. Затем, когда нам надо нарисовать какой-то Pass на наш контекст, мы просто вызываем
Этот Pass должен отрисоваться на экране, а менеджер должен подготовить перед этим все зависимости. Поскольку мы хотим переиспользовать текстуры, наш менеджер будет проверять наличие уже отрисованных текстур, и чтобы он не решил, что текстуры с прошлого кадра — это те текстуры, которые можно не рисовать, мы перед началом отрисовки сбрасываем старые текстуры. Для этого у менеджера есть функция с соответствующим названием start.
Сумятицу в стройную схему внесла потребность рисовать на основной канвас полупрозрачных текстур. При блендинге не нужно стирать предыдущие результаты, да и сам блендинг надо настроить. В нашем случае подготовленные текстуры накладываются на изображение при конечной отрисовке именно блендингом. Процедура такая:
Вот так:
Видно, что небольшое отличие состоит в том, что буфер цвета не стирается, а все остальные буферы очищаются.
Если нам захочется вывести на экран какую-то промежуточную текстуру (например, в целях отладки), мы можем лишь слегка модифицировать render. Например, текстура с SSAO, которуя я привел выше, была отрисована вот таким кодом:
Теперь подробней остановимся на том, как именно рисовать наши проходы сцен в текстуры. Очевидно, что нам потребуется что-то, что умеет отрисовывать сцену, заменяя материал, и что-то, что будет отрисовывать все в экранных координатах.
Это весь класс. Получился довольно простым, поскольку почти всю функциональность удалось оставить в родителе. Как видите, я решил оставить overrideMaterial на тот возможный случай, когда мы сможем заменить материал на всей сцене разом в одну операцию присваивания, а не во время последовательной замены материала на всех подходящих объектах. Собственно, _prerender и _postrender — это и есть довольно умные заменители материала для каждого отдельного меша. Вот как они выглядят в нашем случае:
Scene.traverse — это метод THREE.js, который рекурсивно проходит по всей сцене.
ScreenSpacePass был задуман так, чтобы вытащить из шейдера максимум необходимой информации для того, чтобы работать с ним без лишнего бойлерплейта. Класс получился довольно сложным. Основная сложность пришлась на логику, которая обеспечивает сэмплирование, — то есть на установку правильных разрешений в текстуре. Пришлось заводить дополнительный метод, чтобы задавать разрешение текущего фреймбуфера в тех случаях, когда мы хотим отрисовать на экран, а не в текстуру. Пришлось пойти на этот компромисс между технической сложностью, ответственностью классов, количеством сущностей и временем, выделенным на задачу.
Автоматический поиск и установка юниформ помогли быстро найти такие проблемы, как опечатки в именах текстурных юниформ. В таких случаях GL может взять какую-то другую текстуру, и то, что получается у вас на экране, выглядит совершенно не так, как должно, и при этом у вас нет идей, почему. 
Здесь исходник получился довольно большим, а класс — довольно умным. Впрочем видно, что большая часть кода как раз выясняет, есть ли в шейдере текстурные юниформы, и устанавливает их в качестве зависимостей.
Ну и в самом конце, я покажу, как мы этим пользовались. Application specific-сущность мы назвали EffectComposer. В его конструкторе создаем описанный менеджер и создаем ему пассы:
В качестве примера — содержимое файла passingFragmentShader.glsl:
Шейдер очень короткий — достать пиксель, который проинтерполируется, и тут же отдать его. Всю работу сделает линейная интерполяция в настройках текстуры (GL_LINEAR).
Теперь посмотрим, как будет отрисована positions.
Рабочая сцена нам нужна в других местах программы, поэтому EffectComposer не является ее владельцем, ему ее задают, когда надо.
Как видно, если кто-то сообщил нам об изменении сцены, EffectComposer создаст два Pass-a: один с настройками по умолчанию, а другой — с хитрой заменой материалов. Проходы сцены у нас не содержат каких-то хитрых зависимостей, они, как правило, рисуются сами по себе, однако описываемый подход позволяет это делать, если мы добавим в ScenePass несколько методов для того, чтобы добавлять зависимости. Потому что это неочевидно, какой именно материал из сцены захочет иметь отрисованную зависимость.
Несмотря на простоту использования в нашем случае, нам не удалось добиться полностью автоматической генерации пассов, основанных на шейдерах. Мне не хотелось добавлять в шейдеры маркеров, которые бы дополняли проходы прорисовки сцены дополнительными параметрами, такими как параметры вывода текстуры — GL_RGB, GL_RGBA, GL_FLOAT, GL_UNSIGNED_BYTE. Это бы, с одной стороны, упростило код, но дало бы меньше свободы переиспользования шейдеров. То есть эту настройку все равно пришлось описывать.
Стоит упомянуть, что мне пришлось еще реализовать маппинг зависимостей. Это оказалось полезно, если один шейдер мы хотим использовать в нескольких пассах и с разными входящими текстурами. В таком случае каждый пасс стал больше походить на функцию, поэтому у меня появилась идея, как это сделать немного «функциональнее».
Тем не менее, вся разработка оказалась весьма полезной. В частности, она позволяет без существенных сложностей добавлять нам любые эффекты в наш проект. Хотя мне лично больше всего нравится возможность легкого дебага изображений.
кибер-ниндзя
=====
Эконом решение для Интернета Вещей. Azure IoT Hub + Azure functions
2017-12-19, 09:19
.NET Core, WPF, UWP, Xamarin, IoT
=====
Джеб Кличко
2017-12-18, 08:10
Биоробот
=====
Понятие связи в проекционном моделировании
2017-12-18, 09:17
Пользователь
=====
Multi-pattern matching на GPU миф или реальность
2017-12-18, 10:03
Специалист HPC
=====
Как задавать вопросы в IRC
2017-12-18, 11:10
автор, переводчик, редактор
=====
AliExpress головного мозга
2017-12-20, 10:44
Пользователь
=====
Видеокурсы и вебинары Check Point
2017-12-18, 12:46
Network Security
=====
Клонирует ли Ketchapp игры?
2017-12-18, 11:34
Разработчик и автор
=====
Зимняя стажировка для мобильных разработчиков в Redmadrobot
2017-12-18, 12:50
Пользователь
=====
Время — деньги. Как мы учили Яндекс.Такси точно рассчитывать стоимость поездки
2017-12-18, 12:11


Пользователь
=====
Для тех кто боялся, но все же готов попробовать. (Excel)
2017-12-18, 14:13
инженер ПТО/ инженер сметчик
=====
Телефонный номер без паспорта
2017-12-19, 13:52
Пользователь
=====
Git снизу вверх
2017-12-20, 08:58
Старший инженер по программным решениям, Intel
=====
FAQ по теме интеграции с ЕСИА
2017-12-18, 13:26
Корпоративный облачный провайдер
=====
HyperRAM: использование микросхемы с интерфейсом памяти HyperBus
2017-12-18, 13:48
Разработка и производство электронных устройств
=====
Перевод — Максимальное использование APK Analyzer
2017-12-18, 14:16
Android developer
=====
Интеллектуальные чат-боты на ChatScript: практика разработки и интеграция с JavaScript
2017-12-18, 14:31
Пользователь
=====
Найдена крупнейшая БД украденных паролей: что следует знать
2017-12-18, 15:35
Пользователь
=====
Решение проблемы конфликтов имен CSS классов в приложении на React с помощью webpack лоадера
2017-12-18, 15:50
Приветствую вас, друзья!
Началось всё с того, что я замыслил разработать кое-что так сказать для души. React приложение должно было рендериться поверх чего-то другого, например какого-то сайтика, встал вопрос того, что возможны конфликты CSS классов моего приложения с уже существующей инфраструктурой, ну я конечно же пришел к выводу, что нужно внедрить префиксы для каждого даже самого захудалого класса, ну или оборачивать все определения в класс моего главного контейнера, я все же выбрал префиксы. Но вскоре я устал от них, их получалось так много, что все это казалось мне пустой копипастой, и тогда я задумался над созданием своего лоадера для вебпака. В результате работа над ним переросла из мухи в слона, идеи переполняли меня и в итоге мой ум и руки сотворили монстра, который чуть было не вышел из под моего контроля.
Признаюсь за эти полторы недели его написания я дико устал думать, кодить и документировать сначала на английском, потом переводить с моего корявого английского на чуть менее корявый родной язык, скорее бы уже это закончилось. Зато теперь я знатный программист markdown и пользователь регулярок.
Итак нужен был просто механизм замены classname на my-app-classname, но в итоге получилось то, что я сейчас подробно опишу. Результат своих трудов я опубликовал на npm.
Для начала возможно кому-то будет интересно, как сотворить свой лоадер для вебпака. Достаточно несложно: создаем папку, называем ее my-divine-loader, в ней индексный JS файл и такой код:
Ссылка на Github репозиторий
Как видно из примера, лоадер следует добавить после бабеля и стандартных лоадеров стилей.
Название атрибута DOM элементов, которое будет парситься лоадером.
Оно может быть каким угодно и будет преобразовано в стандартный атрибут "className".
По умолчанию данный параметр равен "class".
или так
Так что "class" и "whateverName" наши названия attributeName.
После обработки лоадером код будет выглядеть так:
Для лоадера "self" — ключевое слово, которое означает наш глобальный или локальный префикс имени класса. 
Конкретно в этом случае у нас нет локального префикса, так что "self" будет равен значению глобального. 
Название атрибута для React элементов, которое будет парситься таким же образом, но не менять своего названия.
Оно также может использоваться для названий переменных и ключей объектов, которые нужно распарсить.
По умолчанию оно равно "classes".
На выходе мы получим всё тот же атрибут "classes": 
Префикс, который будет добавляться к именам классов DOM элементов.
Специальный синтаксис скажет лоадеру нужно ли добавлять какой-то из префиксов или нет.
По умолчанию он пуст, так что это означает, что никакого префикса добавлено не будет, если не задан локальный префикс.
станет 
Точка значит, что имя класса должно иметь префикс.
Подробнее о синтаксисе парсера представлено ниже.
Символ или слово, которыми префиксы и имена классов будут объединяться.
По умолчанию равен "-".
Так что, если наш "delimiter" равен "_", код будет таким:
Если данный параметр задан, лоадер попытается определить локальный префикс самостоятельно.
По умолчанию он не задан.
Есть три варианта значений: 
Для начала лоадер попытается найти строку, которая содержит:
export default (class|function) MySuperClassName 
Затем будет искать:
export default connect(...)(MySuperClassName) 
Если не найдет:
class MySuperClassName 
Ну и в конце концов возьмет первую попавшуюся строку с таким кодом:
function MySuperClassName 
Итак "MySuperClassName" будет преобразовано в локальный префикс "my" + delimiter + "super" + delimiter + "class" + delimiter + "name". 
Для CSS файлов лоадер будет искать данные в кэше, который заполняется при обработке JS файлов.
Если в кэше найдется информация о префиксах для индексного файла в той же директории,
они будут использованы, таким образом JS и CSS будут синхронизированы.
Вам следует располагать JS лоадеры перед CSS лоадерами, чтобы парсер имел нужный кэш.
станет
Также вы можете задавать JS/CSS локальные префиксы с помощью специальных директив.
Больше о них написано ниже.
Лоадер будет формировать локальный префикс из имён JS/CSS файлов:
"SuperItem.js" или "super-item.js" или "super_item.js"
превратится в префикс "super" + delimiter + "item".
Для данного способа файлы должны называться одинаково, чтобы они были синхронизированы.
Лоадер будет формировать локальный префикс из имён директорий:
"SuperItem/index.js" или "super-item/some.js" или "super_item/any.js"
станут префиксом "super" + delimiter + "item".
Самый простой способ синхронизировать JS/CSS, также ваша структура будет более понятна.
Если установлен в true, лоадер будет обфусцировать имена классов в JS и CSS файлах.
Будьте внимательны, все CSS классы будут изменены в любом случае, а JS только внутри атрибутов "attributeName" and extraAttributeName.
По умолчанию эта опция не активна.
станет
Длина в символах, до которой имена классов будут обфусцированы.
По умолчанию значение равно "7". 
Так что, если у нас "obfuscatedLength" равно 4
Лоадер будет автоматически добавлять префиксы к нашим именам классов.
Вам нужно будет использовать немного другой строковый формат имен классов.
По умолчанию этот параметр выключен. 
Например, вот это запись CSS классов для неавтопрефиксного режима:
("prefixAutoResolving" имеет значение "content")
будет преобразовано в
Здесь локальный префикс является "awesome-example-app-container" (глобальный префикс плюс добавленный автоматически от имени класса "Container").
Так что в этом режиме вам нужно добавлять точки для префиксов: одна для локального и две для глобального. 
И наконец пример описания CSS классов для автопрефиксного режима
("prefixAutoResolving" имеет значение "content")
В результате мы получим то же самое, что и в первом случае.
будет также
В этом режиме не нужно добавлять точку для локального префикса, одну нужно добавить для глобального и две для имен классов без префикса. 
В CSS файлах все работает по тем же принципам (нужно добавлять такое же число точек, так что получится максимум три точки) 
Неавтопрефиксный режим:
будет
И то же самое для автопрефиксного режима:
даст тот же самый результат
станет
А теперь результат для случая, когда "prefixAutoResolving" установлен в false,
так что у нас нет добавленного локального префикса,
только переопределенный глобальный
будет
CSS директивы делают абсолютно то же самое и выглядят весьма схоже с JS версиями,
отличие только в последней директиве, которой нет для JS файлов
Обозначает имя класса без префиксов в неавтопрефиксном режиме.
Обозначает имя класса с локальным префиксом в автопрефиксном режиме.
Обозначает имя класса с локальным префиксом в неавтопрефиксном режиме.
Обозначает имя класса с глобальным префиксом в автопрефиксном режиме.
Обозначает имя класса с глобальным префиксом в неавтопрефиксном режиме.
Обозначает имя класса без префиксов в автопрефиксном режиме.
"Мерджит" имя класса или массив имен из переменной.
Лоадер автоматически добавит "импорт" нужного для "мерджинга" модуля.
Переменная уже должна содержать имя/имена классов с префиксами или быть обфусцирована.
Пример того, как это в принципе работает: 
В родительском компоненте используем extraAttributeName: "classes"
в дочернем (с именем Icon)
так что в итоге получим 
в родителе:
в компоненте Icon "import" автоматически добавлен
А так код будет выглядеть в собранном "бандле":
То же самое, что и с переменной выше, но говорит лоадеру, что переменная является строкой, а не массивом или "undefined",
чтобы лоадер не использовал функцию мерджинга, однако если есть другие переменные вида $name, функция все равно будет использована
станет
Добавляет локальный (по отношению к дочернему компоненту) префикс к имени класса.
Возьмем пример с Icon выше и немного изменим его.
В принципе можно просто использовать запись вида ".icon-thing",
но первый вариант соединит их нужным разделителем.
Так что у нас будет такой HTML
Динамическое имя класса, локальный префикс плюс значение переменной.
Это всегда локальный префикс независимо от режима.
будет
Динамическое имя класса, глобальный префикс плюс значение переменной.
Это всегда глобальный префикс независимо от режима.
будет
Невозможно (или просто х.з. как) обфусцировать такой динамический класс,
но для таких целей внедрена специальная "фейковая" функция $classy.
Она создает "карту" имен классов для обфускатора. 
Вот пример:
В неавтопрефиксном режиме будет преобразовано в:
И в автопрефиксном режиме будет:
Так что переменная "className" будет реальным именем класса, которое можно обфусцировать и иметь вид:
Другой способ для этих нужд, но уже c различными "паттернами":
Даст код:
Ну и последнее как можно использовать $classy:
станет
станет
Можно добавлять пробелы между символами "?" и ":" 
Но вот в условии пробелы недопустимы:
Такая запись будет преобразована в некорректное имя класса вида
Если так хочется добавить пробелы, оберните условие в круглые скобки
Возможно указывать только один знак "$" в начале условия,
для остальных переменных условия знак можно не добавлять:
будет
И с отрицанием
будет
Точки работают по таким же принципам, как и всегда, в зависимости от режима.
Одна точка для реальных имён классов без префиксов.
Две точки для имён с локальным префиксом.
Три точки для имён с глобальным префиксом.
"Self" — ключевое слово обозначающее локальный или глобальный префикс, если таковые определены, иначе имя класса будет просто self
Одна точка для имён классов с локальным префиксом.
Две точки для имён с глобальным префиксом.
Три точки для реальных имён классов без префиксов.
Если локальный префикс не определен, будет использован глобальный.
Пример сахарного синтаксиса:
будет преобразовано в:
Можно также с пробелами:
станет
Выражение должно начинаться с ключегово слова "var" и заканчиваться точкой с запятой или переносом строки.
rubb = left: 0; right: 0; top: 0; bottom: 0;
l = left: 0;
l10 = left: 10px;
l-10 = left: -10px;
l50p = left: 50%;
l-50p = left: -50%; 
r = right: 0;
r10 = right: 10px;
r-10 = right: -10px;
r50p = right: 50%;
r-50p = right: -50%; 
t = top: 0;
t10 = top: 10px;
t-10 = top: -10px;
t50p = top: 50%;
t-50p = top: -50%; 
b = bottom: 0;
b10 = bottom: 10px;
b-10 = bottom: -10px;
b50p = bottom: 50%;
b-50p = bottom: -50%; 
z10 = z-index: 10; 
w = width: 100%;
w100 = width: 100px;
w50p = width: 50%; 
h = height: 100%;
h150 = height: 150px;
h20p = height: 20%; 
wh = width: 100%; height: 100%;
wh20 = width: 20px; height: 20px;
wh20p = width: 20%; height: 20%; 
mnw = min-width: 0;
mnw100 = min-width: 100px;
mnh = min-height: 0;
mnh100 = min-height: 100px;
mxw = max-width: none;
mxw100 = max-width: 100px;
mxh = max-height: none;
mxh100 = max-height: 100px; 
auto = margin: auto;
m = margin: 0;
m5 = margin: 5px;
m10-5 = margin: 10px 5px;
m10-5-10-5 = margin: 10px 5px 10px 5px; 
ml = margin-left: 0;
ml5 = margin-left: 5px;
ml-5 = margin-left: -5px;
ml5p = margin-left: 5%;
ml-5p = margin-left: -5%; 
mr = margin-right: 0;
mr5 = margin-right: 5px;
mr-5 = margin-right: -5px;
mr5p = margin-right: 5%;
mr-5p = margin-right: -5%; 
mt = margin-top: 0;
mt5 = margin-top: 5px;
mt-5 = margin-top: -5px;
mt5p = margin-top: 5%;
mt-5p = margin-top: -5%; 
mb = margin-bottom: 0;
mb5 = margin-bottom: 5px;
mb-5 = margin-bottom: -5px;
mb5p = margin-bottom: 5%;
mb-5p = margin-bottom: -5%; 
p = padding: 0;
p5 = padding: 5px;
p10-5 = padding: 10px 5px;
p10-5-10-5 = padding: 10px 5px 10px 5px; 
pl = padding-left: 0;
pl5 = padding-left: 5px;
pl-5 = padding-left: -5px;
pl5p = padding-left: 5%;
pl-5p = padding-left: -5%; 
pr = padding-right: 0;
pr5 = padding-right: 5px;
pr-5 = padding-right: -5px;
pr5p = padding-right: 5%;
pr-5p = padding-right: -5%; 
pt = padding-top: 0;
pt5 = padding-top: 5px;
pt-5 = padding-top: -5px;
pt5p = padding-top: 5%;
pt-5p = padding-top: -5%; 
pb = padding-bottom: 0;
pb5 = padding-bottom: 5px;
pb-5 = padding-bottom: -5px;
pb5p = padding-bottom: 5%;
pb-5p = padding-bottom: -5%; 
flex = display: flex;
flcen = align-item: center; justify-content: center;
bl = display: block;
inb = display: inline-block; 
fix = position: fixed;
abs = position: absolute;
rel = position: relative;
box = box-sizing: border-box; 
ova = overflow: auto;
ovh = overflow: hidden;
ovs = overflow: scroll; 
lt = text-align: left;
rt = text-align: right;
cen = text-align: center;
just = text-align: justify; 
vtop = vertical-align: top;
vmid = vertical-align: middle;
vbot = vertical-align: bottom; 
cur = cursor: default;
cur-name = cursor: name;
pntr = cursor: pointer;
cnt = content: "";
nor = resize: none; 
fl = float: left;
fr = float: right;
clr = clear: both; 
bold = font-weight: bold;
it = font-style: italic;
un = text-decoration: underline; 
lh = line-height: 0;
lh20 = line-height: 20px;
ls = letter-spacing: 0;
ls2 = letter-spacing: 2px;
fs = font-size: 0;
fs15 = font-size: 15px;
ff-name = font-family: name; 
o = opacity: 0;
o5 = opacity: 0.5;
o10 = opacity: 1; 
ol = outline: 0;
ol-000 = outline: 1px solid #000;
ol-EEE-2 = outline: 2px solid #EEE;
ol-EEE-2-dashed = outline: 2px dashed #EEE; 
bo = border: 0;
bo-000 = border: 1px solid #000;
bo-EEE-2 = border: 2px solid #EEE;
bo-EEE-2-dashed = border: 2px dashed #EEE; 
bol = border-left: 0;
bol-000 = border-left: 1px solid #000;
bol-EEE-2 = border-left: 2px solid #EEE;
bol-EEE-2-dashed = border-left: 2px dashed #EEE; 
bor = border-right: 0;
bor-000 = border-right: 1px solid #000;
bor-EEE-2 = border-right: 2px solid #EEE;
bor-EEE-2-dashed = border-right: 2px dashed #EEE; 
bot = border-top: 0;
bot-000 = border-top: 1px solid #000;
bot-EEE-2 = border-top: 2px solid #EEE;
bot-EEE-2-dashed = border-top: 2px dashed #EEE; 
bob = border-bottom: 0;
bob-000 = border-bottom: 1px solid #000;
bob-EEE-2 = border-bottom: 2px solid #EEE;
bob-EEE-2-dashed = border-bottom: 2px dashed #EEE; 
br = border-radius: 0;
br5 = border-radius: 5px;
br50p = border-radius: 50%;
br5-10-10-0 = border-radius: 5px 10px 10px 0; 
bsp = border-spacing: 0;
bsp2 = border-spacing: 2px;
c-fff = color: #fff;
bc-fff = background-color: #fff;
boc-fff = border-color: #fff; 
shad = box-shadow: none;
shad-000-10 = box-shadow: 0 0 10px #000;
shad-000-10-1-1 = box-shadow: 1px 1px 10px #000; 
tshad = text-shadow: none;
tshad-000-2 = text-shadow: 0 0 2px #000;
tshad-000-2-1-1 = text-shadow: 1px 1px 2px #000; 
tra-c-3-bc-3-o-3 = transition: color 0.3s, background-color 0.3s, opacity 0.3s;
rot20 = transform: rotate(20deg);
rot-45 = transform: rotate(-45deg);
ell = text-overflow: ellipsis; overflow: hidden; white-space: nowrap;
nowr = white-space: nowrap;
hid = visibility: hidden; 
norep = background-repeat: no-repeat;
repx = background-repeat: repeat-x;
repy = background-repeat: repeat-y;
cvr = background-size: cover; 
bpcen = background-position: 50% 50%;
bp-20-20 = background-position: 20px 20px;
bp-50p-20p = background-position: 20% 20%;
bp-c-b = background-position: center bottom;
bp-r-t = background-position: right top;
bp-l-10 = background-position: left 10px; 
Тут все посложнее, и для начала нужно определить источник(и) файлов.
Для этих нужд имеются директивы, их может быть несколько, если файлы располагаются в разных директориях.
Путь конечно же к директория:
А сокращения выглядяь так:
Итак сокращение состоит из: 
Так что на выходе вы получите такую запись:
Дальше уже дело за лоадерами картинок, пути указаны для них. 
Итак список сокращений: 
png-png-filename = background-image: url(../some/path/png-filename.png);
jpg-jpg_filename = background-image: url(../some/path/jpg_filename.jpg);
jpeg-oneMoreJpgFilename = background-image: url(../some/path/oneMoreJpgFilename.jpeg);
gif-giffy = background-image: url(../some/path/giffy.gif);
svg-blabla = background-image: url(../some/path/blabla.svg); 
Конечно же можно добавить номер источника: 
png2-a = background-image: url(../path/to/second/source/a.png);
gif33-d = background-image: url(../path/to/33-th/source/d.gif); 
Эти две строки идентичны: 
jpg-e = background-image: url(../path/to/first/source/e.jpg);
jpg1-e = background-image: url(../path/to/first/source/e.jpg); 
Спасибо, дорогой друг, что досмотрел все это до конца, можешь ругать меня или хвалить, возможно где-то мои идеи не совсем красивы с твоей точки зрения, я просто представил свое вИдение. Если понравилось делись с другими, хочу славы и богатства, шучу конечно, просто немножечко внимания. Я написал это для своих нужд, и буду использовать в своем проекте, допиливать, поддерживать и совершенствовать. Вам слово, товарищи!

User
=====
Как в «Альфе» интернет-банк для бизнеса обновляли
2017-12-21, 14:21

=====
Упрощаем лог действий пользователя
2017-12-19, 15:02
User
=====
Лекции Техносферы. Нейронные сети в машинном обучении
2017-12-18, 16:43

Представляем вашему вниманию очередную порцию лекций Техносферы. На курсе изучается использование нейросетевых алгоритмов в различных отраслях, а также отрабатываются все изученные методы на практических задачах. Вы познакомитесь как с классическими, так и с недавно предложенными, но уже зарекомендовавшими себя нейросетевыми алгоритмами. Так как курс ориентирован на практику, вы получите опыт реализации классификаторов изображений, системы переноса стиля и генерации изображений при помощи GAN. Вы научитесь реализовать нейронные сети как с нуля, так и на основе библиотеке PyTorch. Узнаете, как сделать своего чат-бота, как обучать нейросеть играть в компьютерную игру и генерировать человеческие лица. Вы также получите опыт чтения научных статей и самостоятельного проведения научного исследования. 
Список лекций:
Нейронные сети. Базовые блоки полносвязных нейронных сетей. Алгоритм обратного распространения ошибки. 
Алгоритм обратного распространения ошибки для ветвящихся структур. Проблемы обучения нейронных сетей. Предобработка данных, аугментация, регуляризация. Стохастический градиентный спуск. Подготовка данных при помощи PyTorch. 
Графы вычислений в PyTorch. Операции с тензорами. Автоматическое дифференцирование. Полносвязные сети. Ветвящиеся архитектуры. Поведение сети при обучении и предсказании: флаги volatile и requires_grad. Сохранение и загрузка модели. 
Свертка. Пулинг. Светрочные нейронные сети. Примеры использования сверточных сетей. Интерпретация обученных моделей. 
Инициализация весов: He, Xavier. Регуляризация: Dropout, DropConnect. Нормализация: batch normalization. 
Современные архитектуры сверточных сетей. Сети Inception и ResNet. Transfer learning. Применение нейронных сетей для задач сегментации и локализации. 
Задача оптимизации. SGD, Momentum, NAG, Adagrad, Adadelta, Adam. 
Задача снижения размерности. MDS, Isomap. Метод главных компонент (PCA). Вывод главных компонент и доказательство метода множетелей Лагранжа. Автокодировщики. Denoising и разреженные автокодировщики.
Рекуррентные сети. Обратное распространение ошибки сквозь время. LSTM-сети. GRU-сети. Многослойные рекуррентные архитектуры. Модификация dropout и батч-нормализации для рекуррентных сетей. 
Примеры задач. Обучение представлений: Word2Vec. Ускорение пары linear+softmax: hierarchical softmax, differentiated softmax. Генерация предложений. Модель Seq2Seq. Beam search для поиска лучшего ответа. Приемы для повышения разнообразности ответов. 
Генеративные и дискриминативные модели. Равновесие Нэша. Генеративные конкурирующие сети (GAN). Генеративные автокодировщики (AAE). Техника domain adaptation. Domain adaptation для перевода изображений между доменами. Wasserstein GAN. 
Модель вариационного автокодировщика (VAE). Интерпретация обученных моделей: Deep Dream. Перенос стиля: Artistic style. Ускорение стилизации. 
Основные понятия обучения с подкреплением: агент, среда, стратегия, награда. Value function и Q-function. Уравнения Беллмана. Алгоритм Policy iteration. 
Алгоритм Q-learning. Модельные подходы. Алгоритм DQN. Alpha Go. 
Плейлист всех лекций находится по ссылке. Напомним, что актуальные лекции и мастер-классы о программировании от наших IT-специалистов в проектах Технопарк, Техносфера и Технотрек по-прежнему публикуются на канале Технострим.
Другие курсы Техносферы на Хабре:
Информацию обо всех наших образовательных проектах вы можете найти в недавней статье.
Пользователь
=====
Автоматизированная корректировка отступов в верстке на основании типографических стилей и текстовых метрик
2017-12-18, 16:12
User
=====
Внедрение зависимостей в .Net Марка Симана 3 — Сквозные аспекты приложения, перехват, декоратор
2017-12-18, 16:41
Инженер
=====
Серьезно, игры?
2018-01-11, 10:53
User
=====
Avito Product Analytics Meetup — видео, фото, слайды
2017-12-20, 13:04
Недавно в Avito прошёл первый Product Analytics Meetup. Мы говорили об NPS в продуктовой разработке, персонализации веб-сайтов на основе customer journey map, кросс-продуктовых эффектах, способах повышения конверсии, построении и эволюции аналитических хранилищ данных. Под катом — видеозаписи выступлений, слайды от докладчиков и ссылка на фотоотчёт. 

Как действовать в условиях лавинообразного роста объема данных и их сложности? Почему следование ограничениям шестой нормальной формы (Anchor Modeling) помогает оперативно расширять хранилище, эффективно масштабировать нагрузку в среде MPP СУБД и развиваться в условиях микросервисов? Ответы — в видео. 
Если продукт не приносит выручки напрямую, то как измерить его влияние на продажи? Какими метриками разумнее всего воспользоваться? Об этом рассказала аналитик Туту.ру Екатерина Лосева. 
Из видео вы узнаете о том, что такое customer journey map (CJM). Будут показаны основные подходы к автоматизированному выделению паттернов поведения пользователей с помощью методов машинного обучения. Алексей Чернобровов рассказывает о построении персонализации на основе CJM и правильном выборе метрик для оценки улучшения продукта. В докладе приведён ряд понятных примеров, которые помогут в изучении этой темы. 
Секрет успешного бизнеса и светлого будущего — счастье клиентов. Но измерить его и научиться работать над повышением этого уровня — непростая задача. Во всем мире её решают с помощью опросов клиентов. Наиболее успешная методика таких опросов называется Net Promoter Score, или NPS. В докладе рассказана success-story об имплементации NPS во все части продукта Avito и показано, как большой поток голосов пользователей помогает нам принимать эффективные решения и постоянно становиться лучше.
В докладе рассказывается об основных этапах в развитии логики сортировки отелей, описываются проблемы и ошибки, с которыми столкнулись специалисты из Ostrovok.ru, а также выводы, которые были сделаны. 
Спасибо всем, кто пришёл на митап! 
Смотрите фотоотчёт, подписывайтесь на наш Timepad. Напишите в комментариях, какие темы вам бы хотелось обсудить в рамках подобных встреч, будем рады!
Пользователь
=====
Стек: анализируем значения параметров
2017-12-26, 14:44
User
=====
Netflix подбирает оптимальные обложки фильмов для каждого зрителя
2017-12-18, 17:57
автор, переводчик, редактор
=====
Оценка премии опционов — аналитические формулы vs моделирование
2017-12-18, 17:18
















Пользователь
=====
Один день в Альфа-Лаборатории: Java-разработка
2017-12-18, 18:00
ContentProvider
=====
R и Информационная безопасность. Как устранить противоречие интересов и запустить R на Linux в оффлайн-режиме
2017-12-18, 17:58
Является продолжением предыдущих публикаций.
Очень часто попытки применить инструменты DataScience в корпоративной среде встают в полное противоречие с требованиями Службы Информационной Безопасности (СИБ). В мире DataScience рекомендация «поставь с гитхаба» становится практически нерешаемой при полной изоляции аналитической машины от интернета. Тем не менее, задача запуска на linux инфраструктуры R в offline окружении вполне решаемая. Ниже приведу последовательность мантр, которые позволят это исполнить. Если какие-то шаги будут не совсем прозрачными, то скорректирую по мере появления комментариев. Эти же шаги можно использовать и для online инсталляции, пропуская шаги, относящиеся к хитрым трюкам или созданию локальных репозиториев. Собрано по крупицам на основании многократных инсталляций под разнообразные задачи. Практика показала, что тема весьма актуальна.
Шаги исполняются на srchost и dsthost. В качестве srchost может выступать и просто Windows машина аналитика, тогда шаги по развертыванию srchost можно опустить.
На srchost пакеты устанавливаются из интернета, на dsthost — из локального репозитория. Далее srchost будет использоваться для создания miniCRAN.
Необходимо для работы RMarkdown
Добавляем поддержку кириллицы:
Установка Roboto Condensed:
Используем пакет miniCRAN для инициализации репозитория на srchost. Предполагаем, что она работает под управлением CentOS и на ней стоит R. Считаем, что локальная директория /opt/miniCRAN, куда будут загружаться необходимые пакеты, уже создана.
Ставим miniCRAN
Выкачиваем нужные нам пакеты (дополнить чего не хватает для вашего проекта) вместе с зависимостями
на dsthost вручную скопировать содержимое /opt/miniCRAN
На dsthost в файле /usr/lib64/R/library/base/R/Rprofile дописываем
На srchost 
"Установка Microsoft ODBC Driver for SQL Server для Linux и macOS":
на dsthost
После установки библиотек ищем расположение h файлов следующей командой find . -type f -name udunits2.h и запускаем инсталляцию пакета из консоли R со следующими параметрами:
на dsthost
Завести отдельного пользователя с uid > 100! от лица которого будем заходить в RStudio Server:
на srchost
на srchost
Shiny Server
на srchost:
Предыдущая публикация — «Использование R для «промышленной» разработки».
Data Science
=====
Naive Spellchecking, или поиск ближайших слов из словаря по метрике Левенштейна на Scala
2017-12-19, 09:18
Программист
=====
Технология добра
2017-12-19, 16:00
Корпоративный облачный провайдер
=====
Ой, у меня задержка. Часть 2
2017-12-18, 19:06
Лид-редактор Yandex.Cloud, техпис, ИТ-блогер
=====
Задача со звездочкой: как мы перекодировали ФИАС в КЛАДР
2017-12-20, 12:27
Редактор и автор
=====
Как управлять секциями в БД Oracle и не сойти с ума
2017-12-21, 17:27
Пользователь
=====
Туториал по Unreal Engine. Часть 7: звук
2017-12-23, 14:02
Переводчик-фрилансер
=====
Как я взломал 40 сайтов за 7 минут (перевод)
2017-12-19, 09:00

Прошлый летом я заинтересовался вопросами информационной безопасности и взлома. Последний год я много играл в wargames, «захват флага», тестирование на проникновение, постоянно совершенствуя навыки взлома и изучая новые способы заставить компьютеры отклоняться от ожидаемого поведения.
Короче говоря, мой опыт ограничивался имитируемой средой, и, считая себя официальным хакером, я никогда не совал нос в бизнес других людей.
Это будет подробная история о том, как я взломал сервер, на котором размещалось 40 (это точное число) веб-сайтов, и о моих находках.
Друг сообщил мне, что его веб-сайт XSS уязвим, и попросил меня взглянуть. Я попросил у него официальное разрешение на полное тестирование его веб-приложения на его сервере. Ответ был положительным.

Первый шаг – найти как можно больше информации о своем враге, пытаясь как можно меньше его тревожить.
На этом этапе мы запускаем наш таймер и начинаем сканирование.
Множество открытых портов! Судя по тому, что порты FTP (порт 21) и SMB (порты 139/445) открыты, можно предположить, что сервер используется для размещения и совместного использования файлов, а также является веб-сервером (порты 80/443 и прокси на 8080/8081).

При сканировании UDP-порта будет рассмотрено более 1000 портов, если вышеизложенной информации недостаточно. Единственным портом, с которым разрешено взаимодействовать (без учетных данных), является порт 80/443.
Не теряя времени, я запускаю gobuster, чтобы найти какие-нибудь интересные файлы на веб-сервере, пока я буду копать информацию вручную.
Оказывается, путь /admin был «административным инструментом», который позволял аутентифицированным пользователям изменять материал на веб-сервере. Он требует параметры доступа, которых у нас нет (спойлер: gobuster не нашел ничего ценного).
Веб-сайт просит нас войти. Нет проблем. Создаем учетную запись с фиктивной электронной почтой, щелкаем по электронной почте подтверждения и входим в систему через несколько секунд.
Веб-сайт приветствует нас, предлагает перейти к профилю и обновить фотографию. Как мило.
Похоже, сайт сделан на заказ. Я собираюсь протестировать уязвимость с неограниченной загрузкой файлов. На моем терминале я выполняю:
Я пытаюсь загрузить «картинку» и – бинго! Загрузчик позволяет загрузить файл exploit.php. Конечно, у него нет эскизов, но это значит, что мой файл где-то загружен.

Ожидается, что загрузчик выполнит какую-либо обработку загруженного файла, проверит его расширение и заменит принятое расширение, например .jpeg, .jpg, чтобы избежать удаленного выполнения кода злоумышленником, загружающим вредоносный код.
В конце концов, люди заботятся о безопасности.
Похоже, что webshell готов и работает:

Видим, что веб-сервер запускает perl-скрипты (реально? perl?). Мы берём обратную оболочку perl из нашего любимого cheatsheet, устанавливаем IP/Port и получаем в качестве награды low-privileged оболочку – извините, нет скришота.
К моему огромному удивлению, на сервере размещался не 1 сайт, а сразу 40 разных. К сожалению, я не сохранил скриншоты каждой детали, но вывод был примерно таким:
Удивительно, но у меня был доступ на чтение ко всем размещенным веб-сайтам, а это означало, что я мог читать весь бэкенд-код сайтов. Я ограничился кодом example.com.
Примечательно, что внутри каталога cgi-admin/pages все скрипты perl соединялись с базой данных mysql как root. Учетные данные для базы данных были в открытом виде. Пусть они будут root:pwned42.
Разумеется, на сервере была запущена MariaDB, и мне пришлось решить эту проблему, прежде чем получить доступ к базе данных. После этого мы выполняем:
И мы находимся в базе данных с привилегиями root.

Морально я обязан здесь остановиться и поделиться выводами. Потенциальный ущерб уже огромен.
Процесс mysql запускался под root, поэтому я решил, что попробовал выполнить \! whoami в надежде получить root. К сожалению, я все еще был apache.
Я поделился своими выводами и получил разрешение копать глубже.
Прежде чем искать способы повысить свои привилегии до root и иметь возможность причинить огромный потенциальный ущерб, я посмотрел, какие другие интересные файлы мог бы читать, будучи ограниченным пользователем.
Я вспомнил об открытых портах SMB. Это означало, что где-то в папке должна быть другая папка, которая используется в системе среди пользователей. После небольшого поиска в каталоге /home/samba/secure появляется следующее:

Внутри всех этих каталогов были файлы каждого пользователя хостинговой компании. Это включало все виды конфиденциальных данных, среди прочего:
Осмотревшись еще немного как apache, я решил, что пришло время пойти на большую рыбу – получить доступ root. Используя шпаргалки, начинаю перебирать систему.
В процессе исследования на уязвимости я уже перебрал большинство методов и, похоже, не смог найти ничего, что увеличило бы мою точку опоры.
В задачах Capture the Flag, которые я использую для игры, операционная система обычно пропатчена. Это некоторая намеренно неверно настроенная служба, которая в конечном итоге дает вам привилегии root. Однако в реальном мире люди не латают дыры.
Какой Linux работает на сервере?
Какая версия ядра?

Это похоже на старую версию ядра.

Это напоминает вам что-то? Если нет, прочитайте здесь (подсказка: это ОЧЕНЬ серьезно).
Я нашел этот пост в блоге, который указал мне проверить, было ли ядро уязвимым для найденного здесь скрипта.

Временные метки и восстановленные сайты Firefox отредактированы
С последующим:

Я мгновенно написал электронное письмо, полностью раскрывающее детали и потенциальное влияние каждого шага, как описано выше. Уф.
На следующий день со мной связался друг (он связался с работающей на сервере компанией) и рассказал, что ошибка в загрузке файлов была исправлена.
Подводя итоги, мы обнаружили следующее:
Наконец, мы злоупотребили непропатченным ядром для получения доступа root.
Начнем с аплоудера, который дал основной плацдарм. Поскольку бэкенд всего веб-приложения был написан в perl, я не могу предложить решения.
Решение, которое я бы предложил, было бы таким: не использовать perl в 2017 году, но это только мое мнение.
Что касается файловой системы, я рекомендую проявлять большую осторожность при назначении правильных прав доступа к файлам для пользователей в соответствии с принципом наименьших привилегий. Таким образом, даже если низкоприоритетный пользователь, такой как apache, получает доступ, он не может читать конфиденциальные файлы.
Запуск всех веб-сайтов на одном сервере – плохая идея, я не уверен, позволит ли докеризированный подход решить проблему.
Наличие одинаковых учетных данных для всех баз данных – безусловно, плохая идея.
Наконец, пропачьте все. Это всего лишь одна команда: su -c 'yum update' (специфичная для CentOS).
Оригинал: How I Hacked 40 Websites in 7 minutes.
CEO в Southbridge
=====
Сбор и фильтрация событий входа в систему с помощью Log Parser
2017-12-19, 09:23
User
=====
Как прочитать большой файл средствами PHP (не грохнув при этом сервак)
2017-12-18, 23:14
Перевод статьи Christopher Pitt.
PHP разработчикам не так уж часто приходится следить за расходом памяти в своих приложениях. Сам движок PHP неплохо подчищает мусор за нами, да и модель веб-сервера с контекстом исполнения, "умирающим" после выполнения каждого запроса, позволяет даже самому плохому коду не создавать больших долгих проблем.
Однако, в некоторых ситуациях, мы можем столкнуться с проблемами нехватки оперативной памяти — например, пытаясь запустить композер на маленьком VPS, или при открытии большого файла на сервере не богатом ресурсами.

Последняя проблема и будет рассмотрена в этом уроке.
Весь код доступен по ссылке https://github.com/sitepoint-editors/sitepoint-performant-reading-of-big-files-in-php
При проведении любых оптимизаций кода, мы всегда должны замерять результаты его выполнения до и после, для того чтобы оценивать эффективность(или пагубность) наших оптимизаций.
Обычно измеряют загрузку CPU и использование оперативной памяти. Часто бывает, что экономия одного, ведёт к увеличенным затратам другого и наоборот.
В асинхронной модели приложения(мультипроцессорные и многопоточные) всегда очень важно следить как за процессором, так и за памятью. В классических приложениях контроль ресурсов становится проблемой лишь при приближении к лимитам сервера.
Измерять использование CPU внутри PHP плохая идея. Лучше использовать какую-либо утилиту, как top из Ubuntu или macOS. Если вы у вас Windows, то можно использовать Linux Subsystem, чтобы иметь доступ к top.
В этом уроке мы будем измерять использование памяти. Мы посмотрим, как память расходуется в традиционных скриптах, а затем применим парочку фишек для оптимизации и сравним результаты. Надеюсь, к концу статьи, читатель получит базовое понимание основных принципов оптимизации расхода памяти при чтении больших объемов данных.
Будем замерять память так:
Эту функцию мы будем использовать в конце каждого скрипта, и сравнивать полученные значения.
Существует много разных подходов для эффективного чтения данных, но всех их условно можно разделить на две группы: мы либо считываем и сразу же обрабатываем считанную порцию данных(без предварительной загрузки всех данных в память), либо вовсе преобразуем данные в поток, не заморачиваясь над его содержимым.
Давайте представим, что для первого варианта мы хотим читать файл и отдельно обрабатывать каждые 10000 строк. Нужно будет держать по крайней мере 10000 строк в памяти и передавать их в очередь(в какой бы форме она не была реализована).
Для второго сценария, предположим, мы хотим сжать содержимое очень большого ответа API. Нам не важно, что за данные там содержатся, важно вернуть их в сжатой форме.
В обоих случаях нужно считать большие объемы информации. В первом, нам известен формат данных, во втором, формат значения не имеет. Рассмотрим оба варианта.
Есть много функций для работы с файлами. Давайте напишем с их помощью свой ридер:
Тут мы считываем файл с работами Шекспира. Размер файла около 5.5MB и пиковое использование памяти 12.8MB.
А теперь, давайте воспользуемся генератором:
Файл тот же, а пиковое использование памяти упало до 393KB! Но пока мы не выполняем со считываемыми данными никаких операций, это не имеет практической пользы. Для примера, мы можем разбивать документ на части, если встретим две пустые строки:
Хотя мы разбили документ на 1,216 кусков, мы использовали лишь 459KB памяти. Всё это, благодаря особенности генераторов — объем памяти для их работы равен размеру самой большой итерируемой части. В данном случае, самая большая часть состоит из 101,985 символов.
Генераторы могут применяться и в других ситуациях, но данный пример хорошо демонстрирует производительность при чтении больших файлов. Возможно, генераторы один из лучших вариантов для обработки данных.
В ситуациях, когда обработка данных не требуется, мы можем пробрасывать данные из одного файла в другой. Это называется пайпингом( pipe — труба, возможно потому что мы не видим что происходит внутри трубы, но видим что входит и выходит и неё). Это можно сделать с помощью потоковых методов. Но сперва, давайте напишем классический скрипт, который тупо передает данные из одного файла в другой:
Неудивительно, что этот скрипт использует намного больше памяти, чем занимает копируемый файл. Это связано с тем, что он должен читать и хранить содержимое файла в памяти до тех пор пока файл не будет скопирован полностью. Для маленьких файлов в этом нет ничего страшного, но не для больших...
Давайте попробуем стримить(или пайпить) файлы, один в другой: 
Код довольно странный. Мы открываем оба файла, первый на чтение, второй на запись. Затем мы копируем первый во второй, после чего закрываем оба файла. Возможно будет сюрпризом, но мы потратили всего 393KB.
Что-то знакомое. Не похоже ли это на генератор, читающий каждую строчку? Это так, потому что второй аргумент fgets определяет как много байт каждой строки нужно считывать(по умолчанию -1, т.е до конца строки). Необязательный, третий аругмент stream_copy_to_stream делает то же самое. stream_copy_to_stream читает первый поток по одной строке и пишет во второй. 
Пайпинг этого текста не особо полезен для нас. Давайте придумаем реальный пример. Предположим, что мы хотим получить картинку из нашего CDN и передать её в файл или в stdout. Мы могли бы сделать это так:
Для того чтобы осуществить задуманное этим способом потребовалось 581KB. Теперь попробуем сделать то же самое с помощью потоков.
Потратили немного меньше памяти(400KB) при одинаковом результате. А если б нам не нужно было сохранять картинку в памяти, мы могли бы сразу застримить её в stdout:
Существуют и другие потоки, в/из которых можно стримить:
Есть еще одна фишка, которую мы можем использовать — это фильтры. Промежуточный вариант, который дает нам немного контроля над потоком, без необходимости детально погружаться в его содержимое. Допустим, мы хотим сжать файл. Можно применить zip extension:
Хороший код, но он потребляет почти 11MB. С фильтрами, получится лучше: 
Здесь мы используем php://filter/zlib.deflate который считывает и сжимает входящие данные. Мы можем пайпить сжатые данные в файл, или куда-нибудь еще. Этот код использовал лишь 896KB.
Я знаю что это не совсем тот же формат, что и zip архив. Но задумайтесь, если у нас есть возможность выбрать иной формат сжатия, затратив в 12 раз меньше памяти, стоит ли это делать? 
Чтобы распаковать данные, применим другой zip фильтр.
Вот парочка статей, для тех кому хотелось бы поглубже погрузиться в тему потоков: “Understanding Streams in PHP” и“Using PHP Streams Effectively”.
fopen и file_get_contents имеют ряд предустановленных опций, но мы можем менять их как душе угодно. Чтобы сделать это, нужно создать новый контекст потока:
В этом примере мы пытаемся сделать POST запрос к API. Прописываем несколько заголовков, и обращаемся к API по файловому дескриптору. Существует много других опций для кастомизации, так что не будет лишним ознакомиться с документацией по этому вопросу.
Перед тем как закончить, давайте поговорим о создании кастомных протоколов. Если посмотреть в документацию, то можно увидеть пример:
Написание своей реализации такого тянет на отдельную статью. Но если все же озадачиться и сделать это, то можно будет легко зарегистрировать свою обертку для стримов:
Аналогичным образом, можно создать и кастомные фильтры потока. Пример класса фильтра из доков:
И его также легко зарегистрировать:
Свойство filtername в новом классе фильтра должно быть равно highlight-names. Также можно использовать инлайновый фильтр php://filter/highligh-names/resource=story.txt. Создавать фильтры гораздо легче чем протоколы. Но протоколы, имеют более гибконастраеваемые возможности и функциональность. К примеру, дной из причин для которой фильтры не годятся, а требуются протоколы — это операции с директориями, где фильтр будет нужен для обработки каждой порции данных. 
Настоятельно рекомендую поэкспериментировать с созданием собственных протоколов и фильтров. Если получится применить фильтр к функции stream_copy_to_stream, то вы получите колоссальную экономию памяти при работе с большими объемами данных. Представьте что у вас будет фильтр для ресайзинга изображений или фильтр для шифрования, а может и еще что покруче.
Хотя это не самая частая проблема, с которой мы мучаемся, очень легко накосячить при работе с большими файлами. В асинхронных приложениях, вообще очень просто положить весь сервер, если не контролировать использование памяти в своих скриптах
Надеюсь, что этот урок подарил вам несколько новых идей(или освежил их в памяти) и теперь вы сможете работать с большими файлами гораздо эффективнее. Познакомившись с генераторами и потоками( и перестав использовать функции по типу file_get_contents) можно избавить наши приложения от целого класса ошибок. That seems like a good thing to aim for!
TeamLead | Backend Developer
=====
Как мы создавали онлайн-сервис для изучения английского: от стартапа до расцвета
2017-12-19, 10:53
В 2014 году мы запустили проект Puzzle English и поделились с обитателями Хабра историей о наших похождениях по инвестиционным фондам в поисках денег. Инвесторы нашлись: венчурные фонды SOLventures и Genezis Capital профинансировали проект. И вот спустя 3 года у Puzzle English есть не только бизнес-план, но и опыт. Опыт превращения в прибыльный бизнес. Реальность преподала нам много уроков, мы набили некоторое количество шишек и совершили немало открытий. Эти три года пролетели незаметно и были насыщены разнообразными событиями. Всем этим багажом новых знаний мы хотим поделиться с вами. 

Вдруг это поможет вам избежать лишних трудностей на старте вашего проекта.

Даже если вам на самом деле не нужны деньги (хотите обойтись своими средствами или планируете сначала что-то разработать), все равно полезно на старте презентовать проект инвесторам. Это позволит вам бесплатно получить компетентную и разностороннюю оценку вашего проекта и увидеть ваши «слабые» стороны пока вы еще не потратили время на ненужную ерунду. Мы за полгода провели около 30 встреч с представителями разных фондов, и я уверен в том, что мы с пользой провели это время. На каждой встрече мы обнаруживали нашу слабую компетенцию и к следующей наращивали наш потенциал, восполняя пробелы.
Встречаясь с фондами мы привлекли инвестиции, доработали проект и, что немаловажно, получили массу практических рекомендаций по развитию нашего стартапа. Отчасти именно благодаря этим советам мы смогли стать прибыльным бизнесом. Некоторые рекомендации мы восприняли сразу и реализовали, смысл остальных стал ясен в процессе развития, после первых самостоятельно набитых нами шишек.

К тому моменту, когда мы нашли инвесторов, нам удалось собственными силами сделать проект безубыточным, но выручка была совсем небольшая, также как и динамика её роста… Всерьез встал вопрос, привлекать или не привлекать деньги. Мы решили, что инвестиции необходимы для более качественного и динамичного развития проекта.
Поэтому получив в октябре 2014 года первые 500 тысяч долларов мы собрали сильную команду программистов, пригласили технического директора Андрея Строганова, который до этого работал в Яндексе и Одноклассниках и директора по маркетингу, который до этого работал в конкурирующем стартапе. 
Рекомендация "ищите маркетолога на старте проекта", полученная от одного их инвесторов, с которым мы встречались, практически оправдана на все сто процентов. Я уверен в том, что стать прибыльным быстрорастущим проектом мы смогли не только благодаря разнообразию качественных сервисов, но и благодаря усилиям по их продвижению. Совет "ищите экспертов" был принят к действию позже, когда мы осознали необходимость создать комплексный продукт и пригласили экспертов-лингвистов, которые разработали и продолжают разрабатывать для нас качественный уникальный контент.
Благодаря сильной команде к концу 2015 года аудитория Puzzle English доросла до уровня 2 млн. зарегистрированных пользователей (на старте в 2014 году было 60 000), в том же году мы запустили первое мобильное приложение и получили 45,7 млн. рублей выручки по итогам 2015 года.
Среди рекомендаций инвесторов, были и такие, которые содержали в себе рациональное зерно, дали нам хорошие ориентиры, но при критическом рассмотрении мы поняли, что не готовы полностью им следовать. 
В частности, мы не последовали рекомендации сделать все бесплатным, а утверждение "Лучше иметь проект с миллионной аудиторией и 1% платящих пользователей, чем 100% платный проект с тысячной аудиторией" взяли на вооружение, но не восприняли как аксиому. 
Особенность нашего продукта в том, что мы один раз вкладываемся в его создание, а использоваться он может бесконечно. Поэтому приведенное выше утверждение имеет смысл для нас. Но анализ чужого опыта, например, Coursera, показал, что "бесплатность" может оказаться губительной. На этом ресурсе монетизация реализована через продажу сертификата, но до его получения доходит только 1% обучающихся, а получить его желает аудитория, составляющая лишь небольшую долю этого процента. В таком случае сложно достичь безубыточности и уж тем более получить прибыль. 
Бесплатность в глазах пользователя обесценивает продукт, а высокая плата отталкивает. Мы нашли свой подход: на Puzzle English много бесплатного контента и сервисы можно попробовать бесплатно. Но все же за основной контент и полноценные услуги мы ввели весьма демократичные цены, которые в десятки раз ниже офлайновых за аналогичный продукт. Мы также используем гибкую систему скидок, подбадриваем пользователей разнообразными акциями. Таким образом, нам удалось выполнить рекомендацию все того же инвестора: "Сделайте такой ресурс, чтобы на нем задерживались люди и постоянно возвращались. И рекомендовали его друзьям". 

На старте я был увлечен разработкой продукта, количество идей, требовавших воплощения, зашкаливало. Я пробовал разные онлайн курсы, каждый раз находил, что можно в них улучшить, и это открытие сразу хотелось реализовать в своем проекте. Сильная команда программистов могла реализовать все задумки, и мы не отказали себе в удовольствии — начали параллельно делать много продуктов. И только в середине 2016 года мы вспомнили о предупреждении одного из потенциальных инвесторов: "типичная ошибка, когда заботясь сразу о большом количестве KPI, вы размываете фокус внимания. Одни цифры идут вверх, другие идут вниз, вы не понимаете, что делать. Сделайте очень раннюю бету с совершенно минимальным функционалом и смотрите только на один KPI: retention rate – удержание пользователя".
Мы совершили эту ошибку, и реальность заставила нас сфокусироваться. Когда в середине 2016 года будущее перестало казаться светлым, а экономика проекта стала убыточной, мы провели анализ, нашли убыточные продукты и заморозили их развитие. Наш опыт показал, что как бы ни хотелось реализовать все идеи сразу, в начале стоит выделить 1-2 продукта-локомотива, и лишь по достижении устойчивого развития, запускать новые идеи в работу.
Первые успехи вызывают эйфорию. Любые высоты кажутся достижимыми, а амбициозные планы почти реализованными. Сохраняйте хладнокровие, оно вам пригодится в кризисных ситуациях, которые легко могут последовать за эйфорией.
Достигнув к концу 2015 года первых успехов, мы сформировали суперагрессивный план по выручке в 2016 году. И для того, чтобы его реализовать, пригласили в компанию операционного и технического директоров, менеджеров продуктов. Мы переоценили потенциал нашего развития и начали нести потери: хотя проект продолжал расти, но темпов роста не хватало, чтобы покрыть затраты. Нам понадобилось хладнокровие не только для того, чтобы наступить на горло своей песне и заморозить убыточные продукты, но и для того, чтобы уволить сотрудников. Прежде всего мы расстались с теми людьми, которые сами ничего не производили — это были менеджеры, контролирующие работу мобильных и веб-разработчиков. 
В результате уже к ноябрю мы вышли на самоокупаемость. По итогам 2016 года Puzzle English существенно нарастил количество зарегистрированных на платформе пользователей, число которых превысило 3 миллиона человек. Количество активных пользователей, занимающихся как минимум раз в месяц (MAU), также удвоилось и достигло 600 000 человек. Выручка компании составила 80 млн. руб. — почти двукратный рост к 2015 году.
Начиная самостоятельный бизнес-проект, вы должны отдавать себе отчет в том, что покой теперь вам будет только сниться. Вам предстоит работать 24 часа в сутки семь дней в неделю и почти без праздников и выходных. 
Наемные менеджеры не способны развивать бизнес компании эффективнее ее основателей. Даже несмотря на свои квалификацию и управленческий опыт сторонние директора не обладают предпринимательским чутьем и тем пониманием рынка, которое позволило вам — основателю бизнеса найти нишу для компании. Они также склонны применять стандартные схемы и подходы, которые не позволяют эффективно растить стартап в высококонкурентной среде. Наемные менеджеры не способны брать на себя предпринимательские риски за нестандартные бизнес-решения, они пропускают или не хотят улавливать слабые тревожные сигналы, и поэтому основателю, доверившему наемному менеджменту управление бизнесом на столь ранней стадии его развития, впоследствии приходится заниматься антикризисным управлением. 
Это понимание выросло в результате прохождения через кризис 2016 года. Так что за спинами наемников не удастся отсидеться, хотя и хочется иногда заняться только той работой, которая доставляет удовольствие, отбросив рутину повседневного менеджмента.
В стартап проекте абсолютно не применим общепринятый подход к управлению, практикуемый в крупных компаниях, когда на одного сотрудника, производящего продукт, в среднем приходится по 2-3 менеджера. Экономика стартапа такой роскоши не выдержит, да и практического смысла в такой модели управления я не вижу. Кризис 2016 года окончательно убедил меня в этом. В результате, преодолев сложности, в Puzzle English мы применили горизонтальную модель управления, которую практикуют такие известные компании, как Zappos, Valve, Basecamp, Pixar.
При такой организации бизнес-процессов не существует не только менеджеров, но и отделов и должностей в их классическом смысле. Наши сотрудники сами ставят цели, организуют свою работу, придерживаются сроков, анализируют итоги, — несут полную ответственность за функцию, которую они выполняют. 
Избавившись от менеджеров, мы получили рост мотивации и производительности сотрудников, что незамедлительно сказалось на развитии проекта. С ноября 2016 года Puzzle English — прибыльный бизнес.

В нашем случае нестандартным и очень эффективным ресурсом развития компании являются сотрудники. Самостоятельные, целеустремленные профессионалы сами организуют свою работу, осознают свои задачи и ответственность, не испытывают давления бюрократических процедур и имеют достаточно свободы для самореализации, проявления своего предпринимательского потенциала в своей функциональной сфере. Равность позиций сплачивает команду, рождает чувство локтя и объединяет людей, нацеленных на общий результат. 
В Puzzle English многое построено на доверии. Мы даем свободу действий сотрудникам, не контролируем их "от и до", оставляем за людьми право допускать ошибки и учиться на них. У нас также есть возможность “горизонтального” перемещения, к примеру, не справился с функцией, но отлично себя проявил в выполнении чего-то другого, или хочешь расти и осваивать смежные с твоими нынешними функциями области, прошел обучение и хочешь попробовать себя в новом поле деятельности. Мы всегда даем людям второй шанс. И люди максимально реализуют себя, вкладывают в дело весь свой потенциал и свои таланты. Возможно поэтому у нас есть результат.
В 2017 году мы расширили линейку продуктов, начали разработку сервисов на базе искусственного интеллекта и машинного обучения и при этом остались прибыльным бизнесом. Сегодня у нас более 4 500 000 подписчиков, 1 000 000 учащихся, четверть из них учат английский через мобильные устройства.
В ведении бизнеса не бывает универсальных схем, каждый проект уникален и развивается в уникальных обстоятельствах. Все новое и все приходится разгребать самому. Тем не менее фрагментарно чужой опыт все же применим и адаптируем к каждой конкретной ситуации. Надеюсь, наши открытия, наблюдения и выводы будут полезны для вас. Главное, идите до конца, не отчаивайтесь и не опускайте руки даже тогда, когда все идет плохо. Если вы верите в свой проект и вкладываетесь в его развитие, то обязательно найдутся единомышленники и настоящие партнеры. 
Удачи!
User
=====
Магически исчезающий JS фреймворк
2017-12-19, 14:10
Программист
=====
Эксплуатация зданий: что будет, если один раз подойти с умом
2017-12-19, 09:58
Эффект использования
Ожидаемое снижение затрат
Снижение энергопотребления, оптимизация коммунальных сервисов
Благодаря выявлению и анализу наиболее затратных потребителей, случаев неэкономного использования энергии.
2–10%
Снижение затрат на энергопотребление, коммунальные сервисы через управление пиковыми нагрузками
Управление энергозатратными потребителями во избежание излишних пиковых нагрузок.
5–20%
Подтверждение, документирование энергопотребления
Минимизация затрат на создание отчётов по потреблениям благодаря использованию данных по объектам и счётчикам из системы.
50–90%
Снижение затрат на документирование
Благодаря быстрому сведению информации, использованию шаблонов, прямому доступу отчётов к фактическим данным.
30–70%
Снижение затрат на страховые выплаты
Благодаря предъявлению юридически значимых документов возможно снижение страховых выплат по рискам в области эксплуатационной ответственности.
5–15%
Снижение затрат на поиск, повышение качества информации
Снижение затрат на поиск и предоставление актуальной и корректной информации, снижение проблем с недостаточной и ошибочной информацией.
30–70%
Финансовые расчёты
Снижение временных затрат на внутр. фин. расчёты, например, благодаря непосредственному учёту персонала по заданиям, работам.
50–90%
Доступность оборудования
Снижение выходов из строя оборудования и конструкций благодаря автоматизированному контролю сроков эксплуатации.
1–10%
Плановое обслуживание
Снижение затрат на плановое обслуживание и ремонты за счёт эффективного планирования и подготовки.
10–30%
Контроль гарантийных обязательств
Снижение затрат на ремонты за счёт эффективного контроля гарантийных обязательств.
1–5% (от инвестиц. затрат)
Распределение нарядов/заданий
Снижение затрат на сервисные работы благодаря сводному централизованному учёту и распределению корректных нарядов/заданий (напр., технич. обслуж., уборка).
10–30%
Страховые договоры
Снижение общих затрат на страхование благодаря выявлению случаев отсутствующего или дублированного страхования.
1–10%
Регистрация заявок
Снижение затрат на учёт, диспетчеризацию заявок/заданий.
40–80%
Инвентаризация
Снижение временных затрат за счёт применения технологий штрих-кодирования, RFID.
50–90%
Экспертная оценка стоимости
Снижение ежегодных затрат благодаря автоматизации подготовки необходимых документов из системы.
50–80%
Обработка заявок/заданий
Снижение административных, управленческих затрат на диспетчеризацию заданий, контроль выполнения, минимизация ошибок по интерпретации заявок/заданий.
40–80%
Системный инженер
=====
Моделирование активов предприятия при помощи проекционного моделирования
2017-12-19, 09:15
Пользователь
=====
Как ускорить процесс выбора
2017-12-19, 10:29
Пользователь
=====
Блеск и нищета джавовых веб-фреймворков
2017-12-19, 13:10
Привет, Хабр! Помоги выбрать веб-фреймворк? Требования: модный, молодежный, популярный, качественный фреймворк для соло-технономада.
Надо ли нам каждый месяц читать очередной пост про это?
Несколько лет участия в проектах на границе энтерпрайза и системщины окончательно отбили нюх. Чтобы разобраться в вопросе, я заглянул в топ гугла и обнаружил там кучу однобоких рейтингов. Наверное, самым лучшим оказался Java Web Frameworks Index от ZeroTurnaround. 
Хорош он тем, что 
Вот как рейтинг выглядит на момент написания статьи:
Стойте, там Struts в первой десятке? Серьезно? Кажется, я ничего не потерял за эти несколько лет. Точнее, даже начиная с раннего средневековья.
Давайте пробежимся по списку. 
Оу, Spring MVC и Spring Boot — это два разных элемента списка? Наверное, это можно понять и простить? (напишите в комментариях!). Не имеет смысла спрашивать, при чем тут Spring — он, как и Docker, всегда при чем.
Кстати, к нам на JPoint 2018 Moscow собрался Юрген Хеллер — это главный спринговец. Вот его можно дрючить вопросами типа "при чём тут спринг" по полной программе.
Но что действительно страшно, это то, что между ними (то есть по сути, на первом месте) находится JSF. Когда-то я делал на ЛОРе несколько обсуждений на тему, какой шаблонизатор для Java лучший. Годы шли, но всегда находилась половина треда с универсальным ответом: зачем тебе шаблонизатор, когда есть JSF? Вначале был просто JSP/JSTL, но потом они потихоньку сдали позиции, и остался один JSF.
Давайте глянем, что есть нового в JSF. Да, теперь мы можем больше не писать FacesContext facesContext = FacesContext.getCurrentInstance();. Можно сделать @Inject FacesContext facesContext;. Или если ты EL-камикадзе, то можно даже #facesContext. В нужных местах можно навешать @FlowMap или достать настроечку через @ManagedProperty ("#{bean.property}") private String stringProperty; Имхо, всё это совершенно очевидные рефакторинги, в соответствии с текущей модой на синтаксис. То же касается валидации в форме <f:convertDateTime type="localDate" pattern="MM/dd/yyyy"/> — ну запилили в Восьмерке Date-Time API, пришлось отреагировать, чтобы люди не писали бесконечных конвертеров самостоятельно. Список можно продолжить. Так и представляешь, как архитекторы Oracle пилили эти фичи за один вечер, батон колбасы и бутылку водки.
Интересная фича — это тэг <f:websocket>, который можно юзать вот так:
В целом, прогресс с 2009 года (наш эквивалент «XV века») не перестает поражать воображение.
Дальше по рейтингу — Grails и PlayFramework. Grails — это, строго говоря, вообще не Java, а JVM. C PlayFramework под Java API не встречался со времен Play 1, поэтому — можете рассказать об этом в комментариях? Пока условно будем считать PlayFramework вторым годным фреймворком из списка, просто по причине наличия чудесного Scala API (за который можно простить ему историю с ORM и прочие мелкие ляпы).
Grails. Ну, допустим. Закроем глаза, тем более что Барух обещал, что Groovy — это круто. Но у них до сих пор открыты тикеты против Java 9! Ничего личного, чуваки, но это никуда не годится. В самом Groovy тоже какая-то фигня творится с поддержкой модулей и Java 9: насколько понял, --add-opens=java.base/* с нами навечно.
Wicket, Vaadin и GWT хотелось бы выделить в отдельную группу. С Vaadin и GWT я встречался только в смысле правки багов в чужих проектах. Но с Wicket у меня давний и болезненный опыт. Не знаю, кто первый притащил Wicket в Новосибирск, но он как эпидемия прошелся по нашим Java-компаниям. Мы писали на Wicket систему для управления профсоюзами в США. Мы писали мобильную MMO-игру. И для российских государственных компаний тоже писали разное, так что если заходите вылечиться от насморка в соседнюю больницу — осторожней, возможно, там в компьютерах полный неоперабельный Wicket. Каждый раз меня не оставляло ощущение, что Wicket не нужен вообще никогда и нигде. Может быть, про это стоит написать отдельную статью или даже целую книгу?
Давайте посмотрим еще раз на стартовую картинку. (Не знаю, кто настоящий автор, я ее нагуглил вот здесь). 

Wicket появился в том же году, что и термин AJAX. В свою очередь, AJAX спас веб, благодаря этому мы все с вами такие богатые и знаменитые, хе-хе. В свою очередь, Wicket появился как средство управления Аяксом и как эксперимент был очень удачным. Потом он пошел в продакшн, и это история, полная боли и фейлов. С точки зрения архитектуры, он так никогда и не стал кластерным, и в некоторых компаниях стал причиной полного отсутствия горизонтального масштабирования. С перфомансом у него очень плохо — просто гляньте, сколько он весит в памяти и как медленно отвечает на запросы. Ну или просто откройте wicket.apache.org и посчитайте, за сколько загрузится страница. 
С AJAX у него тоже так и не вышло: в 2017 году нас все еще преследуют оптимизации в названиях параметров. Пожалуйста, поднимите руки все, кто с первого раза догадается о назначении следующих параметров запроса: m, mp, e, f, sc, dt, wr, ch, bh, pre, bsh, ah, sh, fh, coh, ep, dep, rt, ad, sp, tr. 
Ответы на задачку находятся здесь. Спойлер: АД расшифровывается как «allow default». Это булевский флаг, который показывает, разрешать ли исполнение поведения по умолчанию для того элемента HTML, который слушает данное событие.
Да, многие из нас работают в банках, и там всё на GWT. Но, оглядываясь назад на этот долгий-долгий путь, давайте честно признаем: управлять JavaScript из Java — наиболее дурацкая и деструктивная идея, которая когда-либо приходила в голову. 
Если посмотреть на репозиторий Wicket, становится очевидно, что в период с 2007 по середину 2010 он был скорее мертв, чем жив, и далее возродился силами всего одного пользователя GitHub — Martin Grigorov, который сделал туда около четырех тысяч коммитов.

Картинка хороша, но давайте обратим внимание на конкретные циферки.
git clone https://github.com/apache/wicket.git
cd ./wicket
И теперь долбанём адским однострочником:
Или можно получить еще более подробную кумулятивную статистику:
Более подробная статистика займет минут 30 (на SSD, на новеньком макбуке с мобильным i7).
В любом случае, мы увидим, что Wicket, по сути, разрабатывается не более чем десятью людьми. Если кто-то из этих десяти человек навернется (особенно Мартин), то вашему свободному времени настанет неминуемый капец — придется вечерами и ночами сидеть и осознавать баги в рендеринге страницы.
Думаю, пора заканчивать избиение младенцев и сделать какой-то вывод.
Совсем недавно было такое время, когда мы возмущались «программистами на фреймворках». «Как же так, — говорили мы на собеседовании, — ты умеешь использовать Spring, но понятия не имеешь, как работает изнутри HashMap! Что за дичь!» В еще больший ужас мы приходили, когда человек начинал рассказывать о десятках различных фреймворков, ни один из которых он не знал даже приблизительно, но все успешно применял на практике. Совсем ужасно, когда человек сам написал пять веб-фреймворков и даже в них не разбирался! 
Ну что ж, если долго жечь поляну, то в конце концов травы на ней не останется. В результате своих возмущений мы получили ситуацию, когда JavaScript-проекты растут как грибы после дождика, а вот создание новых фреймворков для Java оказалось не такой уж популярной задачей. Теперь вы легко найдете того самого Senior Framework Coder (если этот фреймворк — Spring Boot, JSF и Play), который назубок расскажет про устройство табличных компонентов и внутреннюю кухню хэшмапы, но вряд ли сможет написать пять своих фреймворков и десять вариантов хэшмапы. И ни одного такого, кто сможет написать нечто лучшее, чем Spring Boot.
Возможно, вот прямо сейчас стоит остановиться и начать раздувать большущий такой фреймворк-хайп. Что думаете?
И возвращаясь к стартовому вопросу. Эй, читатель! Помоги выбрать веб-фреймворк? Требования: модный, молодежный, популярный, качественный фреймворк, и чтобы им кто-то действительно пользовался в проде, а не как Vert.x.
UPD: если кто-то из новосибирцев пишет на Wicket и не согласен с написанным выше, предлагаю встретиться на JBreak и перетереть за всю хурму. Как раз будет несколько месяцев, чтобы подготовиться к защите любимого фреймворка — готовьтесь тщательней :-)
кибер-ниндзя
=====
СберШифт: пять раз нажимай и в систему попадай
2017-12-19, 11:26
Разработчик UI5
=====
Меняем PID процесса в Linux с помощью модуля ядра
2017-12-19, 16:48
C++ разработчик
=====
IP-АТС Zeon. Настройка интеграции Битрикс24
2017-12-19, 12:36
Компания «АйПиТелефон»
=====
Как сделать внутренний продукт внешним. Опыт команды Яндекс.Трекера
2017-12-21, 15:10
Недавно мы открыли для внешних пользователей Яндекс.Трекер – нашу систему управления задачами и процессами. В Яндексе его используют не только для создания сервисов, но даже для закупки печенья на кухни. 
Как известно, чем меньше компания, тем более простые инструменты она может использовать. Если с утра вы можете поздороваться с каждым сотрудником лично, то вам хватит для работы даже чата в Telegram. Когда появляются отдельные команды, не только поприветствовать каждого лично не получится, но и в статусах задач можно запутаться. 

Облако из слов в заголовках тикетов во внутреннем Яндекс.Трекере
На таком этапе важно сохранять прозрачность процессов: все стороны должны иметь возможность в любой момент узнать о ходе работы над задачей или, например, оставить свой комментарий, который не пропадёт в потоке рабочего чата. Для небольших команд трекер – это и вовсе своего рода новостная лента с последними новостями из жизни их компании.
Сегодня мы расскажем читателям Хабрахабра, почему Яндекс решил создать свой трекер, как он устроен внутри, и с какими сложностями нам пришлось столкнуться, открывая его наружу.
В Яндексе сейчас работает больше шести тысяч человек. Несмотря на то, что многие его части устроены как независимые стартапы со своими командами разного размера, необходимость понимать, что происходит у людей на соседнем этаже всегда есть – их работа может пересекаться с вашей, их улучшения могут помочь вам, а какие-то процессы наоборот могут негативно повлиять на ваши. В такой ситуации сложно, например, призывать коллегу из другого рабочего пространства в Slack. Особенно, когда прозрачность задачи важна для множества людей из разных направлений. 
В какой-то момент мы начали использовать известную всем Джиру. Это хороший инструмент, функциональность которого в принципе всех устраивала, но его было сложно интегрировать с нашими внутренними сервисами. Кроме того, на масштабе в тысячи человек, которым нужно единое пространство, где каждый сможет сориентироваться без фонарика, Джиры переставало хватать. Случалось и такое, что она ложилась под нагрузкой, даже при том, что работала на наших серверах. Яндекс рос, количество тикетов также увеличивалось, а обновления на новые версии занимали всё больше времени (последний апгрейд занял полгода). Нужно было что-то менять.
В конце 2011 года у нас было несколько вариантов решения проблемы:
Разработка собственного трекера началась в январе 2012 года. Первой свои задачи в новый сервис перевезла сама команда трекера через несколько месяцев после начала работы над проектом. Дальше начался процесс переезда остальных команд. Каждая команда выдвигала свои требования к функциональности, их прорабатывали, трекер обрастал новыми фичами, затем перевозили команду. На полный переезд всех команд и закрытие Джиры понадобилось два года.
Но давайте вернемся немного назад и посмотрим на список требований, который был составлен для нового сервиса:
А ещё в момент сбора требований мы определились с теми технологиями, которые будем использовать для создания трекера:
Как и для любых других публичных и внутренних сервисов Яндекса, нам пришлось также задуматься о требованиях к запасу производительности и масштабируемости сервиса. Пример для понимания ситуации. На момент начала проектирования системы у нас было порядка 1 млн задач и 3 тыс. пользователей. На сегодняшний день в сервисе почти 9 млн задач и более 6 тыс. пользователей.
Кстати, несмотря на довольно приличное количество пользователей во внутреннем Трекере, бОльшая часть запросов приходит в трекер, через API от сервисов Яндекса, интегрированных с ним. Именно они и создают основную нагрузку:

Ниже можно увидеть перцентили ответов в середине рабочего дня:

Мы стараемся регулярно оценивать будущую нагрузку на сервис. Строим прогноз на 1-2 года, затем с помощью Лунапарка проверяем, что сервис ее выдержит:

На этом графике видно, что API поиска задач начинает отдавать заметное число ошибок лишь после 500-600 rps. Это позволило оценить, что с учетом роста нагрузки от внутренних клиентов и роста количества данных мы выдержим нагрузку через 2 года.
Кроме высокой нагрузки с сервисом могут случаться и другие неприятные истории, с которыми надо уметь обращаться так, чтобы пользователи этого не замечали. Перечислим некоторые из них. 
Отказ датацентра.
Весьма неприятная ситуация, которая тем не менее регулярно происходит благодаря учениям. Что происходит при этом? Худший случай, это когда мастер монги был в отключенном ДЦ. Но даже в этом случае не требуется вмешательство разработчика или админа благодаря автоматическому failover. В эластике ситуация немного иная: часть данных оказалась в единственном экземпляре т.к. фактор репликации у нас 1. Поэтому он создает новые шарды на уцелевших нодах, чтобы у всех шардов снова была резервная копия. Тем временем балансер над бекендом получает таймаут соединений в тех запросах, которые выполнялись на инстансах в отключенном ДЦ, либо ошибку от работающего бекенда, чей запрос ушел в пропавший ДЦ и не вернулся. В зависимости от обстоятельств, балансер может попытаться повторить запрос или вернуть ошибку пользователю. Но в итоге балансер поймет, что инстансы из отключенного ДЦ ему недоступны и перестанет отправлять туда запросы, проверяя в фоне, не заработал ли все-таки ДЦ и не пора ли возвращать туда нагрузку.
Потеря связности между бекендом и базой/индексом из-за проблем с сетью.
Чуть более простая ситуация на первый взгляд. Так как балансер над бекендом регулярно проверяет его состояние, то ситуация, когда бекенд не может достучаться до базы всплывает весьма быстро. И балансер опять уводит нагрузку с этого бекенда. Есть опасность, что если все бекенды потеряют связь с базой, то их всех же закроют, что в итоге повлияет на 100% запросов.
Высокая нагрузка запросов в поиск трекера.
Поиск по задачам, их фильтрация, сортировка и агрегация – весьма трудозатратные операции. Поэтому именно эта часть API имеет самые строгие лимиты по нагрузке. Раньше мы находили вручную тех, кто заваливал нас запросами и просили их сбавить нагрузку. Сейчас такое происходит всё чаще, поэтому включение rate limits позволило не замечать излишне активного клиента API.
Нашими сервисом не раз интересовались другие компании – они узнавали о нашем внутреннем инструменте от тех, кто покидал Яндекс, но не мог забыть Трекер. И вот в прошлом году внутри мы решили готовить Трекер к выходу в мир – делать из него продукт для других компаний.
Мы сразу начали прорабатывать архитектуру. Перед нами встала большая задача по масштабированию сервиса до сотен тысяч организаций. До этого сервис годами разрабатывался для одной нашей компании, учитывал только её потребности и нюансы. Стало ясно, что текущая архитектура потребует сильных доработок.
В итоге у нас было два варианта решения. 
Очевидно, что стабильность сервиса для внешних пользователей не менее важна, чем для внутренних, поэтому необходимо дублировать базы, поиск, бэкенд и фронтенд в нескольких датацентрах. Это делало первый вариант гораздо более сложным в обслуживании – получалось много точек отказа. Поэтому конечным вариантом мы выбрали второй. 
Переписывание основной части проекта у нас заняло два месяца, для такой задачи это были рекордные сроки. Тем не менее, чтобы не ждать, мы подняли несколько копий трекера на выделенном железе, чтобы было на чём тестировать фронтенд и взаимодействие со смежными сервисами.
Отдельно стоит отметить, что еще на этапе проектирования, мы приняли принципиальное решение сохранить одну кодовую базу для обоих Трекеров: внутреннего и внешнего. Это позволяет не заниматься копированием кода из одного проекта в другой, не снижать скорость релизов и выпускать возможности наружу почти сразу после их появления в нашем внутреннем Трекере.
Но как выяснилось, мало было добавить ещё один параметр во все методы приложения, мы также столкнулись со следующими проблемами:
Отдельный момент – оценка производительности. Из-за множества переделок необходимо было оценить скорость работы, количество организаций, которое бы вместилось в инстанс, а также поддерживаемый rps. Поэтому мы провели очередные стрельбы, предварительно заселив в наш тестовый трекер большое количество организаций. По итогам определили границу нагрузки, после которой новые организации надо будет размещать в новом инстансе. 
Еще один специальный выделенный инстанс Трекера мы сделали, чтобы разместить в нем демоверсию. Чтобы попасть в нее достаточно иметь просто аккаунт на Яндексе. В ней заблокированы некоторые возможности (например, загрузка файлов), но зато можно познакомиться с настоящим интерфейсом Трекера.
Разработчик
=====
Как успешно научить себя программировать
2017-12-19, 14:12
Software Engineer
=====
Рождение сверхновой: как появляются новые функции на примере 3D-подсчета посетителей
2017-12-19, 13:06
Пользователь
=====
Микромодульные центры обработки данных — решение для небольших VPS / VDS провайдеров
2017-12-19, 12:59
Пользователь
=====
Фокус-группы для исследования пользователей: впечатления участника, критика и адаптация метода
2017-12-19, 13:19
Пользователь
=====
Эксфильтрация в Metasploit: DNS туннель для Meterpreter
2017-12-19, 15:42
User
=====
Еще одна кража через SWIFT. Теперь в России
2017-12-19, 13:43
Информационная безопасность
=====
Топ-10 библиотек для React на GitHub
2017-12-19, 14:51
Пользователь
=====
Поле боя — дополненная реальность. Часть III: возможности движка, анимация и POI
2017-12-19, 14:19
Software developer
=====
Первый взгляд на RPG: оказывается, это не только ролевые игры
2017-12-19, 14:16










Программист
=====
Как мы преодолели железные препятствия при автоматизации тестирования
2017-12-19, 14:45
User
=====
На волне майнинга. Новый вирус распространяемый через Facebook
2017-12-19, 15:15
Пользователь
=====
Хорошие новости для веб-мастера: Hamster Marketplace разместил оффер на площадке RunCPA
2017-12-19, 15:25
Корпоративный аккаунт
=====
Как не утонуть в лендингах: история создания японского CarPrice
2017-12-20, 11:30
Пользователь
=====
Key-value для хранения метаданных в СХД. Тестируем встраиваемые базы данных
2017-12-19, 15:51
User
=====
ИТ-прогнозы 2018 года: 8 инфраструктурных трендов
2017-12-19, 15:56
Сетевые технологии и оборудование, гаджеты
=====
Клятва Гиппократа или как защищать информацию в медицинских учреждениях
2017-12-19, 16:06
Студент
=====
Как анализировать тональность твитов с помощью машинного обучения на PHP
2017-12-19, 16:48
Редактор
=====
Voxxed Days Minsk
2017-12-19, 16:50
User
=====
Быстрая и безопасная ОС для web-серфинга с неприступным носителем, легко изменяемым пользователем
2017-12-19, 16:59
Химик и программист.
=====
Гиппократ и IT. Что же между ними общего?
2017-12-19, 17:45
Пользователь
=====
9 учебных проектов для бэкендера
2017-12-20, 00:04
Человек и пароход
=====
SecurityWeek 50: хактивист устал и мухожук, фальшивый криптокошелек для любителей панд, двуликий Янус под Android
2017-12-19, 18:10
Пользователь
=====
Mastering Angular Material Data Tables
2017-12-20, 22:45
User
=====
Больше чем Java?
2017-12-19, 18:40
Пользователь
=====
Исправлять ли unexpected behavior в C# 7 или оставить как есть, усложнив синтаксис языка для компенсации?
2017-12-19, 19:10
Пользователь
=====
Доделал игру, работающую на видеокарте
2017-12-19, 19:10
Пользователь
=====
Как предсказать курс рубля к доллару при помощи SAP Predictive Analytics
2017-12-21, 12:09
Пользователь
=====
Туториал по Unreal Engine. Часть 8: Системы частиц
2017-12-25, 13:48
Переводчик-фрилансер
=====
Эволюция жестких дисков: как изменились винчестеры за 60 лет существования?
2017-12-20, 00:09
Пользователь
=====
Лучшие практики CI/CD с Kubernetes и GitLab (обзор и видео доклада)
2017-12-22, 11:26
Пользователь
=====
Конкурс по криптоанализу в Аризонском Государственном Университете (интервью)
2017-12-19, 22:46
Блокчейн Эксперт
=====
Как Android запускает MainActivity
2017-12-20, 00:51
Android developer
=====
ИТ без капитальных затрат — это не оксиморон
2017-12-20, 07:56
Пользователь
=====
Мобильный UX-дизайн в 2018 году: тенденции и прогнозы
2017-12-20, 10:34
Редактор
=====
Как изменились интерфейсы торговых весов
2017-12-20, 12:49
Бизнес-решения для ритейла
=====
Windows сервера для задач 24x7 — миф или мои «кривые руки»?
2017-12-20, 10:35
Выгребатель костров
=====
Волшебные точки в облаках и что-то принципиально новое: почему каждый сотый житель РФ качает nanoCAD
2017-12-20, 10:50
САПРовец
=====
Операции над конструкциями
2017-12-20, 11:59
Пользователь
=====
ААА! Пришло время переписывать на .NET Coreǃ
2017-12-20, 13:10
Все мы давно хотим перелезть на .NET Core, но постоянно что-то мешает. Например, ничего не поделаешь, когда не хватает важных API. В версии 2.0 процесс упростили благодаря .NET Standard 2.0, но это ещё не всё. Ну что ж, Microsoft-боги вняли нашим молитвам и завезли 20 000 API, доступных в виде одного-единственного пакета в NuGet!

Вкратце, это нужно всем. Имхо, сама возможность перетащить свои тонны легаси на .NET Core — уже достаточное оправдание для любых жертв.
Скептикам же надо как бы напомнить, что .NET Core пилится специально для масштабируемых веб-приложений на всех разумных операционных системах (GNU/Linux, macOS и Windows), а более точно выбрать между Core и Framework поможет специальный документ.
Во-первых, надо понять, что залежи легаси сами собой не разгребутся. Нечего даже и думать о том, чтобы схватить в охапку миллион классов и перетащить их одной кнопкой.
Предположим, вы накодили приложуху на ASP.NET MVC для Windows-локалхоста, и вас за это не уволили. Настало время перетащить ее на правоверный Linux, работающий на Azure! Лучше есть слона по кусочкам, а именно:
Понятно, что к этому великому плану стоит подходить не как к строительству коммунизма, а разумно. Например, если нужно показать биг боссам запуск на Azure — с этого и стоит начать. Если же думать лень (мне, например, точно лень), наши лидеры написали специальную методичку по обретению веры в .NET Core.
Вкратце, нужно расчехлить NuGet, поставить пакет Microsoft.Windows.Compatibility и обнаружить, что стала доступна огромная куча разных нужных и ненужных API.
Важно понимать, что этот самый Microsoft.Windows.Compatibility всё еще яростно допиливается, так что все офигительные истории нам только предстоят. 
Сейчас имеется следующий набор ништяков (таблица получилась огромная; чувак, читающий этот пост с мобилы: прости меня пожалуйста!):
Для танкистов. Не все API одинаково переносимы. Если ты остаешься на Windows, проблем никаких нет. Если же хочешь приобщиться к святости Ричарда Столлмана и Тима Кука на GNU/Linux и macOS соответственно, придется страдать.
Взглянув на табличку ништяков, видим: Windows-only компонентов там чуть ли не половина. Добрые Microsoft-боги, тем не менее, позволяют успешно компилировать такой код под любой платформой. При попытке использования несуществующей фичи мы наткнемся на PlatformNotSupportedException в рантайме, поэтому все такие фичи нужно будет густо обмазать вызовами RuntimeInformation.IsOSPlatform():
Как же понять, какая API-шка будет работать только в Windows? Документацию ведь никто не читает, верно?
Дорогой мой любитель программирования копипастой со StackOverflow! Microsoft — боги суровые, но справедливые, поэтому буквально пару недель назад они запилили API Analyzer tool. С помощью Roslyn эта тула помечает Windows-only API, причем только тогда, когда целью выставлен .NET Core или .NET Standard.

Что делать с ошибками? Как было сказано ранее, страдать.
В примере с картинки кто-то пытается вычитать настроечку в Реестре. Можно обернуть эту строчку в проверку, выполнять ее только на Windows. В GNU/Linux эквивалент этой строчки будет другим, куда более мучительным.
Заметьте, что Windows Compatibility Pack — это метапакет. В Microsoft отлично понимали, что обычные люди не будут мучиться с выискиванием отдельных мелких пакетиков и захотят перейти на новую платформу одним прыжком (с учетом оговорок выше). Тем не менее, если ты — вдумчивый крутой разработчик, то можно притаскивать фичи и поодиночке. Таким образом, можно будет выкинуть куда больше зависимостей на ненужный хлам.
Морально готовишься к портированию. Устанавливаешь Windows Compatibility Pack. Изучаешь ништяки, коих там более 20 тысяч, включая EventLog, WMI, Performance Counters, Windows Services итп.
Если хочешь сделать приложуху кроссплатформенной — запускаешь API Analyzer. Можно предварительно немного поплакать или тяпнуть водочки.
К сожалению, сейчас .NET Core не покрывает нужды десктопной разработки. Возможно, когда-нибудь это изменится. Но это уже совсем другая история. 
Если же хочется больше узнать о .NET вообще и .NET Core в частности, ждем тебя на DotNext 2018 Piter, которая пройдет, внезапно, в Питере. Очень советую к этой конфе уже попробовать что-нибудь запилить или портировать на Core, чтобы накопился пул вопросов к спикерам.
кибер-ниндзя
=====
Ферхюльстом по биткойну
2017-12-21, 20:50
Пользователь
=====
От 15 и больше: как обеспечить масштабируемость CI
2017-12-21, 10:52
Ops Manager
=====
Почему Agile не работает и что с этим делать
2017-12-20, 14:55
Пользователь
=====
Обновление ITIL: что нового появится в 2018 году
2017-12-24, 19:13
Пользователь
=====
Сертификация CompTIA Network+
2017-12-20, 13:28
Консультант по ИБ и процессам
=====
Как за 10 минут сделать клиент к HTTP API на Swagger
2017-12-25, 12:27
Программист, Технический писатель, DevRel
=====
Создание блокчейн-приложения для страховой компании с помощью Hyperledger Fabric от IBM
2017-12-20, 13:30
Разработчик
=====
PVS-Studio и ГОСТы. Как появилось приложение КОМПАС-Эксперт для проверки чертежей
2017-12-23, 11:17
Отечественная САПР-система
=====
Как устроены хранилища данных: обзор для новичков
2017-12-21, 14:37
Пользователь
=====
История программиста, создавшего компанию «Maxilect», на 100% работающую удаленно
2017-12-20, 16:47
Marketing manager
=====
Простая объектная СУБД
2017-12-20, 14:59
Пользователь
=====
Выиграй поездку в норвежский офис Vivaldi
2017-12-20, 15:07
Зрю в корень, жгу глаголом
=====
Пасём биконов тучные стада
2017-12-23, 08:23

Так и сколько же их расплодилось! Где ни включишь снифер — толпа, стадо. Но можно сказать, что потерянное стадо. Как их найти? Что с ними делать? Как поставить их на службу человеку?
Биконы — спящий вулкан. В эфире их очень много, а по делу используется пока малая толика. Каждое устройство Bluetooth Low Energy является биконом, пока не соединится с другим.
Показанные ниже радио-технические извращения, возможно, помогут спасти стада заблудших биконов. Или хотя бы доставят вам легкое технологическое удовольствие. 
Причем удовольствие в модном нынче стиле AR — дополненной реальности.
Обо всем по порядку. Биконы придумала Apple, чтобы завладеть миром. После нее биконы придумали все остальные. Каждый придумал свои, поэтому они слабо совместимы друг с другом. Основное новшество биконов — работа без соединения и идентификации. Это, во-первых, делает возможным принимать их сразу всем, и, во-вторых, экономит энергию и, самое главное, делает энергетический ресурс биконов прогнозируемым. Соответственно, маленькие размеры и долгий срок службы.
На Хабре об этих биконах писали много: здесь, здесь и здесь. И еще, наверное, много где. Не буду повторяться.
Для меня, как для пеленгаторщика, эти свойства стали лучшим подарков. А ну как сделать такую штуку, чтобы пеленговала этих радио-блох!
Сначала, правда, появился пеленгатор WiFi. Антенна в нем точно такая, как нужна для биконов, частоты совпадают. И каждая точка доступа WiFi по сути всегда являлась биконом, но в стандарте WiFi, конечно. Кому интересны технические подробности, можете почитать здесь. Основная фишка здесь в антенне. А точнее в антеннах, их там две. И по разнице между ними идет пеленгование.

В пеленгаторе WiFi нужно было только заменить наши любимые ESP8266 на не менее любимые nRF51822, и он стал для биконов. После легкой правки мобильный софт стал показывать все те же картинки, что и на WiFi.
Теперь нужно было все испытать на "земле". Массово и однообразно биконы используются в indoor-позиционировании. Как я и думал, настоящую систему indoor-навигации найти даже в большом Питере не так-то легко. (Хотя может я плохо искал, подскажите, пожалуйста) Мне пришлось ехать с севера Питера на юг, в ТРК РИО. Он обрадовал своей простотой, там всего один этаж. Биконы висят на колоннах коридора и хорошо видны.

Снифер показал наличие большого количества биконов фирмы Kontakt.io. Это один из первых производителей биконов. 

Расчехляем орудие радио-технического удовольствия:

И идем пасти биконов. Находим первый попавшийся, местоопределяем в дополненной реальности, записываем его в Firebase-базу данных биконов под логичным именем.
Потом следующий.
И закрепляем упражнения.
Можно было бы сделать процесс учета биконов более автоматическим. Можно было бы даже запоминать 3D-сцену благодаря встроенной в гаджет технологии Google Tango. Но надо же оставить какое-то удовольствие на будущее.
Исходники мобильного софта можно посмотреть на Github.
радио-инженер
=====
Node.js и JavaScript для серверной разработки
2017-12-20, 15:22
Пользователь
=====
Диагностика и понимание поведения графического процессора вашего приложения с помощью GAPID
2017-12-20, 15:24
User
=====
Как скомпрометировать систему документооборота в несколько кликов
2017-12-20, 15:51
На пороге уже стоит 2018 год. Но большинство бородатых уязвимостей продолжает жить в разрабатываемых системах. И не смотря на то что появился OWASP Top-10 2017. И приоритетность определенных вещей сильно поменялась. По прежнему ничего не мешает натыкаться на ситуации, которые были актуальны в 2010.

История началась с банального любопытства к продукту компании, в которой работает мой знакомый. Продукт интересный. Покупают данный продукт очень вдумчиво и за ценники с 6 знаками. Баг-баунти официальной у этой компании нет. Но я подумал, что даже если что-то найду — через знакомого разрулю и передам.
Найти основной продукт компании не составило труда. Есть официальный сайт с демо-приложением. Полез туда смотреть что там есть, попутно обернув весь трафик через прокси в Burp Suit.
Увидев input field, мне сразу же захотелось воткнуть туда xss. Воткнул банальный вектор 
И что вы думаете произошло? Конечно, сработало!

Я поискал еще возможные поля ввода информации, и абсолютно везде выполнялась моя XSS атака. А нет, вру! В одном месте почему-то не сработало. Но, потратив немного времени, я его смог одолеть с другим payload вектором и обходом экранирования. Итак, на руках есть как минимум 8 xss векторов. Можно было это число увеличить в разы, но было очень лениво тратить время на обход функциональности системы. Да и видно было, что проблема комплексная.
Уже хотел пойти искать, через знакомого, ответственных за это безобразие. Но подумал, что если все так плохо с экранированием, то возможно есть еще что-то. Отложил возможный контакт на попозже и полез еще смотреть.
В логах были намеки подозревать приложение в возможности реализовать CSRF атаку. Т.е не было никаких CSRF токенов, которые предотвращают возможность подобной атаки. Чтобы подтвердить мои опасения, нужен был реальный пример. А чтобы к моим замечаниям отнеслись максимально ответственно, нужен был пример, который показывает максимальный урон системе.
Пару слов о CSRF, хоть вы и могли бы загуглить это. 
Представьте что вы зашли в веб-клиент своего банка. И тут вам вконтактике присылают ссылочку на "смешных котиков". Вы отложили в сторону номер карты родственника, которому хотели только что перевести деньги, и пошли поглядеть на котиков. Котики были действительно смешные. И после вы решили все же перевести родственнику немного деньжат. Вернувшись в соседнюю вкладку веб-клиента банка, вы пытаетесь сделать перевод, но вот денег у вас уже недостаточно. И вы в жизни не догадаетесь, что обнесли ваш банковский аккаунт котики-бандиты. Эти котики прикидывались смешными, пока их сородичи переводили деньги вашего счета продавцам кошачьего корма и валерьянки.
Это топорный пример. И, безусловно, подобное ни в одном банке уже провернуть не получится. Но этот пример должен вам помочь понять всю серьезность уязвимости CSRF.
А теперь без котиков и с подробностями.
В исследуемом приложении есть простая возможность сделать logout. Можно проверить разлогинивание пользователей через csrf. Вы скажете: что тут такого, это не опасно! Но это и не вектор атаки. Это лишь проверка нашей теории о том, что наша идея работает.
Для этого нам потребовалось смастерить в нашей подсети тестовый сайтик следующего содержания.
Дальше выполняем вход в систему с другого браузера. Открываем в этом браузере тестируемое приложение с активной логин-сессией, а во второй вкладке наш сайтик. Нажимаем на нашу кнопку submit. Вернувшись в предыдущую вкладку с нашим тестовы приложением, мы обнаружим, что уже не залогинены.
Раз наш пример сработал, значит с вероятность 80% сработает и в других местах. 20% я оставил на то, что разработчики в важных местах используют средства защиты от CSRF атаки. Но такое бывает редко. Или CSRF токены есть везде, или их нет вообще.
Т.к наш демо-сайт с приложением позволял залогиниться от администратора системы, я мог посмотреть что может администратор, и, главное, как он это делает. Интересной находкой был сброс пароля пользователю системы. Выглядел он примерно так
И если вас не смутило, что для смены пароля вам не нужно вводить старый пароль, то определенно вас должно смутить то, что пользователи передаются через ID. 
А дальше сценарий атаки примерно следующий:
Код страницы
Все, что нам останется — зайти под пользователем, которому администратор "случайно" установил новый пароль. По такому же примеру работает кейс с созданием нового пользователя в системе. Можно все так же скрыто провернуть и создать супер-пользователя! Под которым можно сидеть в системе и сливать кучу ценной инфы о компании — приказы, документы, финансовые сведения. 
Отдельно во всей этой истории мне хочется выделить реакцию на проблемы. И скорость их исправления.
На зарепорченные XSS отреагировали достаточно быстро. С ними вообще вопросов не было. А вот с CSRF я от ответственного человека стал получать следующие вопросы и возражения.
Только у меня в этот момент дернулся глаз? О какой защите через whitelisted IP может идти речь, если админ со своего компьютера все делает. Это фишка CSRF. Но дальше больше. Ответственный заявил следующее.
Речь идет о массовой смене паролей всем пользователям системы. Тут мне пришлось прям со скриншотами приходить и рассказывать, почему не стоит такую возможность вообще оставлять. 
Можно разом перебрать все id
Ну и остановиться на тех которые нам не интересны
И, по правде говоря, получать уточняющие вопросы на такие очевидные и всем известные уязвимости было печально.
После всех переписок, уточнений, кучу потраченного времени, я был уведомлен о том, что мне будет выплачено вознаграждение по специальной багбаунти специально для меня. При этом расчет этого вознаграждения по сей день остается для меня какой-то магией. Хотя мне была приведена формула, по которой все посчитали. 
Все XSS кроме одной посчитали дублями и просуммировали по понижающему коэффициенту. СSRF по другому коэффициенту. 
В итоге я получил 100 Euro.
Но хорошо, что вообще что-то получил! 
Из-за «интересной» политики поддержки пользователей, большинство клиентов так и останется без этих критических обновлений и исправлений. 
Просто потому, что у этих клиентов нет годового абонемента поддержки. Так что в какой-то крупной компании, использующей этот дорогой продукт, по сей день можно утащить куки или сделать что-то от лица пользователя, подсунув картинку с котиками. Список клиентов внушителен — банки, страховые, промышленные холдинги...
В каком то смысле 0day получилась (уязвимость не имеющая исправления).
На мой взгляд можно бы было поступить разумнее и благороднее. Microsoft вон все еще патчи выпускает для Windows XP. 
Раскрывать название компании и клиентов компании я не стал по морально-этическим соображениям. 
Ну и представители за репутацию переживают.
Таймлайн:
19 июля — были сделаны репорты
9 августа — подтверждены все репорты и начислено вознаграждение
28 августа — исправлены проблемы с фильтрацией XSS
14 ноября — реализовали исправление CSRF для клиентов у которых есть абонемент поддержки
QA Engineer
=====
Головная боль от RecyclerView.Adapter — выход есть
2017-12-20, 15:53
Пользователь
=====
Как я слил 1000$ в продвижение игры и что из этого получилось
2017-12-20, 17:47
Unity Developer
=====
DataTalks #8: изучение пользователей
2017-12-20, 16:28
User
=====
Оптимизация производительности UIKit
2017-12-21, 14:44
iOS разработчик
=====
Помогаем доставке еды: редизайн логотипа и разработка фирменного стиля
2017-12-21, 13:43
Предприниматель
=====
REST — это новый SOAP
2017-12-21, 15:07
Несколько лет назад я разрабатывал для одного большого телекома новую информационную систему. Нам приходилось взаимодействовать со всё нарастающим количеством веб-сервисов, открываемых более старыми системами или бизнес-партнёрами. Как вы понимаете, мы получили добрую порцию SOAP-ада. Заумные WSDL, несовместимые библиотеки, странные баги… Где только возможно мы старались продвинуть — и использовать — простые RPC-протоколы: XMLRPC или JSONRPC.
Наши первые серверы и клиенты, работавшие по этим протоколам, были очень простыми, со скромными возможностями и ненадёжными. Но мы постепенно их улучшали и через несколько сотен строк кода достигли желаемого: 
Теперь можно было надёжно подключаться к любому подобному API с помощью лишь нескольких строк кода. И также мы теперь могли с помощью нескольких декораторов и обновлений документов открывать любой набор функций для широкой аудитории, для серверов и браузеров.
А когда дошло до взаимодействия между разными приложениями (построенными на основе микросервисов), этим уже занимался наш сисадмин. С программной частью уже практически не было никаких неясностей.

Разработчик отдыхает после трудной получасовой интеграции RPC API
А затем появился REST.
REpresentational State Transfer — передача состояния представления.
Эта новая волна сотрясла основы межсервисного взаимодействия.
RPC умер, будущее было за RESTful: ресурсы живут на своих собственных URL, а манипулировать ими можно только по протоколу HTTP.
С тех пор каждый API, который нам приходилось выставлять или к которому мы обращались, превращался в новую трудность, если не сказать — в безумие.
Чтобы не описывать на пальцах, проиллюстрирую на примере. Вот маленький API, типы данных убраны для удобства чтения.
Просто добавьте правильно задокументированную иерархию исключений (InvalidParameterError, MissingParameterError, WorkflowError…) с подклассами для определения важных ситуаций (к примеру, AlreadyExistingUsernameError), и порядок.
В этом API легко разобраться, его легко использовать, он надёжен. Он поддерживается точной машиной состояний (state machine), но ограниченный набор доступных операций удерживает пользователей от необдуманных действий (вроде изменения даты создания аккаунта).
Ожидаемая длительность выставления этого API в качестве простого RPC-сервиса: несколько часов.
Так, а теперь в дело вступает RESTful.
Никаких стандартов, никаких точных спецификаций. Лишь невнятная «философия REST», бесконечные дебаты и множество дурацких костылей.
