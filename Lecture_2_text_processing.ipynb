{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Методы анализа текстов</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этапы анализа текстов\n",
    "\n",
    "При обработке текстов выделяют несколько этапов анализа.\n",
    "* Токенизация (графематический анализ) - выделение абзацев, предложений, токенов. Если абзацы в HTML выделяются довольно просто - по тегам &lt;p&gt;, то с выделением предложений и слов могут быть проблемы.\n",
    "`Г. Мурманск был основан 3 апреля 1915 г. ниже впадения р. Туломы в Кольский залив. Минимальные IP-адреса: 109.124.97.0 - 109.124.97.3.`\n",
    "* Морфологический анализ (стемминг, лемматизация) - определение начальной формы слова или его псевдопрефикса, грамматических параметров. Подробнее описан ниже.\n",
    "* Синтаксический анализ - определение связей между словами (деревья зависимостей) или синтаксически связанных групп слов (деревья составляющих). Первые больше подходят для русского языка, вторые - для английского.\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0d/Wearetryingtounderstandthedifference_%282%29.jpg\">\n",
    "    <a href=\"https://en.wikipedia.org/wiki/Dependency_grammar\">Деревья зависимостей</a>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Competing_sentence_diagrams.png/750px-Competing_sentence_diagrams.png\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Constituent_(linguistics)\">Деревья составляющих</a></center>\n",
    "\n",
    "* Семантический анализ - определение смысла слова и работа с ним (`за'мок` vs `замо'к`, `удаление ребра связанного графа`; не путать с `он видел их семью своими глазами` где имеет место грамматическая неоднозначность `семья`-`семь`). Последнее время чаще используются дистрибутивные модели языка, о которых мы поговорим на отдельном занятии.\n",
    "\n",
    "Задачей морфологического анализа является определение начальной формы слова, его части речи и грамматических параметров. В некоторых случаях от слова требуется только начальная форма, в других - только начальная форма и часть речи.<br>\n",
    "Существует два больших подхода к морфологическому анализу: <b>стемминг</b> и <b>поиск по словарю</b>. Для проведения стемминга оставляется справочник всех окончаний для данного языка. Для пришедшего слова проверяется его окончание и по нему делается прогноз начальной формы и части речи.<br>\n",
    "Например, мы создаем справочник, в котором записываем все окончания прилагательных: <i>-ому, -ему, -ой, -ая, -ий, -ый, ...</i> Теперь все слова, которые имеют такое окончание будут считаться прилагаельными: <i>синий, циклический, красного, больному</i>. Заодно прилагательными будут считаться причастия (<i>делающий, строившему</i>) и местоимения (<i>мой, твой, твоему</i>). Также не понятно что делать со словами, имеющими пустое окончание. Отдельную проблему составляют такие слова, как <i>стекло, больной, вина</i>, которые могут разбираться несколькими вариантами (это явление называется <b>омонимией</b>). Помимо этого, стеммер может просто откусывать окончания, оставляя лишь псевдооснову.<br>\n",
    "Большинство проблем здесь решается, но точность работы бессловарных стеммеров находится на уровне 80%. Чтобы повысить точность испольуют морфологический анализ со словарем. Разработчики составляют словарь слов, встретившихся в текстах (<a href=\"http://opencorpora.org/dict.php\">здесь</a> можно найти пример такого словаря). Теперь каждое слово будет искаться в словаре и не предсказываться, а выдаваться точно. Для слов, отсутствующих в словаре, может применяться предсказание, пообное работе стеммера.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Морфологический анализ\n",
    "\n",
    "Есть несколько наиболее распространенных библиотеки для морфологического анализа текстов на Python: pymorphy3, pymystem и nltk. Рассмотрим работу с ними.\n",
    "\n",
    "Библиотеки pymorphy основана на словаре [OpenCorpora](opencorpora.org/) и позволяет проводить анализ отдельных слов, то есть предварительно необходимо провести графематический анализ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3 # Морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты анализа отдельного слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)), Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)), Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))]\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy3.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform = morph.parse('стекло')  # Проведем анализ слова \"стекло\"...\n",
    "print(wordform)                 # ... и посмотрим на результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода, слово \"стекло\" может быть неодушевленным существительным среднего рода, единственного числа, именительного падежа `tag=OpencorporaTag('NOUN,inan,neut sing,nomn')`, аналогично, но в винительном падеже (`'NOUN,inan,neut sing,accs'`), и глаголом `'VERB,perf,intr neut,sing,past,indc'`. При этом в первой форме оно встречается в 75% случаев (<i>score=0.75</i>), во второй в 18,75% случаев (<i>score=0.1875</i>), а как глагол - лишь в 6,25% (<i>score=0.0625</i>). Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.690476"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpencorporaTag('NOUN,inan,neut sing,nomn')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy3 умеет синтезировать нужные нам формы слова. Для этого необходимо получить объект типа `Parse` для нужного слова, а затем вызвать функцию `inflect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='стёклам', tag=OpencorporaTag('NOUN,inan,neut plur,datv'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёклам', 157, 8),))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].inflect({'plur','datv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо этого можно получить вообще всю лексему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),)),\n",
       " Parse(word='стекла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стекла', 157, 1),)),\n",
       " Parse(word='стеклу', tag=OpencorporaTag('NOUN,inan,neut sing,datv'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стеклу', 157, 2),)),\n",
       " Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),)),\n",
       " Parse(word='стеклом', tag=OpencorporaTag('NOUN,inan,neut sing,ablt'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стеклом', 157, 4),)),\n",
       " Parse(word='стекле', tag=OpencorporaTag('NOUN,inan,neut sing,loct'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стекле', 157, 5),)),\n",
       " Parse(word='стёкла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёкла', 157, 6),)),\n",
       " Parse(word='стёкол', tag=OpencorporaTag('NOUN,inan,neut plur,gent'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёкол', 157, 7),)),\n",
       " Parse(word='стёклам', tag=OpencorporaTag('NOUN,inan,neut plur,datv'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёклам', 157, 8),)),\n",
       " Parse(word='стёкла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёкла', 157, 9),)),\n",
       " Parse(word='стёклами', tag=OpencorporaTag('NOUN,inan,neut plur,ablt'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёклами', 157, 10),)),\n",
       " Parse(word='стёклах', tag=OpencorporaTag('NOUN,inan,neut plur,loct'), normal_form='стекло', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стёклах', 157, 11),))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform[0].lexeme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на `methods_stack=((<DictionaryAnalyzer>, 'лось', 121, 0)` и `methods_stack=((<FakeDictionary>, 'варкалось', 224, 9)`. Наличие строки <FakeDictionary> говорито том, что слово было предсказано. Причем в первом случае оно предсказано как форма слова _лось_, к которому добавлена неизвестная приставка. Во втором случае - это совершенно незнакомое слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А ещё pymorphy умеет предсказывать незнакомые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='пырялись', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='пыряться', score=1.0, methods_stack=((FakeDictionary(), 'пырялись', 234, 10), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'ялись')))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordform = morph.parse('пырялись') \n",
    "wordform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам проводит графематический анализ и снимает омонимию. \n",
    "\n",
    "Функция `lemmatize` делит текст на слова и знаки препинания, а затем возвращает для них только начальную форму.\n",
    "\n",
    "Функция `analyze` возвращает не только начальную форму, но и всю информацию о слове, как это делал перед этим Pymorphy. \n",
    "\n",
    "Как видно из примера, делает он это не всегда корректно, но нам не придется думать о том, какое вариант разбора следует взять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3 # Еще один морфологический анализатор. При первом запуске грузит словари из Сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['этот', ' ', 'тип', ' ', 'становиться', ' ', 'есть', ' ', 'в', ' ', 'цех', '.', '\\n']\n",
      "[{'analysis': [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'эти'}, {'text': ' '}, {'analysis': [{'lex': 'тип', 'wt': 0.8700298642, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], 'text': 'типы'}, {'text': ' '}, {'analysis': [{'lex': 'становиться', 'wt': 0.9821285244, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'стали'}, {'text': ' '}, {'analysis': [{'lex': 'есть', 'wt': 0.0492236161, 'gr': 'V,несов,пе=инф'}], 'text': 'есть'}, {'text': ' '}, {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}, {'text': ' '}, {'analysis': [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], 'text': 'цеху'}, {'text': '.'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "mystem = pymystem3.Mystem()\n",
    "print(mystem.lemmatize('эти типы стали есть в цеху.'))\n",
    "print(mystem.analyze('эти типы стали есть в цеху.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы можно представить в таком виде.\n",
    "\n",
    "```\n",
    "['этот', \n",
    " ' ',\n",
    " 'тип',\n",
    " ' ',\n",
    " 'становиться',\n",
    " ' ',\n",
    " 'есть',\n",
    " ' ',\n",
    " 'в',\n",
    " ' ',\n",
    " 'цех',\n",
    " '.',\n",
    " '\\n']```\n",
    " \n",
    "```\n",
    "[\n",
    "  {'analysis': \n",
    "    [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], \n",
    "   'text': 'эти'\n",
    "  }, \n",
    "  {\n",
    "    'text': ' '\n",
    "  }, \n",
    "  {'analysis': \n",
    "    [{'lex': 'тип', 'wt': 0.8700298667, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], \n",
    "   'text': 'типы'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'становиться', 'wt': 0.9821285009, 'gr': 'V,нп=прош,мн,изъяв,сов'}], \n",
    "   'text': 'стали'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'есть', 'wt': 0.04922361672, 'gr': 'V,несов,пе=инф'}], \n",
    "   'text': 'есть'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'в', 'wt': 0.9999917746, 'gr': 'PR='}], \n",
    "   'text': 'в'}, \n",
    "  {'text': ' '}, \n",
    "  {'analysis': \n",
    "    [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], \n",
    "   'text': 'цеху'}, \n",
    "  {'text': '.'}, \n",
    "  {'text': '\\n'}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть результатом является список токенов (в том числе и пробельных или знаков препинания), для части из которых имеется результат анализа, который хранится в словаре с ключём `analysis`. Анализ хранит в списке один или несколько вариантов разбора, у каждого из которых есть лемма `lex`, набор грамматических параметров `gr` и некоторый вес `wt`, который показывает степень уверенности системы в правильности ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APRO\n"
     ]
    }
   ],
   "source": [
    "my_res=mystem.analyze('эти типы стали есть в цеху.')\n",
    "if 'analysis' in my_res[0].keys(): # Проверяем, что это не разделитель.\n",
    "    print(my_res[0]['analysis'][0]['gr'].split(\"=\")[0]) # Берем из него анализ, из того грамматическсие параметы, \n",
    "                                                        # а из них выделяем часть речи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна библиотека - NLTK. По сравнению с двумя предыдущими библиотеками она обладает более широкой функциональностью и изначально писалась для работы с разными языками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Иностранный морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом использования необходимо загрузить необходимые библиотеки или корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # По дороге будут появляться поле ввода. Грузит всё из Сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сразу скачать нужный пакет, если вы знаете как он назыввается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['averaged_perceptron_tagger_ru', 'stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `word_tokenize` возвращает начальные формы слов. \n",
    "\n",
    "Функция `pos_tag` возвращает список начальных форм и их частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Эти', 'типы', 'стали', 'есть', 'в', 'цеху'],\n",
       " [('Эти', 'типы'),\n",
       "  ('типы', 'стали'),\n",
       "  ('стали', 'есть'),\n",
       "  ('есть', 'в'),\n",
       "  ('в', 'цеху')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('Эти типы стали есть в цеху') # Токенизация.\n",
    "bi_tokens = list(nltk.bigrams(tokens))\n",
    "tokens, bi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Эти', 'JJ'),\n",
       "  ('типы', 'NNP'),\n",
       "  ('стали', 'NNP'),\n",
       "  ('есть', 'NNP'),\n",
       "  ('в', 'NNP'),\n",
       "  ('цеху', 'NN')],\n",
       " [(('Эти', 'JJ'), ('типы', 'NNP')),\n",
       "  (('типы', 'NNP'), ('стали', 'NNP')),\n",
       "  (('стали', 'NNP'), ('есть', 'NNP')),\n",
       "  (('есть', 'NNP'), ('в', 'NNP')),\n",
       "  (('в', 'NNP'), ('цеху', 'NN'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(tokens) # Частеречная разметка.\n",
    "bi_pos = list(nltk.bigrams(pos))\n",
    "pos, bi_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У NLTK заведен список стоп-слов, которые лучше фильтровать при анализе текстов. Но их не очень много. Зато самые мешающиеся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего русских стоп-слов 151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Эти', 'типы', 'стали', 'цеху']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставим только те слова, которых нет в списке стоп-слов.\n",
    "filtered_words = [token for token in tokens if token not in nltk.corpus.stopwords.words('russian')]\n",
    "print('всего русских стоп-слов', len(nltk.corpus.stopwords.words('russian')))\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ту же самую задачу в других библиотеках можно решить при помощи фильтра частей речи. Можно считать, что значимыми являются лишь существительные, прилагательные, глаголы, причастия и деепричастия. Ниже приведены названия частей речи для разных библиотек.\n",
    "<table>\n",
    "<tr><th>Часть речи</th><th>Pymorphy</th><th>Mystem</th><th>NLTK</th></tr>\n",
    "<tr><td>Существительное</td><td>NOUN</td><td>S</td><td>NN</td></tr>\n",
    "<tr><td>Прилагательное</td><td>ADJF, ADJS</td><td>A</td><td>NNP</td></tr>\n",
    "<tr><td>Глагол</td><td>VERB</td><td>V</td><td>JJ</td></tr>\n",
    "<tr><td>Причастие</td><td>PRTF, PRTS</td><td>V</td><td>NNP</td></tr>\n",
    "<tr><td>Деепричастие</td><td>GRND</td><td>V</td><td>NNP</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем новость с сайта Лента.ру и приведем все слова её текста к начальным формам при помощи разных библиотек. Прибавим при этом к словам части речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Марсоход «Кьюриосити» отыскал одно из лучших свидетельств существования жидкой воды в кратере Гейл в далеком прошлом Марса. Ровер обнаружил на скалах волнообразную текстуру, которая сформировалась, когда скалы были дном неглубокого озера, сообщается на сайте NASA.\\n\n",
    "«Кьюриосити» работает на Марсе более десяти лет, преодолев за это время почти 30 километров. Основной целью ровера стала пятикилометровая гора Шарпа, расположенная в центре кратера Гейл и покрытая массивом из эродированных слоев осадочных пород. Там он проводит геологические исследования и изучает атмосферу Красной планеты.\\n\n",
    "Восьмого февраля 2023 года команда ровера сообщила, что обнаружила одно из лучших свидетельств существования жидкой воды в кратере за все время работы «Кьюриосити». Поднявшись почти на 800 метров над основанием горы Шарп ровер попал в область, названную «Долиной маркерной полосы» (Marker Band Valley) — тонком слое темных скал, который выделяется на фоне остальной части горы. Породы здесь очень твердые, и ранее ровер не смог получить их образец, хотя сделал несколько попыток.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['марсоход_NOUN',\n",
       " 'кьюриосити_NOUN',\n",
       " 'отыскать_VERB',\n",
       " 'один_ADJF',\n",
       " 'из_PREP',\n",
       " 'хороший_ADJF',\n",
       " 'свидетельство_NOUN',\n",
       " 'существование_NOUN',\n",
       " 'жидкий_ADJF',\n",
       " 'вода_NOUN',\n",
       " 'в_PREP',\n",
       " 'кратер_NOUN',\n",
       " 'гейл_NOUN',\n",
       " 'в_PREP',\n",
       " 'далёкий_ADJF',\n",
       " 'прошлый_ADJF',\n",
       " 'марс_NOUN',\n",
       " 'ровер_NOUN',\n",
       " 'обнаружить_VERB',\n",
       " 'на_PREP',\n",
       " 'скала_NOUN',\n",
       " 'волнообразный_ADJF',\n",
       " 'текстура_NOUN',\n",
       " 'который_ADJF',\n",
       " 'сформироваться_VERB',\n",
       " 'когда_CONJ',\n",
       " 'скала_NOUN',\n",
       " 'быть_VERB',\n",
       " 'дно_NOUN',\n",
       " 'неглубокий_ADJF',\n",
       " 'озеро_NOUN',\n",
       " 'сообщаться_VERB',\n",
       " 'на_PREP',\n",
       " 'сайт_NOUN',\n",
       " 'nasa_None',\n",
       " 'кьюриосити_NOUN',\n",
       " 'работать_VERB',\n",
       " 'на_PREP',\n",
       " 'марс_NOUN',\n",
       " 'более_ADVB',\n",
       " 'десять_NUMR',\n",
       " 'год_NOUN',\n",
       " 'преодолеть_GRND',\n",
       " 'за_PREP',\n",
       " 'это_PRCL',\n",
       " 'время_NOUN',\n",
       " 'почти_ADVB',\n",
       " 'километр_NOUN',\n",
       " 'основной_ADJF',\n",
       " 'цель_NOUN',\n",
       " 'ровер_NOUN',\n",
       " 'стать_VERB',\n",
       " 'пятикилометровый_ADJF',\n",
       " 'гора_NOUN',\n",
       " 'шарп_NOUN',\n",
       " 'расположить_PRTF',\n",
       " 'в_PREP',\n",
       " 'центр_NOUN',\n",
       " 'кратер_NOUN',\n",
       " 'гейл_NOUN',\n",
       " 'и_CONJ',\n",
       " 'покрыть_PRTF',\n",
       " 'массив_NOUN',\n",
       " 'из_PREP',\n",
       " 'эродировать_PRTF',\n",
       " 'слой_NOUN',\n",
       " 'осадочный_ADJF',\n",
       " 'порода_NOUN',\n",
       " 'там_ADVB',\n",
       " 'он_NPRO',\n",
       " 'проводить_VERB',\n",
       " 'геологический_ADJF',\n",
       " 'исследование_NOUN',\n",
       " 'и_CONJ',\n",
       " 'изучать_VERB',\n",
       " 'атмосфера_NOUN',\n",
       " 'красный_ADJF',\n",
       " 'планета_NOUN',\n",
       " 'восьмой_ADJF',\n",
       " 'февраль_NOUN',\n",
       " 'год_NOUN',\n",
       " 'команда_NOUN',\n",
       " 'ровер_NOUN',\n",
       " 'сообщить_VERB',\n",
       " 'что_CONJ',\n",
       " 'обнаружить_VERB',\n",
       " 'один_ADJF',\n",
       " 'из_PREP',\n",
       " 'хороший_ADJF',\n",
       " 'свидетельство_NOUN',\n",
       " 'существование_NOUN',\n",
       " 'жидкий_ADJF',\n",
       " 'вода_NOUN',\n",
       " 'в_PREP',\n",
       " 'кратер_NOUN',\n",
       " 'за_PREP',\n",
       " 'всё_PRCL',\n",
       " 'время_NOUN',\n",
       " 'работа_NOUN',\n",
       " 'кьюриосити_NOUN',\n",
       " 'подняться_GRND',\n",
       " 'почти_ADVB',\n",
       " 'на_PREP',\n",
       " 'метр_NOUN',\n",
       " 'над_PREP',\n",
       " 'основание_NOUN',\n",
       " 'гора_NOUN',\n",
       " 'шарп_NOUN',\n",
       " 'ровер_NOUN',\n",
       " 'попасть_VERB',\n",
       " 'в_PREP',\n",
       " 'область_NOUN',\n",
       " 'назвать_PRTF',\n",
       " 'долина_NOUN',\n",
       " 'маркерный_ADJF',\n",
       " 'полоса_NOUN',\n",
       " 'marker_None',\n",
       " 'band_None',\n",
       " 'valley_None',\n",
       " 'тонкий_ADJF',\n",
       " 'слой_NOUN',\n",
       " 'тёмный_ADJF',\n",
       " 'скала_NOUN',\n",
       " 'который_ADJF',\n",
       " 'выделяться_VERB',\n",
       " 'на_PREP',\n",
       " 'фон_NOUN',\n",
       " 'остальной_ADJF',\n",
       " 'часть_NOUN',\n",
       " 'гора_NOUN',\n",
       " 'порода_NOUN',\n",
       " 'здесь_ADVB',\n",
       " 'очень_ADVB',\n",
       " 'твёрдый_ADJF',\n",
       " 'и_CONJ',\n",
       " 'ранее_ADVB',\n",
       " 'ровер_NOUN',\n",
       " 'не_PRCL',\n",
       " 'смочь_VERB',\n",
       " 'получить_INFN',\n",
       " 'они_NPRO',\n",
       " 'образец_NOUN',\n",
       " 'хотя_CONJ',\n",
       " 'сделать_VERB',\n",
       " 'несколько_ADVB',\n",
       " 'попытка_NOUN']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymorphy\n",
    "def normalizePymorphy(text):\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        words.append(pv[0].normal_form + '_' + str(pv[0].tag.POS)) # Берем наиболее вероятную форму.\n",
    "    return words    \n",
    "        \n",
    "# Обратите внимание, что про иностранные слова словарь ничего не знает.\n",
    "normalizePymorphy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['марсоход_S',\n",
       " 'кьюриосити_S',\n",
       " 'отыскивать_V',\n",
       " 'один_A',\n",
       " 'из_P',\n",
       " 'хороший_A',\n",
       " 'свидетельство_S',\n",
       " 'существование_S',\n",
       " 'жидкий_A',\n",
       " 'вода_S',\n",
       " 'в_P',\n",
       " 'кратер_S',\n",
       " 'гейл_S',\n",
       " 'в_P',\n",
       " 'далекий_A',\n",
       " 'прошлое_S',\n",
       " 'марс_S',\n",
       " 'ровер_S',\n",
       " 'обнаруживать_V',\n",
       " 'на_P',\n",
       " 'скала_S',\n",
       " 'волнообразный_A',\n",
       " 'текстура_S',\n",
       " 'который_A',\n",
       " 'сформировываться_V',\n",
       " 'когда_C',\n",
       " 'скала_S',\n",
       " 'быть_V',\n",
       " 'дно_S',\n",
       " 'неглубокий_A',\n",
       " 'озеро_S',\n",
       " 'сообщаться_V',\n",
       " 'на_P',\n",
       " 'сайт_S',\n",
       " 'NASA_U',\n",
       " 'кьюриосити_S',\n",
       " 'работать_V',\n",
       " 'на_P',\n",
       " 'марс_S',\n",
       " 'много_A',\n",
       " 'десять_N',\n",
       " 'год_S',\n",
       " 'преодолевать_V',\n",
       " 'за_P',\n",
       " 'этот_A',\n",
       " 'время_S',\n",
       " 'почти_A',\n",
       " 'километр_S',\n",
       " 'основной_A',\n",
       " 'цель_S',\n",
       " 'ровер_S',\n",
       " 'становиться_V',\n",
       " 'пятикилометровый_A',\n",
       " 'гора_S',\n",
       " 'шарп_S',\n",
       " 'располагать_V',\n",
       " 'в_P',\n",
       " 'центр_S',\n",
       " 'кратер_S',\n",
       " 'гейл_S',\n",
       " 'и_C',\n",
       " 'покрывать_V',\n",
       " 'массив_S',\n",
       " 'из_P',\n",
       " 'эродировать_V',\n",
       " 'слой_S',\n",
       " 'осадочный_A',\n",
       " 'порода_S',\n",
       " 'там_A',\n",
       " 'он_S',\n",
       " 'проводить_V',\n",
       " 'геологический_A',\n",
       " 'исследование_S',\n",
       " 'и_C',\n",
       " 'изучать_V',\n",
       " 'атмосфера_S',\n",
       " 'красный_A',\n",
       " 'планета_S',\n",
       " 'восьмой_A',\n",
       " 'февраль_S',\n",
       " 'год_S',\n",
       " 'команда_S',\n",
       " 'ровер_S',\n",
       " 'сообщать_V',\n",
       " 'что_C',\n",
       " 'обнаруживать_V',\n",
       " 'один_A',\n",
       " 'из_P',\n",
       " 'хороший_A',\n",
       " 'свидетельство_S',\n",
       " 'существование_S',\n",
       " 'жидкий_A',\n",
       " 'вода_S',\n",
       " 'в_P',\n",
       " 'кратер_S',\n",
       " 'за_P',\n",
       " 'весь_A',\n",
       " 'время_S',\n",
       " 'работа_S',\n",
       " 'кьюриосити_S',\n",
       " 'подниматься_V',\n",
       " 'почти_A',\n",
       " 'на_P',\n",
       " 'метр_S',\n",
       " 'над_P',\n",
       " 'основание_S',\n",
       " 'гора_S',\n",
       " 'шарп_S',\n",
       " 'ровер_S',\n",
       " 'попадать_V',\n",
       " 'в_P',\n",
       " 'область_S',\n",
       " 'называть_V',\n",
       " 'долина_S',\n",
       " 'маркерный_A',\n",
       " 'полоса_S',\n",
       " 'Marker_U',\n",
       " 'Band_U',\n",
       " 'Valley_U',\n",
       " 'тонкий_A',\n",
       " 'слой_S',\n",
       " 'темный_A',\n",
       " 'скала_S',\n",
       " 'который_A',\n",
       " 'выделяться_V',\n",
       " 'на_P',\n",
       " 'фон_S',\n",
       " 'остальной_A',\n",
       " 'часть_S',\n",
       " 'гора_S',\n",
       " 'порода_S',\n",
       " 'здесь_A',\n",
       " 'очень_A',\n",
       " 'твердый_A',\n",
       " 'и_C',\n",
       " 'ранее_A',\n",
       " 'ровер_S',\n",
       " 'не_P',\n",
       " 'смочь_V',\n",
       " 'получать_V',\n",
       " 'они_S',\n",
       " 'образец_S',\n",
       " 'хотя_C',\n",
       " 'сделать_V',\n",
       " 'несколько_N',\n",
       " 'попытка_S']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyMystem\n",
    "def normalizePymystem(text):\n",
    "    tokens = mystem.analyze(text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if 'analysis' in t.keys():\n",
    "            if t['analysis'] != []:\n",
    "                words.append(t['analysis'][0]['lex']+'_'+t['analysis'][0]['gr'][0])\n",
    "            else:\n",
    "                words.append(t['text']+'_'+'U')\n",
    "    return words    \n",
    "        \n",
    "# Не все считают, что причастие всегда выступает в роли глагола, но иногда так значительно проще.\n",
    "normalizePymystem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Марсоход_JJ',\n",
       " '«_NNP',\n",
       " 'Кьюриосити_NNP',\n",
       " '»_NNP',\n",
       " 'отыскал_NNP',\n",
       " 'одно_NNP',\n",
       " 'из_NNP',\n",
       " 'лучших_NNP',\n",
       " 'свидетельств_NNP',\n",
       " 'существования_NNP',\n",
       " 'жидкой_NNP',\n",
       " 'воды_NNP',\n",
       " 'в_NNP',\n",
       " 'кратере_NNP',\n",
       " 'Гейл_NNP',\n",
       " 'в_NNP',\n",
       " 'далеком_NNP',\n",
       " 'прошлом_NNP',\n",
       " 'Марса_NNP',\n",
       " 'Ровер_VB',\n",
       " 'обнаружил_JJ',\n",
       " 'на_NNP',\n",
       " 'скалах_NNP',\n",
       " 'волнообразную_NNP',\n",
       " 'текстуру_NNP',\n",
       " 'которая_NNP',\n",
       " 'сформировалась_NNP',\n",
       " 'когда_NNP',\n",
       " 'скалы_NNP',\n",
       " 'были_NNP',\n",
       " 'дном_NNP',\n",
       " 'неглубокого_NNP',\n",
       " 'озера_NNP',\n",
       " 'сообщается_NNP',\n",
       " 'на_NNP',\n",
       " 'сайте_NNP',\n",
       " 'NASA_NNP',\n",
       " '«_VB',\n",
       " 'Кьюриосити_JJ',\n",
       " '»_NNP',\n",
       " 'работает_NNP',\n",
       " 'на_NNP',\n",
       " 'Марсе_NNP',\n",
       " 'более_NNP',\n",
       " 'десяти_NNP',\n",
       " 'лет_NNP',\n",
       " 'преодолев_NNP',\n",
       " 'за_NNP',\n",
       " 'это_NNP',\n",
       " 'время_NNP',\n",
       " 'почти_VBD',\n",
       " '30_CD',\n",
       " 'километров_NN',\n",
       " 'Основной_CC',\n",
       " 'целью_JJ',\n",
       " 'ровера_NNP',\n",
       " 'стала_NNP',\n",
       " 'пятикилометровая_NNP',\n",
       " 'гора_NNP',\n",
       " 'Шарпа_NNP',\n",
       " 'расположенная_NNP',\n",
       " 'в_NNP',\n",
       " 'центре_NNP',\n",
       " 'кратера_NNP',\n",
       " 'Гейл_NNP',\n",
       " 'и_NNP',\n",
       " 'покрытая_NNP',\n",
       " 'массивом_NNP',\n",
       " 'из_NNP',\n",
       " 'эродированных_NNP',\n",
       " 'слоев_NNP',\n",
       " 'осадочных_NNP',\n",
       " 'пород_NNP',\n",
       " 'Там_VB',\n",
       " 'он_JJ',\n",
       " 'проводит_NNP',\n",
       " 'геологические_NNP',\n",
       " 'исследования_NNP',\n",
       " 'и_NNP',\n",
       " 'изучает_NNP',\n",
       " 'атмосферу_NNP',\n",
       " 'Красной_NNP',\n",
       " 'планеты_NNP',\n",
       " 'Восьмого_NN',\n",
       " 'февраля_JJ',\n",
       " '2023_CD',\n",
       " 'года_NNP',\n",
       " 'команда_NNP',\n",
       " 'ровера_NNP',\n",
       " 'сообщила_NNP',\n",
       " 'что_NNP',\n",
       " 'обнаружила_NNP',\n",
       " 'одно_NNP',\n",
       " 'из_NNP',\n",
       " 'лучших_NNP',\n",
       " 'свидетельств_NNP',\n",
       " 'существования_NNP',\n",
       " 'жидкой_NNP',\n",
       " 'воды_NNP',\n",
       " 'в_NNP',\n",
       " 'кратере_NNP',\n",
       " 'за_NNP',\n",
       " 'все_NNP',\n",
       " 'время_NNP',\n",
       " 'работы_NNP',\n",
       " '«_NNP',\n",
       " 'Кьюриосити_NNP',\n",
       " '»_NNP',\n",
       " 'Поднявшись_VB',\n",
       " 'почти_JJ',\n",
       " 'на_$',\n",
       " '800_CD',\n",
       " 'метров_NNP',\n",
       " 'над_NNP',\n",
       " 'основанием_NNP',\n",
       " 'горы_NNP',\n",
       " 'Шарп_NNP',\n",
       " 'ровер_NNP',\n",
       " 'попал_NNP',\n",
       " 'в_NNP',\n",
       " 'область_NNP',\n",
       " 'названную_NNP',\n",
       " '«_NNP',\n",
       " 'Долиной_NNP',\n",
       " 'маркерной_NNP',\n",
       " 'полосы_NNP',\n",
       " '»_NNP',\n",
       " 'Marker_NNP',\n",
       " 'Band_NNP',\n",
       " 'Valley_NNP',\n",
       " '—_VBP',\n",
       " 'тонком_JJ',\n",
       " 'слое_NNP',\n",
       " 'темных_NNP',\n",
       " 'скал_NNP',\n",
       " 'который_NNP',\n",
       " 'выделяется_NNP',\n",
       " 'на_NNP',\n",
       " 'фоне_NNP',\n",
       " 'остальной_NNP',\n",
       " 'части_NNP',\n",
       " 'горы_NNP',\n",
       " 'Породы_VB',\n",
       " 'здесь_JJ',\n",
       " 'очень_NNP',\n",
       " 'твердые_NNP',\n",
       " 'и_NNP',\n",
       " 'ранее_NNP',\n",
       " 'ровер_NNP',\n",
       " 'не_NNP',\n",
       " 'смог_NNP',\n",
       " 'получить_NNP',\n",
       " 'их_NNP',\n",
       " 'образец_NNP',\n",
       " 'хотя_NNP',\n",
       " 'сделал_NNP',\n",
       " 'несколько_NNP',\n",
       " 'попыток_NNP']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "def normalizeNLTK(text):\n",
    "    tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if t[0] != t[1]:\n",
    "            words.append(t[0]+'_'+t[1])\n",
    "    return words    \n",
    "        \n",
    "# А вот здесь с частеречной разметкой всё плохо, а параметров нет вовсе.\n",
    "normalizeNLTK(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # Не считать же частоты самим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'в_PREP': 5, 'ровер_NOUN': 5, 'на_PREP': 5, 'кьюриосити_NOUN': 3, 'из_PREP': 3, 'кратер_NOUN': 3, 'скала_NOUN': 3, 'гора_NOUN': 3, 'и_CONJ': 3, 'один_ADJF': 2, 'хороший_ADJF': 2, 'свидетельство_NOUN': 2, 'существование_NOUN': 2, 'жидкий_ADJF': 2, 'вода_NOUN': 2, 'гейл_NOUN': 2, 'марс_NOUN': 2, 'обнаружить_VERB': 2, 'который_ADJF': 2, 'год_NOUN': 2, 'за_PREP': 2, 'время_NOUN': 2, 'почти_ADVB': 2, 'шарп_NOUN': 2, 'слой_NOUN': 2, 'порода_NOUN': 2, 'марсоход_NOUN': 1, 'отыскать_VERB': 1, 'далёкий_ADJF': 1, 'прошлый_ADJF': 1, 'волнообразный_ADJF': 1, 'текстура_NOUN': 1, 'сформироваться_VERB': 1, 'когда_CONJ': 1, 'быть_VERB': 1, 'дно_NOUN': 1, 'неглубокий_ADJF': 1, 'озеро_NOUN': 1, 'сообщаться_VERB': 1, 'сайт_NOUN': 1, 'nasa_None': 1, 'работать_VERB': 1, 'более_ADVB': 1, 'десять_NUMR': 1, 'преодолеть_GRND': 1, 'это_PRCL': 1, 'километр_NOUN': 1, 'основной_ADJF': 1, 'цель_NOUN': 1, 'стать_VERB': 1, 'пятикилометровый_ADJF': 1, 'расположить_PRTF': 1, 'центр_NOUN': 1, 'покрыть_PRTF': 1, 'массив_NOUN': 1, 'эродировать_PRTF': 1, 'осадочный_ADJF': 1, 'там_ADVB': 1, 'он_NPRO': 1, 'проводить_VERB': 1, 'геологический_ADJF': 1, 'исследование_NOUN': 1, 'изучать_VERB': 1, 'атмосфера_NOUN': 1, 'красный_ADJF': 1, 'планета_NOUN': 1, 'восьмой_ADJF': 1, 'февраль_NOUN': 1, 'команда_NOUN': 1, 'сообщить_VERB': 1, 'что_CONJ': 1, 'всё_PRCL': 1, 'работа_NOUN': 1, 'подняться_GRND': 1, 'метр_NOUN': 1, 'над_PREP': 1, 'основание_NOUN': 1, 'попасть_VERB': 1, 'область_NOUN': 1, 'назвать_PRTF': 1, 'долина_NOUN': 1, 'маркерный_ADJF': 1, 'полоса_NOUN': 1, 'marker_None': 1, 'band_None': 1, 'valley_None': 1, 'тонкий_ADJF': 1, 'тёмный_ADJF': 1, 'выделяться_VERB': 1, 'фон_NOUN': 1, 'остальной_ADJF': 1, 'часть_NOUN': 1, 'здесь_ADVB': 1, 'очень_ADVB': 1, 'твёрдый_ADJF': 1, 'ранее_ADVB': 1, 'не_PRCL': 1, 'смочь_VERB': 1, 'получить_INFN': 1, 'они_NPRO': 1, 'образец_NOUN': 1, 'хотя_CONJ': 1, 'сделать_VERB': 1, 'несколько_ADVB': 1, 'попытка_NOUN': 1})\n",
      "\n",
      "{'кьюриосити_NOUN': 3, 'один_ADJF': 2, 'из_PREP': 3, 'хороший_ADJF': 2, 'свидетельство_NOUN': 2, 'существование_NOUN': 2, 'жидкий_ADJF': 2, 'вода_NOUN': 2, 'в_PREP': 5, 'кратер_NOUN': 3, 'гейл_NOUN': 2, 'марс_NOUN': 2, 'ровер_NOUN': 5, 'обнаружить_VERB': 2, 'на_PREP': 5, 'скала_NOUN': 3, 'который_ADJF': 2, 'год_NOUN': 2, 'за_PREP': 2, 'время_NOUN': 2, 'почти_ADVB': 2, 'гора_NOUN': 3, 'шарп_NOUN': 2, 'и_CONJ': 3, 'слой_NOUN': 2, 'порода_NOUN': 2}\n"
     ]
    }
   ],
   "source": [
    "words = normalizePymorphy(text)\n",
    "wdict = Counter(words) # Объект сразу посчитает частоты элементов списка.\n",
    "print(wdict)\n",
    "print()\n",
    "print({w:n for w,n in wdict.items() if n>1}) # Посмотрим какие слова встречаются больше одного раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, выше мы провели преобразование текста в вектор. Пространство вектора определено на словаре текста - количество измерений совпадает с количеством слов, каждому измерению сопоставлено какое-то слово и отложена его частота. Подобный подход называют мешком слов (Bag of Words, BoW), так как все слова перемешиваются, их порядок больше не соблюдается, а сами слова сваливаются в один \"мешок\".\n",
    "\n",
    "![](img/donkey_carrot_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на распределение частот в отдельных словах и парах. Такое распределение называется [распределением Ципфа](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A6%D0%B8%D0%BF%D1%84%D0%B0) и является характерным практически для любого распределения частот слов и их комбинаций в текстах на любом естественном языке.\n",
    "\n",
    "$p_i=\\frac{p_0}{\\beta^{-\\alpha*i}}$, где $\\alpha\\approx1$.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/ru/thumb/d/d8/WikipediaZipf20061023.png/450px-WikipediaZipf20061023.png)\n",
    "\n",
    "Но вообще, для расчета частот существует CountVectorizer, который позволяет сделать это всё за один раз и очень хорошо ложится в конвейер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, всё то же самое можно было посчитать при помощи класса [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) из библиотеки [skLearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text). Но её мы оставим на самостоятельное изучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обратите внимание на ещё пару фактов. Во-первых, наиболее частотными оказали служебные слова. Во-вторых, не все слова из аннотации, написанной человеком, вошли в список наиболее частотных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'кьюриосити_NOUN': 3, 'один_ADJF': 2, 'из_PREP': 3, 'хороший_ADJF': 2, 'свидетельство_NOUN': 2, 'существование_NOUN': 2, 'жидкий_ADJF': 2, 'вода_NOUN': 2, 'в_PREP': 5, 'кратер_NOUN': 3, 'гейл_NOUN': 2, 'марс_NOUN': 2, 'ровер_NOUN': 5, 'обнаружить_VERB': 2, 'на_PREP': 5, 'скала_NOUN': 3, 'который_ADJF': 2, 'год_NOUN': 2, 'за_PREP': 2, 'время_NOUN': 2, 'почти_ADVB': 2, 'гора_NOUN': 3, 'шарп_NOUN': 2, 'и_CONJ': 3, 'слой_NOUN': 2, 'порода_NOUN': 2}\n"
     ]
    }
   ],
   "source": [
    "print({w:n for w,n in wdict.items() if n>1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая бы оставляла слова только со значимыми частями речи (краткое и полное прилагательное, существительное, глагол, краткая и полная формы причастия, деепричастие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_POS = ['ADJF', 'ADJS', 'NOUN', 'VERB', 'PRTF', 'PRTS', 'GRND']\n",
    "\n",
    "def getMostFrequentWordsFiltered(text):\n",
    "    \n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        if pv[0].tag.POS in imp_POS and pv[0].normal_form != 'быть':\n",
    "            words.append(pv[0].normal_form)\n",
    "    wdict = Counter(words)\n",
    "    wdict = {w:n for w,n in wdict.items() if n>1}\n",
    "    return sorted(wdict.items(), key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ровер', 5),\n",
       " ('кьюриосити', 3),\n",
       " ('кратер', 3),\n",
       " ('скала', 3),\n",
       " ('гора', 3),\n",
       " ('один', 2),\n",
       " ('хороший', 2),\n",
       " ('свидетельство', 2),\n",
       " ('существование', 2),\n",
       " ('жидкий', 2),\n",
       " ('вода', 2),\n",
       " ('гейл', 2),\n",
       " ('марс', 2),\n",
       " ('обнаружить', 2),\n",
       " ('который', 2),\n",
       " ('год', 2),\n",
       " ('время', 2),\n",
       " ('шарп', 2),\n",
       " ('слой', 2),\n",
       " ('порода', 2)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostFrequentWordsFiltered(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим себе, что мы взяли много текстов и для каждого из них посчитали вектор слов. Результатом работы является [разреженная матрица](https://docs.scipy.org/doc/scipy/reference/sparse.html) частот слов. Если мы возьмем большое количество текстов, то в кажом из них встречается не так много разных слов, но словарь всех текстов вместе будет огромен. Обработка текстов должна вестись в едином пространстве. Пусть это будет пространство словаря всех текстов (в противном случае у каждого текста будет свое собственное пространство, что чрезвычайно неудобно). Получается, что для текста с маленьким словарем мы должны хранить большое число нулей. Для того, чтобы этого избежать, хранят, например, один раз номер строки, индексы ненулевых значений и сами значения, то есть чуть больше двух чисел на ненулевое значение. Если считать, что словарь заметки - 100 слов, а словарь всех текстов - 100 000 слов, мы получаем экономию места в 500 раз. То, что считалось на кластере, теперь может считаться на недобуке с 2 Гб оперативной памяти.\n",
    "\n",
    "![](img/term-document-matrix-bow-annotated.png)  \n",
    "Изображение взято [отсюда](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем другой показатель для подсчета важности слов в тексте - $TF*IDF$. Здесь $TF$ - Term Frequency, частота термина в документе, а $IDF$ - Inverted Document Frequency, обратная частота термина в коллекции (количество документов, в которых встречается данный термин).\n",
    "\n",
    "Идея метрики очень проста. Если слово встречается почти во всех документах - его различительная сила очень мала и само слово не является важным. Если слово часто встречается в данном документе, то оно являетсяя важным для него.\n",
    "\n",
    "\n",
    "![](img/term-document-matrix-tfidf-annotated.png)  \n",
    "Изображение взято [отсюда](https://livebook.manning.com/book/natural-language-processing-in-action/chapter-4/v-4/61).\n",
    "\n",
    "\n",
    "Метрика считается на коллекции документов для каждого слова, каждого документа. Для расчета меры можно использовать `TfidfVectorizer`, который работает так же как `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи функции `fit_transform` можно получить разреженное представление матрицы частот слов. Основная проблема состоит в том, что индексы в матрице представляют собой индексы в словаре переданных текстов. Сам словарь хранится в свойстве `vocabulary_` и умеет возвращать индекс по слову (но не наоборот)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/war_and_peace3.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "# Выделяем все слова написанные русской кириллицей.\n",
    "words = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "newtext = ' '.join(words)\n",
    "\n",
    "with open('data/sebastopol.txt') as fil:\n",
    "    textSb = fil.read()\n",
    "words3 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textSb)]\n",
    "newtext3 = ' '.join(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t3\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t3\n",
      "  (0, 9)\t1\n",
      "  (0, 7)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "7400\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "counter = CountVectorizer()\n",
    "# Просим посчитать частоты слов.\n",
    "res = counter.fit_transform([newtext, newtext3])\n",
    "# Разреженное представление счетчика.\n",
    "print(res[0,:10])\n",
    "# Можно получить индекс по слову, ...\n",
    "print(counter.vocabulary_.get('левый'))\n",
    "# ... но не наоборот.\n",
    "print(counter.vocabulary_.get(20342))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А ещё можно создать функцию-анализатор, которая поможет с обработкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeaningfullWords(text: str, morph: pymorphy3.MorphAnalyzer):\n",
    "    words = []\n",
    "    tokens = re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        if pv[0].tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "            words.append(pv[0].normal_form)\n",
    "    return words\n",
    "\n",
    "lemmaCounter = CountVectorizer(ngram_range=(1,3), \n",
    "                               token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext3, morph))]\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1 = analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лев',\n",
       " 'николаевич',\n",
       " 'толстой',\n",
       " 'война',\n",
       " 'мир',\n",
       " 'тот',\n",
       " 'олег',\n",
       " 'колесников',\n",
       " 'часть',\n",
       " 'первый']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaCounter = TfidfVectorizer(ngram_range=(1,3), \n",
    "                               token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext3, morph))]\n",
    "res_tfidf = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x124377 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 127153 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лев', 46014),\n",
       " ('николаевич', 59614),\n",
       " ('толстой', 109339),\n",
       " ('война', 12704),\n",
       " ('мир', 50863),\n",
       " ('тот', 109674),\n",
       " ('олег', 64364),\n",
       " ('колесников', 41260),\n",
       " ('часть', 119174),\n",
       " ('первый', 69467)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lemmaCounter.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.00248751, 0.00248751, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.00248751, 0.00248751, 0.00248751]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_tfidf = res_tfidf.todense()\n",
    "dense_tfidf[1, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('быть', 0.4477814550808446),\n",
       " ('который', 0.2707927376575859),\n",
       " ('сказать', 0.26371318896065554),\n",
       " ('володя', 0.24626395219853367),\n",
       " ('офицер', 0.1964574763398172),\n",
       " ('один', 0.16813928155209582),\n",
       " ('брат', 0.14513074828707218),\n",
       " ('козелец', 0.12686324810227492),\n",
       " ('солдат', 0.11504266632511818),\n",
       " ('другой', 0.10973300480242042)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dct = dict()\n",
    "rev_dct = {key: word for word, key in lemmaCounter.vocabulary_.items()}\n",
    "for i in range(dense_tfidf.shape[1]):\n",
    "    if dense_tfidf[1, i] != 0:\n",
    "        tf_dct[rev_dct[i]] = dense_tfidf[1, i]\n",
    "\n",
    "sorted(tf_dct.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но нам необходимо искать новости, которые интересны пользователю.\n",
    "\n",
    "Для определения меры сходства двух статей теперь может использоваться косинусная мера сходства, рассчитываемая по следующей формуле: $cos(a,b)=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$.\n",
    "\n",
    "Косинусная мера сходство смотрит не на близость конечных точек, а на то, в какую сторону направлены векторы.\n",
    "\n",
    "![](img/cosine_watch.jpg)\n",
    "\n",
    "Вообще-то, использовать стандартную функцию рассчета косинусной меры сходства из <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">sklearn</a> было бы быстрее. Но в данной задаче нам бы пришлось сводить все словари в один, чтобы на одних и тех же местах в векторе были частоты одних и тех же слов. Чтобы избежать подобной работы, напишем собственную функцию рассчета косинусного расстояния, работающую с разреженными векторами в виде питоновских словарей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь следует остановиться на том, что косинусная мера является мерой расстояния между объектами, причем подобных мер существует достаточно много. Обычно мы используем Евклидово расстояние: $d_E=\\sqrt{\\sum {(a_i-b_i)^2}}$. Но при обработке текстов оно работает гораздо хуже. Представим себе, что у нас есть два текста: текст статьи и текст статьи, объединенный с самим собой. Евклидово расстояние между ними будет значительным, тогда как содержание не изменится.\n",
    "\n",
    "Помимо Евклидового расстояния часто используется Манхэттенское расстояние (расстояние городских кварталов). Если взглянуть на карту Манхэттена, то мы увидим, что практически все улицы параллельны и перпендикулярны друг другу. Это означает, что выбирая любой не удлинняющий маршрут из одной точки в другую я пройду одно и то же расстояние: $d_M=\\sum {|a_i-b_i|}$. Манхэттенское расстояние используется в тех случаях, когда мы берем, например, взвешенную сумму параметров с тем, чтобы получить единую оценку. Например, оценивая различные офисы мы складываем с некоторыми коэффициентами стоимость, площадь, расстояние от центра или дома, оценку инфраструктуры и так далее. Аналогично можно брать разницу между двумя векторными представлениями офисов, чтобы найти насколько они сходны.\n",
    "\n",
    "Расстояние Жаккара берет отношение размера пересечения словарей к их объединению: $d_J=\\frac{A \\cup B}{A \\cap B}$. Эта мера проверяет степень совпадения словарей двух текстов. Если словари совпадают полностью, то тексты, скорее всего, говорят об одном и том же.\n",
    "\n",
    "Однако, в одном тексте может обсуждаться производство шестеренок, а во вводной части однажды будет упомянуто, что они необходимы для сбора механизмов, тогда как в другом тексте будут обсуждаться сами механизмы с единственным упоминанием, что они состоят из шестеренок. Случайное появление отдельных слов сделает тексты более похожими, чем это следует из их содержания. Этот недостаток устраняет косинусная мера сходства: $d_{cos}=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$. Если в одном из текстов не встречается слово из другого текста, то соответствующий член суммы в числителе будет равен нулю. Если в одном тексте слово встречается часто, а в другом редко, произведение не будет слишком большим. Проблему представляет случай, когда в обоих текстах есть несколько очень часто встречающихся слов. Тогда их произведение будет забивать все остальные слова, искажая общий смысл.\n",
    "\n",
    "Следует иметь в виду, что косинусная мера сходства является величиной, обратно зависящей от расстояния: два одинаковых текста будут иметь косинусное сходство равное 1, тогда как расстояние между ними равно нулю, и наоборот.\n",
    "\n",
    "В качестве мер расстояния также используются [корреляция](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F) и [дивергенция Кулльбака-Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
